% LaTeX file for Chapter 03
<<'preamble03',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch03_fig', 
    self.contained=FALSE,
    cache=FALSE
) 
@

<<echo=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch02_fig',
    self.contained=FALSE,
    cache=TRUE
)
@


<<echo=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figure/ch02_fig',
               echo=TRUE, message=FALSE,
               fig.width=8.1, fig.height=3.7,
               out.width='\\textwidth-3cm',
               message=FALSE, fig.align='center',
               background="gray98", tidy=FALSE, #tidy.opts=list(width.cutoff=60),
               cache=TRUE
)
options(width=74)
@

<<echo=FALSE>>= 
PATH_HOME = path.expand("~") # user home
PATH = file.path(PATH_HOME, 'Data/PubBias')
PATH2 = file.path(PATH_HOME, 'PubBias')
FILE = 'cochrane_2018-06-09.csv'
PATH_DATA = file.path(PATH, 'data')
PATH_CODE = file.path(PATH2, 'code')
PATH_RESULTS = file.path(PATH2, 'results')
PATH_FIGURES = file.path(PATH_RESULTS, 'figures')

file_results = "pb.RData"

source(file.path(PATH_CODE, 'PubBias_functions.R'))


data = pb.readData(path = PATH_DATA, file = FILE)
tmp = pb.clean(data)
data = tmp[[1]]
aliases = tmp[[2]]

file.bin <- "pb.bin.RData"
if (file.exists(file.path(PATH_RESULTS, file.bin))) {
  load(file.path(PATH_RESULTS, file.bin))
} else { 
  meta.bin <- meta.bin.complete(data, min.study.number = 10, sig.level = 0.05)
  save(meta.bin, file =  file.path(PATH_RESULTS, file.bin))
}

file.cont <- "pb.cont.RData"
if (file.exists(file.path(PATH_RESULTS, file.cont))) {
  load(file.path(PATH_RESULTS, file.cont))
} else { 
  meta.cont <- meta.cont.complete(data, min.study.number = 10, sig.level = 0.05)
  save(meta.cont, file =  file.path(PATH_RESULTS, file.cont))
}

meta <- pb.meta.merge(meta.bin, meta.cont)

file.cont <- "mly.cont.RData"
if (file.exists(file.path(PATH_RESULTS, file.cont))) {
  load(file.path(PATH_RESULTS, file.cont))
} else { 
  data.cont <- mly.cont(data.ext, 0.05, min.study.number = 2)
  save(data.cont, file =  file.path(PATH_RESULTS, file.cont))
}

file.bin <- "mly.bin.RData"
if (file.exists(file.path(PATH_RESULTS, file.bin))) {
  load(file.path(PATH_RESULTS, file.bin))
} else { 
  data.bin <- mly.bin(data.ext, 0.05, min.study.number = 2)
  save(data.bin, file =  file.path(PATH_RESULTS, file.bin))
}


load(file.path(PATH_RESULTS, file = "mly.RData"))
load(file.path(PATH_RESULTS, file = "data.processed.RData"))

require(biostatUZH)
require(tidyverse)
require(meta)
require(metasens)
require(gridExtra)
require(xtable)
@


\chapter{Results}

<<echo = FALSE, warning =FALSE>>=
#Merge comparisons with meta-analysis result:
mly.tomerge <- mly %>% select(meta.id, sig.Q, n.sig.type, n.sig.fishersz, sig.fixef, sig.ranef, sig.ranef.hkn)

data.mly <- merge(x = data.ext, y = mly.tomerge, by = c("meta.id")) #ideally, 278626 comparisons ( = sum(mly$n)), 
# but 300831 (18261 have smaller sample size than mly, but same meta.id)

data.mly <- data.mly %>% filter(total1 > 11 & total2 > 11) #Now 282550

                   

sig.meta.c <- data.mly %>% select(sig.type, sig.fixef, sig.ranef, sig.ranef.hkn) %>% 
  gather(key = "meta.analysis.method", value = "sig.meta.analysis") %>% mutate(sig.meta.analysis = factor(sig.meta.analysis))

sig.meta.count.c <- sig.meta.c %>% 
  group_by(sig.meta.analysis, meta.analysis.method) %>% count()

sig.meta.c <- sig.meta.c %>%  ggplot(aes(x = meta.analysis.method, fill = sig.meta.analysis)) + 
  geom_bar() + coord_flip() +
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("meta.nonsig", "meta.sig")) +
  annotate("text", x = sig.meta.count.c[sig.meta.count.c$sig.meta.analysis == 0,]$meta.analysis.method, y = 100000, 
           label = paste(sig.meta.count.c[sig.meta.count.c$sig.meta.analysis == 0,]$n, "nonsig."), 
           color = "white")


#Change in significance after meta-analysis, separated by significant effect size estimate
sig.meta <- data.mly %>% 
  select(meta.id, study.id, sig.fixef, sig.ranef, sig.ranef.hkn) %>% 
  gather(key = "meta.analysis.method", value = "sig.meta.analysis", sig.fixef:sig.ranef.hkn) %>% mutate(sig.meta.analysis = factor(sig.meta.analysis))

# duplicate.studies <- data.mly %>% group_by(file.nr, comparison.nr, outcome.nr, subgroup.nr, study.name) %>% 
#   count() %>% filter(n > 1) #Merging is not possible for these since studies bear multiple results, but are not distinguishable and are multiplied while merging.

sig.meta.plot <- merge(y = select(data.ext, meta.id, study.id, sig.type), 
                       x = sig.meta, by = c("meta.id", "study.id")) %>% 
  mutate(sig.type = factor(sig.type))

sig.meta.count <- sig.meta.plot %>% 
  group_by(sig.meta.analysis, meta.analysis.method, sig.type) %>% count()

sig.meta.plot1 <- sig.meta.plot %>% filter(sig.type == 1) %>%  ggplot(aes(x = meta.analysis.method, fill = sig.meta.analysis)) + 
  geom_bar() + coord_flip() + ggtitle("Significant studies") +
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("non-significant", "significant")) +
  annotate("text", x = sig.meta.count[which(sig.meta.count$sig.type == 1)[1:3],]$meta.analysis.method, y = 20000, 
           label = paste(sig.meta.count[which(sig.meta.count$sig.type == 1)[1:3],]$n, "changed to nonsig."), 
           color = "white")

nonsig.meta.plot <- sig.meta.plot %>% filter(sig.type == 0) %>%  ggplot(aes(x = meta.analysis.method, fill = sig.meta.analysis)) + 
  geom_bar() + coord_flip() + ggtitle("Non-significant studies") +
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("non-significant", "significant")) +
  annotate("text", x = sig.meta.count[which(sig.meta.count$sig.type == 0)[4:6],]$meta.analysis.method, y = 100000, 
           label = paste(sig.meta.count[which(sig.meta.count$sig.type == 0)[4:6],]$n, "changed to sig."), 
           color = "white")


#Change in significance after meta-analysis, separated by significant heterogeneity
sig.meta.Q <- data.mly %>% 
  select(meta.id, sig.Q, sig.fixef, sig.ranef, sig.ranef.hkn) %>% 
  gather(key = "meta.analysis.method", value = "sig.meta.analysis", sig.fixef:sig.ranef.hkn) %>% 
  mutate(sig.meta.analysis = factor(sig.meta.analysis))


sig.meta.count.Q <- sig.meta.Q %>% 
  group_by(sig.meta.analysis, meta.analysis.method, sig.Q) %>% count()

sig.meta.Q1 <- sig.meta.Q %>% filter(sig.Q == 1) %>%  ggplot(aes(x = meta.analysis.method, fill = sig.meta.analysis)) + 
  geom_bar() + coord_flip() + ggtitle("Significant heterogeneity Q studies") +
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("non-significant", "significant")) +
  annotate("text", x = sig.meta.count.Q[which(sig.meta.count.Q$sig.Q == 1)[1:3],]$meta.analysis.method, y = 20000, 
           label = paste(sig.meta.count.Q[which(sig.meta.count.Q$sig.Q == 1)[1:3],]$n, "nonsig."), 
           color = "white")

nonsig.meta.Q <- sig.meta.Q %>% filter(sig.Q == 0) %>%  ggplot(aes(x = meta.analysis.method, fill = sig.meta.analysis)) + 
  geom_bar() + coord_flip() + ggtitle("Non-significant heterogeneity Q studies") +
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("non-significant", "significant")) +
  annotate("text", x = sig.meta.count.Q[which(sig.meta.count.Q$sig.Q == 0)[1:3],]$meta.analysis.method, y = 100000, 
           label = paste(sig.meta.count.Q[which(sig.meta.count.Q$sig.Q == 0)[1:3],]$n, "nonsig."), 
           color = "white")

@


\section{Meta-analysis}
The data at hand is composed of over 400,000 comparisons with balanced sample size $n > 24$, which has been chosen as a threshold to assess the significance of treatment effect. The significance of the p-values of treatment effect estimates was calculated for all comparisons in the dataset with acceptable sample size and outcome measures as odds ratios, risk ratios, risk differences and mean and standardized mean differences. This led to somewhat more than 400,000 significance test results. Since most of the times, the study publication year is available, the fraction of significant treatment effects found is shown over time in Figure \ref{study.significance.overtime}.The $p$-value was chosen to be $0.05$  only times where a reasonbly large number of effect estimates is available is shown in order to reduce random fluctuation ($n > 800$). 

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
data.ext %>% filter(!is.na(sig.type)) %>%  ggplot(aes(x = study.year, fill = factor(sig.type), stat(count))) + geom_density(na.rm = T, position = "fill") + 
  labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))+ scale_x_continuous(limits = c(1970, 2017))
@
\caption{Mean of the absolute value of the normalized effect size plotted against the total sample size.}
\label{study.significance.overtime}
\end{figure}

It is possible to analyse all studies with one or more replica by meta-analysis. There are different meta-analys methods that can be applied. While fixed effects meta-analysis pools the estimates and their variance, random effects meta-analysis additionally estimates the between study variances and adds this variance to the pooled within-study variances when assessing the uncertainity of the pooled effect estimate. The Hartung and Knapp adjustment of p-values and confidence intervals for random effects meta-analysis is more conservative than random-effects meta-analysis and will yieild less significant results than the latter.

\vspace{0mm}
In the following, the study results are combined with the results of their corresponding meta-analysis. Thus, every study that has one or more replicate was analysed by meta-analysis. In the following, the significance of the study, the primary significance, is compared with the significance of the meta-analysis or secondary significance.

\vspace{0mm}
In Figure \ref{primary.secondary.significance}, the overall fraction of studies with significant and non-significant meta-analysis results is shown. The meta-analysis method used to pool the treatment effect estimates and assessing significance is indicated on the right hand side of the Figure. One can see the overall fraction of rejected null-hypotheses of no treatment effect.

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
plot(sig.meta.c)
@
\caption{Overall fraction of studies whose treatment effect estimate was significant when pooled by means of meta-analysis. The fractions have been calculated
by fixed-effects, random-effects and Hartung and Knapp adjusted random-effects meta-analysis}
\label{primary.secondary.significance}
\end{figure}

One can visualise the fraction of studies with significant treatment effect separately for studies with significant primary treatment effect, i.e. non-pooled effect, and non-significant primary treatment effect. This can be seen in Figure \ref{primary.secondary.significance.sep.sig}. One can see that primary significance and non-significance is often overruled when performing a meta-analysis.

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
grid.arrange(sig.meta.plot1, nonsig.meta.plot, ncol= 1)
@
\caption{Overall fraction of studies whose treatment effect estimate was significant when pooled by means of meta-analysis, separated by significance of study treatment effect estimate. The fractions have been calculated
by fixed-effects, random-effects and Hartung and Knapp adjusted random-effects meta-analysis}
\label{primary.secondary.significance}
\end{figure}

The separation of studies can also be made based on the significance of heterogeneity between them when pooling them by means of a meta-analysis. Significant heterogeneity between studies corresponds to a rejection of the null-hypothesis that all the study treatment effect estimates share the same underlying distribution. The test used to assess heterogeneity was based on the between-study heterogeneity estimate $Q$ estimated as \citet{tau.estimator} suggested. This is shown in Figure \ref{primary.secondary.significance.sep.sig}.

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
grid.arrange(sig.meta.Q1, nonsig.meta.Q, ncol= 1)
@
\caption{Overall fraction of studies whose treatment effect estimate was significant when pooled by means of meta-analysis, separated by significance of study treatment effect estimate. The fractions have been calculated
by fixed-effects, random-effects and Hartung and Knapp adjusted random-effects meta-analysis}
\label{primary.secondary.significance.sep.sig}
\end{figure}


\section{Small study effects}
One crucial assumption in meta analysis is that the availability and publication of studies does not depend on their effect and the variance of the effect. If this is not given, one often speaks of publication bias. In fact, there can also be other reasons for this (see discussion section). A more appropriate term for the phenomenon is small study effect. If small study effects are present in a meta-analysis, the classical approaches to merge single study results in to an overall intervention effect fails to provide an appropriate estimate of the treatment effect. 

\vspace{0mm}
To provide an overview over the issue, first it is shown how median absolute effect size decreases with increasing sample size of the comparisons in Figure \ref{effect.samplesize}. In some sense, this is the same idea as for a funnel plot, by depicting the size of effects relative to their variance. 

\vspace{0mm}
A clear trend of decrease of absolute effect size with increasing sample size (i.e. smaller variance) is visible. It is particularly substantive from very small trials ($n$ = 10) to medium sample size ($n$ = 100) and afterwards it evens off. With increasing sample size, there are fewer results, therefore, the variation between medians increases. All effects are normalized by subtracting the mean effect size of the dataset and dividing through the standard deviation. Note that various types of outcome measures are included, such as mean difference and risk ratios, and are normalized with respect to all effects.

\begin{figure}
<<echo=FALSE>>=
data %>% filter(total1 + total2 > 10 & total1 + total2 < 200) %>% 
  mutate(sample.size = total1 + total2, scaled.effect = abs(scale(effect, center = T, scale = T))) %>%
  group_by(sample.size) %>% 
  summarize(median.effect = median(scaled.effect, na.rm = T)) %>% 
  ggplot(aes(x = sample.size, y = median.effect)) + geom_point() + theme_bw() + xlab("sample size") + ylab("median absolute normalized effect size")
@
\caption{Median of the absolute value of the normalized effect size plotted against the total sample size.}
\label{effect.samplesize}
\end{figure}

The median absolute normalized effect size can be visualized for the different outcome measures separately. Then it is well visible in Figure \ref{effect.samplesize.separated} that the trend of effect size decrease holds in particualar for risk and odds ratios, while it is way more stable for mean differences and standardized mean differences. Instead of normalizing the effects (i.e. subtracting mean and dividing through the standard deviation) for all effect sizes, the effects are normalized with respect to the effects of the same outcome measures. 


\begin{figure}
<<echo=FALSE>>=
data %>% filter(total1 + total2 > 10 & total1 + total2 < 100) %>% 
  filter(outcome.measure.new == "Risk Ratio" | outcome.measure.new == "Odds Ratio" | outcome.measure.new == "Mean Difference" |
           outcome.measure.new == "Std. Mean Difference") %>% 
  group_by(outcome.measure.new) %>% 
  mutate(sample.size = total1 + total2, scaled.effect = abs(scale(effect, center = T, scale = T))) %>%
  group_by(sample.size, outcome.measure.new) %>% 
  summarize(median.effect = median(scaled.effect, na.rm = T)) %>% 
  ggplot(aes(x = sample.size, y = median.effect, group = outcome.measure.new)) + geom_point() + facet_wrap(~outcome.measure.new) + 
  theme_bw() + xlab("sample size") + ylab("median absolute normalized effect size")
@
\caption{Median of the absolute value of the normalized effect size plotted against the total sample size, separated for outcome measures.}
\label{effect.samplesize.separated}
\end{figure}

A second way to analyse meta-analyses is a cumulative meta-analysis that should reveal shifts in treatment effect sizes over time. This can again be done for the entire dataset, i.e. for all effect estimates and their ordering in time. It is important to scale the effect estimates here with respect to the estimates of the replication studies that are about the same subject, have the same outcome measure, etc. Also, only effect estimates that can be compared to other estimates before are included, i.e. only study results with one or more replica are included. Time needs to be scaled and normalized in order to compare multiple time-trends in effect size to each other and gain insights in the overall trend. This is done in Figure \ref{effects.overtime.separated}, separared for odds rati, risk ratio, and mean and standardized difference outcome measures. In order to reduce the spread over time of effect sizes, only studies between 1970 and 2019 were included, as well as studies with a minimal group size of 12 participants (each group).

\begin{figure}
<<echo=FALSE>>=
data.ext %>% filter(outcome.measure.new == "Mean Difference" | outcome.measure.new == "Std. Mean Difference" | 
                outcome.measure.new == "Risk Ratio" | outcome.measure.new == "Odds Ratio") %>% 
  filter(total1 > 11 & total2 > 11) %>% filter(!is.na(study.year) & study.year > 1970 & study.year < 2019) %>% 
  group_by(meta.id) %>% mutate(n = n()) %>% filter(n > 1) %>% 
  mutate(scaled.effect = abs(scale(effect)), scaled.time = scale(study.year)) %>% 
  ggplot(aes(x = scaled.time, y = scaled.effect)) + geom_point(size = .5, alpha = 0.2)  + facet_wrap(~outcome.measure.new) +
  geom_smooth() + theme_bw() +  ylab("absolute scaled effect")  + ylab("scaled time")
@
\caption{Absolute values of scaled effect sizes over scaled time, separated for outcome measures.}
\label{effects.overtime.separated}
\end{figure}


% It is also possible to look how the amount of meta-analysis with significant pooled effect size estimates changes over time, e.g. mean publication year (of studies included in the meta-analysis). 0.05 is used here as the threshold for $p$-value significance and subsequent null-hypothesis rejection. To constrict random fluctuation in the proportion, only mean publication years with a certain amount of meta-analyses are shown ($n < 180$). The $p$-value of the fixed effects meta analysis model is used to decide about rejection of the null-hypothesis. As Figure \ref{treatment.effect.significance.overtime} shows, the proportion of meta-analyses with significant treatment effect estimates stays roughly constant over time. 
% 
% \begin{figure}
% <<echo=FALSE>>=
% meta %>% filter(mean.publication.year < 2013 & mean.publication.year > 1990) %>% 
%   ggplot(aes(x = mean.publication.year, fill = factor(sig.fixef), stat(count))) + geom_density(na.rm = T, position = "fill") + 
%   labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
% @
% \caption{Proportion of significant treatment effect estimates over time (mean publication year).}
% \label{effect.samplesize}
% \end{figure}
% 



\subsection{Small Study Effect Tests}

<<echo=FALSE, warning=FALSE>>=
#Test Results: Binary
test.bin <- meta.bin %>% ungroup() %>% summarize(egger.test = mean(egger.test),
                                                          schwarzer.test = mean(schwarzer.test),
                                                          rucker.test = mean(rucker.test),
                                                          harbord.test = mean(harbord.test),
                                                          peter.test = mean(peter.test))
test.bin <- test.bin %>% gather(key = "test.type", value = "mean")

p.bin <- meta.bin %>% filter(sig.fixef.bin == 1) %>% ungroup() %>% 
  select(egger.test, schwarzer.test, rucker.test, harbord.test, peter.test) %>% 
  gather(key = "test.type", value = "null.hypothesis") %>%  
  mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
  ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
  theme_bw() + xlab(label = NULL) +
  annotate("text", x = test.bin$test.type, y = 1000, 
           label = paste(round(test.bin$mean, 2)*100, "% rejected"), 
           color = "white")



test.sig.bin <- meta.bin %>% ungroup() %>% summarize(egger.test = mean(egger.test),
                                                              schwarzer.test = mean(schwarzer.test),
                                                              rucker.test = mean(rucker.test),
                                                              harbord.test = mean(harbord.test),
                                                              peter.test = mean(peter.test))
test.sig.bin <- test.sig.bin %>% gather(key = "test.type", value = "mean")

p1 <- meta.bin %>% ungroup() %>% 
select(egger.test, schwarzer.test, rucker.test, harbord.test, peter.test) %>% 
  gather(key = "test.type", value = "null.hypothesis") %>% 
  mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
  ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
  theme_bw() + ggtitle("Significant Pooled Effects")+ xlab(label = NULL) + theme(legend.position="none") +
  annotate("text", x = test.sig.bin$test.type, y = 1750, 
           label = paste(round(test.sig.bin$mean, 2)*100, "% rejected"), 
           color = "white")

test.nonsig.bin <- meta.bin %>% filter(sig.fixef.bin == 0) %>% ungroup() %>% summarize(egger.test = mean(egger.test),
                                                                                                schwarzer.test = mean(schwarzer.test),
                                                                                                rucker.test = mean(rucker.test),
                                                                                                harbord.test = mean(harbord.test),
                                                                                                peter.test = mean(peter.test))
test.nonsig.bin <- test.nonsig.bin %>% gather(key = "test.type", value = "mean")

p2 <- meta.bin %>% 
  filter(sig.fixef.bin == 0) %>% ungroup() %>% 
  select(egger.test, schwarzer.test, rucker.test, harbord.test, peter.test) %>% 
  gather(key = "test.type", value = "null.hypothesis") %>% 
  mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
  ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
  theme_bw() + ggtitle("Non-significant Pooled Effects")+ xlab(label = NULL) +theme(legend.position="none") +
  annotate("text", x = test.nonsig.bin$test.type, y = 650, 
           label = paste(round(test.nonsig.bin$mean, 2)*100, "% rejected"), 
           color = "white")

#Test Results: Continuous
test.cont <- meta.cont %>% ungroup() %>% summarize(egger.test = mean(egger.test),
                                                            begg.test = mean(begg.test),
                                                            thomson.test = mean(thomson.test))

test.cont <- test.cont %>% gather(key = "test.type", value = "mean")

p.cont <- meta.cont %>% ungroup() %>% 
  select(egger.test, thomson.test, begg.test) %>% 
  gather(key = "test.type", value = "null.hypothesis") %>% 
  mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
  ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
  theme_bw() + xlab(label = NULL) +
  annotate("text", x = test.cont$test.type, y = 750, 
           label = paste(round(test.cont$mean, 2)*100, "% rejected"), 
           color = "white")


test.sig.cont <- meta.cont %>% filter(sig.fixef.cont == 1) %>% ungroup() %>% summarize(egger.test = mean(egger.test),
                                                                                                begg.test = mean(begg.test),
                                                                                                thomson.test = mean(thomson.test))

test.sig.cont <- test.sig.cont %>% gather(key = "test.type", value = "mean")

p3 <- meta.cont %>% 
  filter(sig.fixef.cont == 1) %>% ungroup() %>% 
  select(egger.test, thomson.test, begg.test) %>% 
  gather(key = "test.type", value = "null.hypothesis") %>% 
  mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
  ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
  theme_bw() + ggtitle("Significant Pooled Effects")+ xlab(label = NULL) + theme(legend.position="none") +
  annotate("text", x = test.sig.cont$test.type, y = 600, 
           label = paste(round(test.sig.cont$mean, 2)*100, "% rejected"), 
           color = "white")

test.nonsig.cont <- meta.cont %>% filter(sig.fixef.cont == 0) %>% ungroup() %>% summarize(egger.test = mean(egger.test),
                                                                                                   begg.test = mean(begg.test),
                                                                                                   thomson.test = mean(thomson.test))

test.nonsig.cont <- test.nonsig.cont %>% gather(key = "test.type", value = "mean")

p4 <- meta.cont %>% 
  filter(sig.fixef.cont == 0) %>% ungroup() %>% 
  select(egger.test, thomson.test, begg.test) %>% 
  gather(key = "test.type", value = "null.hypothesis") %>% 
  mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
  ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
  theme_bw() + ggtitle("Non-significant Pooled Effects")+ xlab(label = NULL) + theme(legend.position="none") +
  annotate("text", x = test.nonsig.cont$test.type, y = 100, 
           label = paste(round(test.nonsig.cont$mean, 2)*100, "% rejected"), 
           color = "white")


########################################################################################################################################
########################################################################################################################################

#Agreement proportions of publication bias tests:

#Binary:
n.bin <- dim(meta.bin)[1]
meta.bin <- meta.bin %>%ungroup() %>%  mutate(egger.schwarzer = ifelse(egger.test == schwarzer.test, "agree", "disagree"),
                                        egger.peter = ifelse(egger.test == peter.test, "agree", "disagree"),
                                        egger.rucker = ifelse(egger.test == rucker.test, "agree", "disagree"),
                                        egger.harbord = ifelse(egger.test == harbord.test, "agree", "disagree"),
                                        schwarzer.peter = ifelse(schwarzer.test == peter.test, "agree", "disagree"),
                                        schwarzer.rucker = ifelse(schwarzer.test == rucker.test, "agree", "disagree"),
                                        schwarzer.harbord = ifelse(schwarzer.test == harbord.test, "agree", "disagree"),
                                        rucker.peter = ifelse(rucker.test == peter.test, "agree", "disagree"),
                                        rucker.harbord = ifelse(rucker.test == harbord.test, "agree", "disagree"),
                                        harbord.peter = ifelse(harbord.test == peter.test, "agree", "disagree"))

agreement.bin <- meta.bin %>% ungroup() %>% summarise(egger.schwarzer = sum(egger.schwarzer == "agree")/n(),
                                            egger.peter = sum(egger.peter == "agree")/n(),
                                            egger.rucker = sum(egger.rucker == "agree")/n(),
                                            egger.harbord = sum(egger.harbord == "agree")/n(),
                                            schwarzer.peter = sum(schwarzer.peter == "agree")/n(),
                                            schwarzer.rucker = sum(schwarzer.rucker == "agree")/n(),
                                            schwarzer.harbord = sum(schwarzer.harbord == "agree")/n(),
                                            rucker.peter = sum(rucker.peter == "agree")/n(),
                                            harbord.peter = sum(harbord.peter == "agree")/n())

correlation.bin <- meta.bin %>%ungroup() %>%  summarise(egger.schwarzer = cor(pval.egger.bin, pval.schwarzer.bin),
                                              egger.peter = cor(pval.egger.bin, pval.peter.bin),
                                              egger.rucker = cor(pval.egger.bin, pval.rucker.bin),
                                              egger.harbord = cor(pval.egger.bin, pval.harbord.bin),
                                              schwarzer.peter = cor(pval.schwarzer.bin, pval.peter.bin),
                                              schwarzer.rucker = cor(pval.schwarzer.bin, pval.rucker.bin),
                                              schwarzer.harbord = cor(pval.schwarzer.bin, pval.harbord.bin),
                                              rucker.peter = cor(pval.rucker.bin, pval.peter.bin),
                                              harbord.peter = cor(pval.harbord.bin, pval.peter.bin))

binary.tests.agreement <- rbind(agreement.bin, correlation.bin)
rownames(binary.tests.agreement) <- c("Test Agreement","P-value Correlation")


#Continuous:
n.cont <- dim(meta.cont)[1]
meta.cont <- meta.cont %>% ungroup() %>% mutate(thomson.egger = ifelse(thomson.test == egger.test, "agree", "disagree"),
                                          thomson.begg = ifelse(thomson.test == begg.test, "agree", "disagree"),
                                          egger.begg = ifelse(egger.test == begg.test, "agree", "disagree"))

agreement.cont <- meta.cont %>% ungroup() %>%  summarise(thomson.egger = sum(thomson.egger == "agree")/n(),
                                              thomson.begg = sum(thomson.begg == "agree")/n(),
                                              egger.begg = sum(egger.begg == "agree")/n())

correlation.cont <- meta.cont %>% ungroup() %>% summarise(thomson.egger = cor(pval.thomson.cont, pval.egger.cont),
                                                thomson.begg = cor(pval.thomson.cont, pval.begg.cont),
                                                egger.begg = cor(pval.egger.cont, pval.begg.cont))

cont.tests.agreement <- rbind(agreement.cont, correlation.cont)
rownames(cont.tests.agreement) <- c("Test Agreement","P-value Correlation")

########################################################################################################################################
########################################################################################################################################


#Trimfill missing studies fractions statistics:
trimfill.cont.mean <- meta.cont %>% ungroup %>% summarise(mean = mean(missing.trim.cont)) %>% select(mean)
trimfill.cont.median <- meta.cont %>% ungroup %>% summarise(median = median(missing.trim.cont)) %>% select(median)

trimfill.bin.mean <- meta.bin %>% ungroup %>% summarise(mean = mean(missing.trim.bin)) %>% select(mean)
trimfill.bin.median <- meta.bin %>% ungroup %>% summarise(median = median(missing.trim.bin)) %>% select(median)

########################################################################################################################################
########################################################################################################################################

change.ranef.bin <- meta.bin %>% ungroup() %>% select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>% mutate(sig.change = factor(sig.change)) %>% group_by(sig.change, correction.method) %>% count()

p.change.ranef.bin <- meta.bin %>% ungroup() %>% 
  select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>%  
  mutate(sig.change = factor(sig.change)) %>% 
  ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Binary outcomes")+ 
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged.nonsig", "change.to.sig", "unchanged.sig", "change.to.nonsig")) +
  annotate("text", x = change.ranef.bin[change.ranef.bin$sig.change == 3,]$correction.method, y = 1000, 
         label = paste(change.ranef.bin[change.ranef.bin$sig.change == 3,]$n, "changed to nonsig."), 
         color = "white")

change.ranef.cont <- meta.cont %>% ungroup() %>% select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>% group_by(sig.change, correction.method) %>% count()

p.change.ranef.cont <- meta.cont %>% ungroup() %>% 
  select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>%  
  mutate(sig.change = factor(sig.change)) %>% 
  ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Continuous outcomes") +
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged.nonsig", "change.to.sig", "unchanged.sig", "change.to.nonsig")) +
  annotate("text", x = change.ranef.cont[change.ranef.cont$sig.change == 3,]$correction.method, y = 500, 
         label = paste(change.ranef.cont[change.ranef.cont$sig.change == 3,]$n, "changed to nonsig."), 
         color = "white")

change.ranef <- meta %>% ungroup() %>% select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>% group_by(sig.change, correction.method) %>% count()

p.ranef.change <- meta %>% ungroup() %>% 
  select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>%  
  mutate(sig.change = factor(sig.change)) %>% 
  ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + 
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged.nonsig", "change.to.sig", "unchanged.sig", "change.to.nonsig")) +
  annotate("text", x = change.ranef[change.ranef$sig.change == 3,]$correction.method, y = 1500, 
         label = paste(change.ranef[change.ranef$sig.change == 3,]$n, "changed to nonsig."), 
         color = "white")

#Fixed effects meta-analysis significance
change.fixef.bin <- meta.bin %>% ungroup() %>% select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>% group_by(sig.change, correction.method) %>% count()

p.change.fixef.bin <- meta.bin %>% ungroup() %>% 
  select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>%  
  mutate(sig.change = factor(sig.change)) %>% 
  ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Binary outcomes")+  
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged.nonsig", "change.to.sig", "unchanged.sig", "change.to.nonsig")) +
  annotate("text", x = change.fixef.bin[change.fixef.bin$sig.change == 3,]$correction.method, y = 800, 
         label = paste(change.fixef.bin[change.fixef.bin$sig.change == 3,]$n, "changed to nonsig."), 
         color = "white")

change.fixef.cont <- meta.cont %>% ungroup() %>% select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>% group_by(sig.change, correction.method) %>% count()

p.change.fixef.cont <- meta.cont %>% ungroup() %>% 
  select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>%  
  mutate(sig.change = factor(sig.change)) %>% 
  ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Continuous outcomes")+ 
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged.nonsig", "change.to.nonsig", "unchanged.sig", "change.to.sig")) +
  annotate("text", x = change.fixef.cont[change.fixef.cont$sig.change == 3,]$correction.method, y = 300, 
         label = paste(change.fixef.cont[change.fixef.cont$sig.change == 3,]$n, "changed to nonsig."), 
         color = "white")

change.fixef <- meta %>% ungroup() %>% select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>% group_by(sig.change, correction.method) %>% count()

p.fixef.change <- meta %>% ungroup() %>% 
  select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
  gather(key = "correction.method", value = "sig.change") %>%  
  mutate(sig.change = factor(sig.change)) %>% 
  ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + 
  theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged.nonsig", "change.to.nonsig", "unchanged.sig", "change.to.sig")) +
  annotate("text", x = change.fixef[change.fixef$sig.change == 3,]$correction.method, y = 1500, 
         label = paste(change.fixef[change.fixef$sig.change == 3,]$n, "changed to nonsig."), 
         color = "white")
  
########################################################################################################################################
########################################################################################################################################

p.missing.copas <- meta %>% ggplot(aes(x = missing.copas/n)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20)
p.missing.trim <- meta %>% ggplot(aes(x = missing.trim)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20)
@

There are tests that can be applied to find out if small study effects are present in the meta analysis. For the precise description, see the methods section. Application of the tests is only recommended if there are ten or more studies \citep{cochrane.handbook} that can be used, so all meta-analyses with less than ten studies have been excluded.

\vspace{0mm}
There are modifications to make tests more appropriate in case of binary outcomes, therefore the results have been separated in continuous and dichotomous outcome test results. In Figure \ref{bias.results.cont} the proportion of test results that led to rejection of the null hypothesis of no small study effect based on the 5 \% level are shown for continuous outcomes ($n$ = \Sexpr{n.cont})
The same is shown in Figure \ref{bias.results.bin} for dichotomous outcome measures ($n$ = \Sexpr{n.bin}).


\begin{figure}
<<echo=FALSE, warning=FALSE>>=
plot(p.cont)
@
\caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (continuous outcomes).}
\label{bias.results.cont}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
plot(p.bin)
@
\caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (dichotomous outcomes).}
\label{bias.results.bin}
\end{figure}

The same plots can be shown separately for meta-analyses with significant and non-significant pooled effect sizes. This is done in Figure \ref{bias.results.cont.sep} for continuous outcomes and \ref{bias.results.bin.sep} for binary outcomes.

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
grid.arrange(p3, p4, ncol = 2)
@
\caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (continuous outcomes).}
\label{bias.results.cont.sep}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
grid.arrange(p1, p2, ncol = 2)
@
\caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (dichotomous outcomes).}
\label{bias.results.bin.sep}
\end{figure}

Furthermore one can look if the frequencies of tests that reject the null hypotheses change over time (mean publication year of the studies included in the meta analyses). The proportion of the test results are shown in Figure \ref{pub.bias.time.overtime}. The Figure suggests that the frequency of publication bias remains constant over time. The mean publication years have been restricted such that at least 180 meta-analyses are available per year such that random fluctuation is restricted to some extent. The significance threshold for the $p$-values used is 0.05, and the small study effect test used is Thomson's test (with the arcsine variance stabilizing transformation function used in the case of binary outcomes).

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
meta %>% filter(mean.publication.year < 2013 & mean.publication.year > 1990) %>% 
  ggplot(aes(x = mean.publication.year, fill = factor(thomson.test), stat(count))) + geom_density(na.rm = T, position = "fill") + 
  labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
@
\caption{Proportion of test results where the null hypothesis of no small study effect is rejected over time (mean publication.year).}
\label{pub.bias.overtime}
\end{figure}


The agreement of the tests, i.e. the proportion of meta-analyses where the rest results are equal between tests, is shown in Table \ref{agreement.bin} and Table \ref{agreement.cont}, again separated for outcome types. Agreement in tests for binary outcomes is better than continuous outcomes, with some variation between tests (binary outcomes: 83 to 91\%). Correlation varies more between tests, both for continuous and binary outcome tests.

<<results = 'asis', echo = FALSE, warning = FALSE>>=
print(xtable(x = t(binary.tests.agreement), caption = "Proportion of tests that agree in rejection ar acceptance of the null hypothesis that there is no small study effect (dichotomous outcomes)", label = "agreement.bin",  align = "lll", digits = 2), include.rownames = T, size = "footnotesize")
@


<<results = 'asis', echo = FALSE, warning = FALSE>>=
print(xtable(x = t(cont.tests.agreement), caption = "Proportion of tests that agree in rejection ar acceptance of the null hypothesis that there is no small study effect (continuous outcomes)", label = "agreement.cont",  align = "lll", digits = 2), include.rownames = T, size = "footnotesize")
@

\vspace{0mm}
Test performance depends on the sample size, despite having restricted sample size to a minimum of 10 studies. The p-values of the Thompson and Sharp tests are shown with respect to the sample size of the meta-analysis in Figure \ref{pvalues.samplesize}. %(Suggests that 10 is likely too small)
In the case of binary outcomes, the arcsine variance stabilizing function has been applied prior to use of Thompson and Sharp's test. A s trend towards more rejections for larger sample sizes can be seen.

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
meta %>% filter(n < 40) %>% ggplot(aes(stat(count), x = n, fill = factor(thomson.test))) + 
  geom_density(position = "fill")
@
\caption{P-values of Thomson and Sharp's test for small study effects and their corresponding sample size.}
\label{pvalues.samplesize}
\end{figure}



\vspace{0mm}
One can use the proportion of added studies by the trim-and-fill method from the overall number of studies to further investigate the extent of small study effects. The mean fraction of trimmed comparisons for binary outcomes is \Sexpr{round(trimfill.bin.mean,2)} and the median \Sexpr{round(trimfill.bin.median,2)}. 
% A histogram with those fractions is shown in Figure \ref{trimfill.cont} for continuous outcomes and \ref{trimfill.bin} for dichotomous outcomes. In both cases, the method very commonly adds supposedly unpublished studies to the meta-analyses. 
In Figure \ref{trimfill.pvalues.bin} and Figure \ref{trimfill.pvalues.cont}, the relationship between fraction of added studies by trim-and-fill and the hypothesis test decisions of the small study effects tests is shown for continuous and dichotomous outcomes. In the case of Peters test dor dichotomous outcomes, there is less agreement with the trim-and fill method than in the case of Thomson and Sharp's test for continuos outcomes in the sense that the fraction of meta-analyses with rejected null hypothesises increases more clearly when there are more studies added by trim-and-fill. 

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
meta.bin %>% filter(missing.trim.bin < 0.5) %>% 
  ggplot(aes(x = missing.trim.bin, fill = factor(peter.test), stat(count))) + 
  geom_density(na.rm = T, position = "fill") + 
  labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
@
\caption{Fraction of added studies for small study effect correction by trim and fill and the corresponding small study effect test decision (based on 5\% significance level) of Peters test and dichotomous outcomes}
\label{trimfill.pvalues.bin}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
meta.cont %>% filter(missing.trim.cont < 0.5) %>% 
  ggplot(aes(x = missing.trim.cont, fill = factor(thomson.test), stat(count))) + 
  geom_density(na.rm = T, position = "fill") + 
  labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
@
\caption{Fraction of added studies for small study effect correction by trim and fill and the corresponding small study effect test decision (based on 5\% significance level) of Thomson and Sharp's test and continuous outcomes}
\label{trimfill.pvalues.cont}
\end{figure}


\subsection{Small Study Effect Correction}
Multiple methods are available to correct for the effects of small study effects in order to get an unbiased estimate. Three of them will be applied to the meta-analyses shown previously that have ten or more study results and are therefore eligible for testing for publication bias. 

\vspace{0mm}
The extent to what the results of the meta-analysis results are changed can be investigated. Because statistical significance is often used to decide if there is a treatment effect, a non-significant corrected effect size estimate can indicate that an observed treatment effect has been accepted because of small study effects. Therefore, the cases have been counted in which 
\begin{enumerate}
\item Significance or non-significance of pooled estimate of meta-analysis did not change after correction for small study effects.
\item Significance of pooled estimate of meta-analysis did change to non-significance after correction for small study effects.
\item Non-significance of pooled estimate of meta-analysis did change to significance after correction for small study effects.
\end{enumerate}

The results of this can be seen in Figure \ref{significance.change.fixed} for all three methods, comparing the significance of the corrected pooled effect size estimate with the significance of the pooled effect size estimate of the fixed effects meta-analysis. The same for significance of random effects meta-analysis is shown in Figure \ref{significance.change.random}. The significance threshold was chosen such that the $p$-value had to be $< 0.05$ for rejection of the null hypothesis of no treatment effect. The correction methods were trim-and-fill, copas selection model and regression with random effects and shrinkage of within-study-variance methods. More details to the applied correction methods and their application are in the methods section \ref{methods}. Notably, the correction methods has been applied to all meta-analyses, thus also for such that had no significant small study effect test result. 


\begin{figure}
<<echo=FALSE, warning=FALSE>>=
plot(p.fixef.change)
@
\caption{Change in signficance of fixed effects meta-analysis pooled estimate after correction.}
\label{significance.change.fixed}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
plot(p.ranef.change)
@
\caption{Change in signficance of random effects meta-analysis pooled estimate after correction.}
\label{significance.change.random}
\end{figure}

Since it has been previously seen in Figure \ref{test.results} that the results of small study effects vary considerably between continuous outcomes, the results in significance change from fixed effects meta-analysis can be seen separately in Figure \ref{significance.change.fixed.sep} for continuous and binary outcomes. The change in significance from random effects meta-analysis to significance of corrected estimate can be seen in Figure \ref{significance.change.fixed.sep}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
grid.arrange(p.change.fixef.bin, p.change.fixef.cont, ncol = 1)
@
\caption{Change in signficance of fixed effects meta-analysis pooled estimate after correction, separated for continuous and binary outcomes.}
\label{significance.change.fixed.sep}
\end{figure}

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
grid.arrange(p.change.ranef.bin, p.change.ranef.cont, ncol = 1)
@
\caption{Change in signficance of random effects meta-analysis pooled estimate after correction, separated for continuous and binary outcomes.}
\label{significance.change.random}
\end{figure}

Similarly, the number of missing studies per meta-analysis, i.e. those which have not been included because of small study effects, are estimated by the copas and trim-and-fill method and their empirical distribution is shown in histograms in Figure \ref{missing.studies.distribution}. For visualisation, the fraction of unpublished studies from the total fraction of available studies is shown.

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
grid.arrange(p.missing.copas, p.missing.trim, ncol = 1)
@
\caption{Fraction of missing studies estimated of the number of total studies included in the meta-analysis for copas selection and trim-and-fill method.}
\label{missing.studies.distribution}
\end{figure}








% # <<results = 'asis', Echo = FALSE>>=
% # rejection.bin <- meta.bin %>% summarize(egger.rejection = mean(egger.test),
% #                                             schwarzer.rejection = mean(schwarzer.test),
% #                                             rucker.rejection = mean(rucker.test),
% #                                             harbord.rejection = mean(harbord.test),
% #                                             peter.rejection = mean(peter.test))
% # print(xtable(rejection.bin), label = "bias.results", caption = "Cumulative number of groups with number of reproduction trials >= n", align = "llll", digits = 0), include.rownames = F, size = "footnotesize")
% # @






% 
% For continuous outcomes, three tests are available: Eggers (based on linear regression), Thompson and Sharp (weighted linear regression) and Begg and Mazumdar (rank based) test. The following three figures show the distribution of p-values of the corresponding tests. Note that only meta analyses with more than 10 comparisons have been included. 
% 
% \vspace{0mm}
% Since each histogram of p-values has 20 bins, the content of the bin with the smallest p-values is equal to the number of meta-analyses whose reporting bias test reports a p-value < 0.05. The fraction of those analyses in which we would reject the null-hypothesis based on the 5 \% threshold can therefore be assessed by eye, and would be for example for Eggers test somewhat less than one third of all analyses. 
% 
% \begin{figure}
% <<echo=FALSE>>=
% data %>% filter(outcome.measure == "Mean Difference" | outcome.measure == "Std. Mean Difference") %>% #filter(file.nr < 503) %>% 
%   filter(sd1 > 0 & sd2 > 0 ) %>% filter(!is.na(sd1) & !is.na(sd2)) %>% 
%   filter(mean1 != 0 | mean2 != 0 ) %>% filter(!is.na(mean1) & !is.na(mean2)) %>% 
%   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
%   mutate(n = n()) %>% filter(n > 9) %>% 
%   summarize(pval = metabias(metacont(n.e = total1, mean.e = mean1, sd.e = sd1, n.c = total2, mean.c = mean2, sd.c = sd2), 
%                             method = "linreg")$p.val) %>% 
%   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
%   theme_bw() + labs(title = "Eggers Reporting Bias Test P-values for Continuous Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% @
% \caption{Histogram of p-values for Eggers reporting bias test (linear regression based) for continuous outcome meta analysis.}
% \label{egger.cont}
% \end{figure}
% 
% 
% \begin{figure}
% <<echo=FALSE>>=
% data %>% filter(outcome.measure == "Mean Difference" | outcome.measure == "Std. Mean Difference") %>% #filter(file.nr < 503) %>% 
%   filter(sd1 > 0 & sd2 > 0 ) %>% filter(!is.na(sd1) & !is.na(sd2)) %>% 
%   filter(mean1 != 0 | mean2 != 0 ) %>% filter(!is.na(mean1) & !is.na(mean2)) %>% 
%   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
%   mutate(n = n()) %>% filter(n > 9) %>% 
%   summarize(pval = metabias(metacont(n.e = total1, mean.e = mean1, sd.e = sd1, n.c = total2, mean.c = mean2, sd.c = sd2), 
%                             method = "mm")$p.val) %>% 
%   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
%   theme_bw() + labs(title = "Thomson Sharp Reporting Bias Test P-values for Continuous Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% @
% \caption{Histogram of p-values for Thompsom and Sharp reporting bias test (weighted linear regression based) for continuous outcome meta analysis.}
% \label{thomson.cont}
% \end{figure}
% 
% \begin{figure}
% <<echo=FALSE>>=
% data %>% filter(outcome.measure == "Mean Difference" | outcome.measure == "Std. Mean Difference") %>% #filter(file.nr < 503) %>% 
%   filter(sd1 > 0 & sd2 > 0 ) %>% filter(!is.na(sd1) & !is.na(sd2)) %>% 
%   filter(mean1 != 0 | mean2 != 0 ) %>% filter(!is.na(mean1) & !is.na(mean2)) %>% 
%   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
%   mutate(n = n()) %>% filter(n > 9) %>% 
%   summarize(pval = metabias(metacont(n.e = total1, mean.e = mean1, sd.e = sd1, n.c = total2, mean.c = mean2, sd.c = sd2), 
%                             method = "rank")$p.val) %>% 
%   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
%   theme_bw() + labs(title = "Begg and Mazumdar Reporting Bias Test P-values for Continuous Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% @
% \caption{Histogram of p-values for Begg and Mazumdar reporting bias test (rank based) for continuous outcome meta analysis.}
% \label{begg.cont}
% \end{figure}
% 
% 
% For binary outcomes, Peters and Harbords reporting bias test have been chosen. Also here, only meta-analyses with more than 10 comparisons are included.
% 
% \begin{figure}
% <<echo=FALSE>>=
% data %>% filter(outcome.measure == "Risk Ratio" | outcome.measure == "Odds Ratio") %>% filter(file.nr != 3014) %>% 
%   filter(events1 > 0 | events2 > 0) %>% 
%   filter(total1 - events1 > 0 | total2 - events2 > 0) %>%
%   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
%   mutate(n = n()) %>% filter(n > 9) %>% 
%   summarize(pval = metabias(metabin(event.e = events1, n.e = total1, event.c = events2, n.c = total2, sm = "OR"), method = "peters")$p.val) %>% 
%   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
%   theme_bw() + labs(title = "Peters Reporting Bias Test P-values for Binary Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% @
% \caption{Histogram of p-values for Peters reporting bias test (rank based) for continuous outcome meta analysis.}
% \label{peters.bin}
% \end{figure}
% 
% 
% \begin{figure}
% <<echo=FALSE>>=
% data %>% filter(outcome.measure == "Risk Ratio" | outcome.measure == "Odds Ratio") %>% filter(file.nr != 3014) %>% 
%   filter(events1 > 0 | events2 > 0) %>% 
%   filter(total1 - events1 > 0 | total2 - events2 > 0) %>%
%   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
%   mutate(n = n()) %>% filter(n > 9) %>% 
%   summarize(pval = metabias(metabin(event.e = events1, n.e = total1, event.c = events2, n.c = total2, sm = "OR"), method = "score")$p.val) %>% 
%   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
%   theme_bw() + labs(title = "Harbord Reporting Bias Test P-values for Binary Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% @
% \caption{Histogram of p-values for Harbord reporting bias test (rank based) for continuous outcome meta analysis.}
% \label{harbord.bin}
% \end{figure}

