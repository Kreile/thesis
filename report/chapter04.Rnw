% LaTeX file for Chapter 03


<<'preamble03',include=FALSE, echo=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figure/ch03_fig',
               echo=TRUE, message=FALSE,
               fig.width=8, fig.height=3,
               self.contained = FALSE,
               out.width='\\textwidth-3cm',
               message=FALSE, fig.align='center',
               background="gray98", tidy=FALSE, #tidy.opts=list(width.cutoff=60),
               cache=TRUE
)
options(width=74)
@

<<'Data03', echo=FALSE, warning=FALSE>>= 
#Load data:
rm(list = ls())
PATH_HOME = path.expand("~") # user home
PATH = file.path(PATH_HOME, 'Data/PubBias')
PATH2 = file.path(PATH_HOME, 'PubBias')
FILE = 'cochrane_2019-07-04.csv'
PATH_DATA = file.path(PATH, 'data')
PATH_CODE = file.path(PATH2, 'code')
PATH_RESULTS = file.path(PATH2, 'results_new')
# PATH_FIGURES = file.path(PATH_RESULTS, 'figures')

source(file.path(PATH_CODE, 'PubBias_functions.R'))
require(corrgram)

load(file.path(PATH_DATA, "PubBias_2019-07-19.RData"))
load(file.path(PATH_RESULTS, "data_used_for_analysis.RData"))
load(file.path(PATH_RESULTS, "meta_analyses_summary_complete.RData"))
load(file.path(PATH_RESULTS, "meta_id_vector.RData"))
load(file.path(PATH_RESULTS, "meta_id_I2.RData"))
load(file.path(PATH_RESULTS, "meta.bin.RData"))
load(file.path(PATH_RESULTS, "meta.cont.RData"))
load(file.path(PATH_RESULTS, "meta.iv.RData"))



@


\chapter{Results} \label{ch:Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<echo=FALSE, warning=FALSE>>=
p.samplesize.z  <- data.ext2 %>% filter(total1 + total2 > 15 & total1 + total2 < 300) %>% ungroup() %>%
  mutate(std.z = abs(z), sample.size = total1 + total2) %>%
  group_by(sample.size) %>%
  summarize(median.effect = median(std.z, na.rm = T)) %>%
  ggplot(aes(x = sample.size, y = median.effect)) + geom_point(size = 0.8) +
  theme_bw() + xlab("sample size") + ylab("median absolute z-score") +
  xlim(c(15, 300)) + ylim(c(0, 0.25)) +
  scale_x_continuous(limits = c(15, 300), breaks = c(15, 100, 200, 300))

p.samplesize.effect.sep <- data %>% filter(total1 + total2 > 15 & total1 + total2 < 300) %>%
  filter(outcome.measure.new == "Risk Ratio" |
           outcome.measure.new == "Odds Ratio" |
           outcome.measure.new == "Mean Difference" |
           outcome.measure.new == "Std. Mean Difference") %>%
  group_by(outcome.measure.new) %>%
  mutate(sample.size = total1 + total2, scaled.effect = abs(scale(effect, center = T, scale = T))) %>%
  group_by(sample.size, outcome.measure.new) %>%
  summarize(median.effect = median(scaled.effect, na.rm = T)) %>%
  ggplot(aes(x = sample.size, y = median.effect, group = outcome.measure.new)) + geom_point(size = .5) + facet_wrap(~outcome.measure.new, scales = "free") +
  theme_bw() + xlab("sample size") + ylab("median absolute normalized effect size") +
  xlim(c(15, 300)) +
  scale_x_continuous(limits = c(15, 300), breaks = c(15, 100, 200, 300))
@

The median $z$-score for a given sample size of a trial is shown in Figure \ref{z.samplesize}. It is clearly visible that the absolute value decreases with increasing sample size, i.e. that the effect size is becoming smaller. The trend flattens off after $\sim$ sample size = 400 (not shown).
-- Appendix -- \\

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
plot(p.samplesize.z)
@
\caption{Median of the absolute $z$-score plotted against the total sample size.}
\label{z.samplesize}
\end{figure}

Only for illustration, the same trend is reproduced in Figure \ref{effect.samplesize.separated} with a similar method, using the original effect size measures ``Odds Ratio'', ``Risk Ratio'', ``Mean Difference'' and ``Std. Mean Difference'' (the most commonly used in the dataset). The ``normalized effect'' is the original effect size, normalized with respect to all other effect sizes of the same measure (i.e. subtraction of mean and division through standard error of the mean).

\begin{figure}
<<echo=FALSE>>=
plot(p.samplesize.effect.sep)
@
\caption{Median of the absolute value of the normalized, original effect size plotted against the total sample size.}
\label{effect.samplesize.separated}
\end{figure}



\section{Publication Bias Test Results} \label{sec:publication.bias.tests}
<<echo=FALSE, warning=FALSE>>=
#Agreement proportions of publication bias tests:

#Binary:
meta.bin <- meta.bin %>%ungroup() %>%  mutate(tes.d.schwarzer = ifelse(tes.d.test == schwarzer.test, "agree", "disagree"),
                                              tes.d.peter = ifelse(tes.d.test == peter.test, "agree", "disagree"),
                                              tes.d.rucker = ifelse(tes.d.test == rucker.test, "agree", "disagree"),
                                              tes.d.harbord = ifelse(tes.d.test == harbord.test, "agree", "disagree"),
                                              schwarzer.peter = ifelse(schwarzer.test == peter.test, "agree", "disagree"),
                                              schwarzer.rucker = ifelse(schwarzer.test == rucker.test, "agree", "disagree"),
                                              schwarzer.harbord = ifelse(schwarzer.test == harbord.test, "agree", "disagree"),
                                              rucker.peter = ifelse(rucker.test == peter.test, "agree", "disagree"),
                                              rucker.harbord = ifelse(rucker.test == harbord.test, "agree", "disagree"),
                                              harbord.peter = ifelse(harbord.test == peter.test, "agree", "disagree"))
agreement.bin <- meta.bin %>% ungroup() %>% summarise(tes.d.schwarzer = sum(tes.d.schwarzer == "agree")/n(),
                                                      tes.d.peter = sum(tes.d.peter == "agree")/n(),
                                                      tes.d.rucker = sum(tes.d.rucker == "agree")/n(),
                                                      tes.d.harbord = sum(tes.d.harbord == "agree")/n(),
                                                      schwarzer.peter = sum(schwarzer.peter == "agree")/n(),
                                                      schwarzer.rucker = sum(schwarzer.rucker == "agree")/n(),
                                                      schwarzer.harbord = sum(schwarzer.harbord == "agree")/n(),
                                                      rucker.peter = sum(rucker.peter == "agree")/n(),
                                                      harbord.peter = sum(harbord.peter == "agree")/n())
meta.bin <- meta.bin %>%ungroup() %>%  mutate(tes.d.schwarzer = ifelse(tes.d.test + schwarzer.test > 1, "agree", "disagree"),
                                              tes.d.peter = ifelse(tes.d.test + peter.test > 1, "agree", "disagree"),
                                              tes.d.rucker = ifelse(tes.d.test + rucker.test > 1, "agree", "disagree"),
                                              tes.d.harbord = ifelse(tes.d.test + harbord.test > 1, "agree", "disagree"),
                                              schwarzer.peter = ifelse(schwarzer.test + peter.test > 1, "agree", "disagree"),
                                              schwarzer.rucker = ifelse(schwarzer.test + rucker.test > 1, "agree", "disagree"),
                                              schwarzer.harbord = ifelse(schwarzer.test + harbord.test > 1, "agree", "disagree"),
                                              rucker.peter = ifelse(rucker.test + peter.test > 1, "agree", "disagree"),
                                              rucker.harbord = ifelse(rucker.test + harbord.test > 1, "agree", "disagree"),
                                              harbord.peter = ifelse(harbord.test + peter.test > 1, "agree", "disagree"))
agreement.sig.bin <- meta.bin %>% ungroup() %>% summarise(tes.d.schwarzer = sum(tes.d.schwarzer == "agree")/sum(tes.d.test),
                                                      tes.d.peter = sum(tes.d.peter == "agree")/sum(tes.d.test),
                                                      tes.d.rucker = sum(tes.d.rucker == "agree")/sum(tes.d.test),
                                                      tes.d.harbord = sum(tes.d.harbord == "agree")/sum(tes.d.test),
                                                      schwarzer.peter = sum(schwarzer.peter == "agree")/sum(schwarzer.test),
                                                      schwarzer.rucker = sum(schwarzer.rucker == "agree")/sum(schwarzer.test),
                                                      schwarzer.harbord = sum(schwarzer.harbord == "agree")/sum(schwarzer.test),
                                                      rucker.peter = sum(rucker.peter == "agree")/sum(rucker.test),
                                                      harbord.peter = sum(harbord.peter == "agree")/sum(peter.test))

binary.tests.agreement <- rbind(agreement.bin, agreement.sig.bin)
rownames(binary.tests.agreement) <- c("Agreement (overall)", "Agreement (significance)")
colnames(binary.tests.agreement) <- c("Excess significance, Schwarzer", "Excess significance, Peter",
                                      "Excess significance, Rucker", "Excess significance, Harbord",
                                      "Peter, Schwarzer", "Schwarzer, Rucker", "Schwarzer, Harbord",
                                      "Rucker, Peter", "Harbord, Peter")
#--------------------------------------------------------------------------------------------------------------------#


#Continuous:
meta.cont <- meta.cont %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test == egger.test, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test == begg.test, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test == begg.test, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test == egger.test, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test ==tes.d.test, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test == begg.test, "agree", "disagree"))
agreement.cont <- meta.cont %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/n(),
                                                         thompson.tes.d = sum(thompson.tes.d == "agree")/n(),
                                                         tes.d.begg = sum(tes.d.begg == "agree")/n(),
                                                         thompson.egger = sum(thompson.egger == "agree")/n(),
                                                         thompson.begg = sum(thompson.begg == "agree")/n(),
                                                         egger.begg = sum(egger.begg == "agree")/n())
meta.cont <- meta.cont %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test + egger.test > 1, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test + begg.test > 1, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test + begg.test > 1, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test + egger.test > 1, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test +tes.d.test > 1, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test + begg.test > 1, "agree", "disagree"))
agreement.sig.cont <- meta.cont %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/sum(tes.d.test),
                                                             thompson.tes.d = sum(thompson.tes.d == "agree")/sum(tes.d.test),
                                                             tes.d.begg = sum(tes.d.begg == "agree")/sum(tes.d.test),
                                                             thompson.egger = sum(thompson.egger == "agree")/sum(thompson.test),
                                                             thompson.begg = sum(thompson.begg == "agree")/sum(begg.test),
                                                             egger.begg = sum(egger.begg == "agree")/sum(begg.test))



cont.tests.agreement <- rbind(agreement.cont, agreement.sig.cont)
rownames(cont.tests.agreement) <- c("Agreement (overall)", "Agreement (significance)")
colnames(cont.tests.agreement) <- c( "Excess significance, Egger", "Excess significance, Thompson",
                                     "Excess significance, Begg","Thompson, Egger", "Thompson, Begg", "Egger, Begg")

#--------------------------------------------------------------------------------------------------------------------#


#IV:
meta.iv <- meta.iv %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test == egger.test, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test == begg.test, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test == begg.test, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test == egger.test, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test ==tes.d.test, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test == begg.test, "agree", "disagree"))
agreement.iv <- meta.iv %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/n(),
                                                         thompson.tes.d = sum(thompson.tes.d == "agree")/n(),
                                                         tes.d.begg = sum(tes.d.begg == "agree")/n(),
                                                         thompson.egger = sum(thompson.egger == "agree")/n(),
                                                         thompson.begg = sum(thompson.begg == "agree")/n(),
                                                         egger.begg = sum(egger.begg == "agree")/n())
meta.iv <- meta.iv %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test + egger.test > 1, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test + begg.test > 1, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test + begg.test > 1, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test + egger.test > 1, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test +tes.d.test > 1, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test + begg.test > 1, "agree", "disagree"))
agreement.sig.iv <- meta.iv %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/sum(egger.test),
                                                             thompson.tes.d = sum(thompson.tes.d == "agree")/sum(thompson.test),
                                                             tes.d.begg = sum(tes.d.begg == "agree")/sum(begg.test),
                                                             thompson.egger = sum(thompson.egger == "agree")/sum(thompson.test),
                                                             thompson.begg = sum(thompson.begg == "agree")/sum(begg.test),
                                                             egger.begg = sum(egger.begg == "agree")/sum(begg.test))

iv.tests.agreement <- rbind(agreement.iv, agreement.sig.iv)
rownames(iv.tests.agreement) <- c("Agreement (overall)","Agreement (significance)")
colnames(iv.tests.agreement) <- c("Excess significance, Egger (IV)", "Excess significance, Thompson (IV)",
                                    "Excess significance, Begg (IV)", "Thompson, Egger (IV)",
                                    "Thompson, Begg (IV)", "Egger, Begg (IV)")

#--------------------------------------------------------------------------------------------------------------------#

#Merging:
test.agreement <- rbind(t(binary.tests.agreement), t(cont.tests.agreement), t(iv.tests.agreement))
#--------------------------------------------------------------------------------------------------------------------#

sig.level <- 0.1
meta.f <- meta.f %>% rowwise() %>% 
  mutate(i2f = factor(ifelse(I2 == 0, 1, 0)))

meta.bin <- meta.f %>% filter(outcome.flag == "DICH")
meta.cont <- meta.f %>% filter(outcome.flag == "CONT")
meta.iv <- meta.f %>% filter(outcome.flag == "IV")



dat_text <- data.frame(
  label = paste(c(sum(meta.bin$harbord.test), sum(meta.bin$peter.test), 
                  sum(meta.bin$rucker.test), sum(meta.bin$schwarzer.test), sum(meta.bin$tes.d.test, na.rm = T)), "< 0.1,",
                round(c(mean(meta.bin$harbord.test), mean(meta.bin$peter.test), 
                        mean(meta.bin$rucker.test), mean(meta.bin$schwarzer.test), mean(meta.bin$tes.d.test, na.rm = T)),2)*100, "%"),
  test.type   = c("pval1.harbord", "pval1.peter", "pval1.rucker", "pval1.schwarzer", "pval1.d.tes"),
  i2f = factor(rep(0, times = 5)))

labels <- c(pval1.harbord = "Harbord", pval1.peter = "Peter", pval1.rucker = "Rucker", pval1.schwarzer = "Schwarzer", pval1.d.tes = "Excess Significance")

p.dist.bin <- meta.bin %>% ungroup() %>% 
  select(i2f, pval1.harbord, pval1.peter, pval1.rucker, pval1.d.tes, pval1.schwarzer) %>% 
  gather(key = "test.type", value = "p.value", pval1.harbord:pval1.schwarzer) %>% 
  ggplot(aes(x = p.value, fill = i2f)) + geom_histogram(boundary = 0, binwidth = 0.1) + theme_bw() + 
  facet_wrap(~test.type, labeller = labeller(test.type = labels)) + 
  geom_text(data = dat_text, mapping = aes(x = 0.5, y = 200, label = label), color = "black") + 
  theme(strip.text.x = element_text(size=7)) + ggtitle("Binary Outcomes") + 
  theme(legend.position = "none")

#--------------------------------------------------------------------------------------------------------------------#

dat_text <- data.frame(
  label = paste(c(sum(meta.cont$begg.test), sum(meta.cont$egger.test), sum(meta.cont$thompson.test), sum(meta.cont$tes.d.test, na.rm = T)), "< 0.1,",
                round(c(mean(meta.cont$begg.test), mean(meta.cont$egger.test), mean(meta.cont$thompson.test), mean(meta.cont$tes.d.test, na.rm = T)),2)*100, "%"),
  test.type   = c("pval1.begg", "pval1.egger", "pval1.thompson", "pval1.d.tes"),
  i2f = factor(rep(0, times = 4)))

labels <- c(pval1.begg = "Begg Mazumdar", pval1.egger = "Egger", pval1.thompson = "Thompson Sharp", pval1.d.tes = "Excess significance")
p.dist.cont <- meta.cont %>% ungroup() %>% 
  select(i2f, pval1.egger, pval1.thompson, pval1.begg, pval1.d.tes) %>% 
  gather(key = "test.type", value = "p.value", pval1.egger:pval1.d.tes) %>% 
  ggplot(aes(x = p.value, fill = i2f)) + geom_histogram(boundary = 0, binwidth = 0.1) + theme_bw() + 
  facet_wrap(~test.type, labeller = labeller(test.type = labels)) + 
  geom_text(data = dat_text, mapping = aes(x = 0.5, y = 60, label = label),  color = "black") + 
  theme(strip.text.x = element_text(size=7)) + ggtitle("Continuous Outcomes") + 
  theme(legend.position = "none")

#--------------------------------------------------------------------------------------------------------------------#

dat_text <- data.frame(
  label = paste(c(sum(meta.iv$begg.test), sum(meta.iv$egger.test), sum(meta.iv$thompson.test), sum(meta.iv$tes.d.test, na.rm = T)), "< 0.1,",
                round(c(mean(meta.iv$begg.test), mean(meta.iv$egger.test), mean(meta.iv$thompson.test), mean(meta.iv$tes.d.test, na.rm = T)),2)*100, "%"),
  test.type   = c("pval1.begg", "pval1.egger", "pval1.thompson", "pval1.d.tes"),
  i2f = factor(rep(0, times = 4)))

p.dist.iv <- meta.iv %>% ungroup() %>% 
  select(i2f, pval1.egger, pval1.thompson, pval1.begg, pval1.d.tes) %>% 
  gather(key = "test.type", value = "p.value", pval1.egger:pval1.d.tes) %>% 
  ggplot(aes(x = p.value, fill = i2f)) + geom_histogram(boundary = 0, binwidth = 0.1) + theme_bw() + 
  facet_wrap(~test.type, labeller = labeller(test.type = labels)) + 
  guides(fill=guide_legend(title=expression(paste(I^2, " Heterogeneity")))) + 
  geom_text(data = dat_text, mapping = aes(x = 0.5, y = 40, label = label),  color = "black")  + 
  scale_fill_discrete(labels = c("> 0", "= 0")) +
  theme(strip.text.x = element_text(size=7)) + 
  ggtitle("IV Outcomes") 
#--------------------------------------------------------------------------------------------------------------------#


################################################################################################
################################################################################################
#PUBLICATION BIAS TEST AGREEMENT
################################################################################################
################################################################################################

cor.data <- meta.f %>% select(stat.rucker, stat.harbord, stat.peter, stat.schwarzer, 
                             stat.egger, stat.thompson, stat.begg, I2)
colnames(cor.data) <- c("Rucker", "Harbord", "Peter", "Schwarzer", "Egger", "Thompson",
                        "Begg", "I-squared")

panel.pts.new <- function (x, y, corr = NULL, col.regions, cor.method, ...) 
{
  
  plot.xy(xy.coords(x, y), type = "p", ...)
  box(col = "lightgray")
  rect(xleft = -1.644854, ybottom = -1.644854, xright = 1.644854, ytop = 1.644854, density = NULL, angle = 45,
       col = NA, border = "white", lty = 1, lwd = 1.4, 
       ...)
  if (!is.null(corr)) 
    return()
  
}

# Tukey Mean difference plots;
#--------------------------------------------------------------------------------------------------------------------#
logit <- function(x) log(x/(1-x))

egger.thompson.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.egger)+ logit(pval1.thompson)))/2, 
         difference = logit(pval1.egger) - logit(pval1.thompson)) %>% 
  filter(difference < 10) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Egger) - logit(", italic(p), "-value Thompson)" )))
#--------------------------------------------------------------------------------------------------------------------#

egger.excess.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.egger)+ logit(pval1.d.tes)))/2, 
         difference = logit(pval1.egger) - logit(pval1.d.tes)) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Egger) - logit(", italic(p), "-value excess significance)" )))
#--------------------------------------------------------------------------------------------------------------------#

harbord.rucker.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.harbord)+ logit(pval1.rucker)))/2, 
         difference = logit(pval1.harbord) - logit(pval1.rucker)) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .4) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Harbord) - logit(", italic(p), "-value Rucker)" )))
#--------------------------------------------------------------------------------------------------------------------#

harbord.excess.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.harbord)+ logit(pval1.d.tes)))/2, 
         difference = logit(pval1.harbord) - logit(pval1.d.tes)) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .4) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + #theme(legend.position = "bottom") +
  scale_color_manual(labels = c("systematic error", "95 % conf. int."), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Harbord) - logit(", italic(p), "-value excess significance)" )))
#--------------------------------------------------------------------------------------------------------------------#

#Table with proportions of significant test results:
meta.f <- meta.f %>% 
mutate(test.sum.bin = tes.d.test + schwarzer.test + harbord.test + peter.test + rucker.test,
       test.sum.else = tes.d.test + egger.test + thompson.test + begg.test, 
       test.sum.bin2 = tes.d.test + harbord.test,
       test.sum.else2 = tes.d.test + egger.test, 
       test.min.bin = ifelse(schwarzer.test + harbord.test + peter.test + rucker.test > 0, 1, 0),
       test.min.else = ifelse(egger.test + thompson.test + begg.test > 0, 1, 0),
       test.min.bin2 = ifelse(test.sum.bin2 > 0, 1, 0),
       test.min.else2 = ifelse(test.sum.else2 > 0, 1, 0))


mean.min.bin <- round(mean(meta.f$test.min.bin, na.rm = T)*100, 1)
mean.min.else <- round(mean(meta.f$test.min.else, na.rm = T)*100, 1)

mean.min.bin2 <- round(mean(meta.f$test.min.bin2, na.rm = T)*100, 1)
mean.min.else2 <- round(mean(meta.f$test.min.else2, na.rm = T)*100, 1)

both.bin <- round(length(which(meta.f$test.sum.bin2 == 2))/
                    length(which(!is.na(meta.f$test.sum.bin2)))*100, 1)

both.else <- round(length(which(meta.f$test.sum.else2 == 2))/
                     length(which(!is.na(meta.f$test.sum.else2)))*100, 1)

summary.bin <- round(meta.f %>% 
  group_by(test.sum.bin) %>% summarise(n = n()) %>% select(n)/dim(meta.bin)[1] * 100, 1)

summary.else <- round(meta.f %>% group_by(test.sum.else) %>% 
                        summarise(n = n()) %>% 
                        select(n)/(dim(meta.cont)[1] + dim(meta.iv)[1]) * 100, 1)

number.sig.tests <- data.frame(number = seq(from = 0, to = 5),
                               binary = paste(summary.bin$n[-7], "%"),
                               cont = as.character(paste(c(summary.else$n[-c(6,7)], NA), "%")))
number.sig.tests <- as.matrix(number.sig.tests)
number.sig.tests[6,3][1] <- "-"



colnames(number.sig.tests) <- c("Count", "Binary Outcomes", "Continuous and IV")

meta.f <- meta.f %>% mutate(tes.d.test.b = ifelse(pval1.d.tes < 0.05, 1, 0),
                            harbord.test.b = ifelse(pval1.harbord < 0.05, 1, 0),
                            egger.test.b = ifelse(pval1.egger < 0.05, 1, 0),
                            test.sum.bin2 = tes.d.test.b + harbord.test.b,
                            test.sum.else2 = tes.d.test.b + egger.test.b, 
                            test.min.bin2 = ifelse(test.sum.bin2 > 0, 1, 0),
                            test.min.else2 = ifelse(test.sum.else2 > 0, 1, 0))

mean.min.bin2.b <- round(mean(meta.f$test.min.bin2, na.rm = T)*100, 1)
mean.min.else2.b <- round(mean(meta.f$test.min.else2, na.rm = T)*100, 1)

both.bin.b <- round(length(which(meta.f$test.sum.bin2 == 2))/
                    length(which(!is.na(meta.f$test.sum.bin2)))*100, 1)

both.else.b <- round(length(which(meta.f$test.sum.else2 == 2))/
                     length(which(!is.na(meta.f$test.sum.else2)))*100, 1)

meta.f <- meta.f %>% rowwise() %>% mutate(egger.test = ifelse(pval1.egger < 0.1/3, 1, 0),
                            thompson.test = ifelse(pval1.thompson < 0.1/3, 1, 0),
                            begg.test = ifelse(pval1.begg < 0.1/3, 1, 0),
                            
                            schwarzer.test = ifelse(pval1.schwarzer < 0.1/4, 1, 0),
                            rucker.test = ifelse(pval1.rucker < 0.1/4, 1, 0),
                            harbord.test = ifelse(pval1.harbord < 0.1/4, 1, 0),
                            peter.test = ifelse(pval1.peter < 0.1/4, 1, 0),
                            test.sum.bin = tes.d.test + schwarzer.test + harbord.test + peter.test + rucker.test,
                            test.sum.else = tes.d.test + egger.test + thompson.test + begg.test, 
                            test.min.bin = ifelse(schwarzer.test + harbord.test + peter.test + rucker.test > 0, 1, 0),
                            test.min.else = ifelse(egger.test + thompson.test + begg.test > 0, 1, 0))


mean.min.bin.b <- round(mean(meta.f$test.min.bin, na.rm = T)*100, 1)
mean.min.else.b <- round(mean(meta.f$test.min.else, na.rm = T)*100, 1)


#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#


@



The meta-analyses fulfilling the criteria from chapter \ref{ch:dataset}, section \ref{sec:Processing}, are analysed with one-sided small study effects tests and excess significance tests. The direction in which bias is expected is the one on which more significant results are ($p$-value < 0.05, two-sided). The tests are applied on the original effect size measures, i.e. risk ratios, Hedges $g$ or other effect measures (\texttt{outcome.flag} = \texttt{IV}). Different tests are applied depending on the outcome type being binary, continuous or IV.\\
Thus, multiple tests are applied on the same data. The results can then be displayed separately to compare the results from the different tests. A histogram of $p$-values will summarize the overall evidence against the null-hypothesis of no publication bias, as displayed in Figure \ref{fig:test}. \\
``Excess significance'' denotes the excess of significant $p$-values testing method from \citet{excess.significance}, see \ref{sec:excess.significance}. For continuous and IV outcomes, the names refer to: 

\begin{itemize}
\item Egger's test, the weighted linear regression test as described in section \ref{sec:Egger}
\item Thompson and Sharp's test, the weighted linear regression test adjusted for between-study heterogeneity, section \ref{sec:Thompson}
\item Begg and Mazumdar's test, the rank test described in section \ref{sec:Begg}
\end{itemize}

For binary outcomes, the names refer to:
\begin{itemize}
\item Harbord's test, the likelihood score based test (section \ref{sec:Harbord})
\item Peter's test, the weighted linear regression with inverse sample size as explanatory variable (study size proxy) described in section \ref{sec:Peter}
\item R\"ucker's test, the test based on the arcsine transformation of proportions, in combination with Thompson and Sharp's regression test (section \ref{sec:Rucker})
\item Schwarzer's test, the rank based test using the expected event counts computed with the hypergeometric distribution (section \ref{sec:Schwarzer})
\end{itemize}

\begin{figure}
<<echo=FALSE, fig.height = 9>>=
grid.arrange(p.dist.bin,
             p.dist.cont,
             p.dist.iv, ncol = 1)
@
\caption{Histogram of one-sided $p$-values for small study effect in direction of larger effect sizes. The testing method is indicated in the header, bin width is equal to 0.1. The proportion of meta-analyses with signifiant publication bias based on the threshold of 0.1 is displayed inside the figures.}
\label{fig:test}
\end{figure}

%Agreement table -> Chunck 1
The histograms in Figure \ref{fig:test} show that most distributions are clearly skewed to the right, indicating evidence for small study effects. Exceptions from this are the excess significance and Schwarzer's test, and the tests for IV outcomes. There, there is no evidence for publication bias, and the histograms are even skewed to the left (Schwarzer). But note that the sample size of IV outcomes is small ($n$ = \Sexpr{dim(meta.iv)[1]}). \\
There is discrepancy between continuous and binary data test results; the evidence for small study effects in continuous data is clearly larger. Also, we see that the $I^2$ statistic of the proportion of overall between-study heterogeneity is distributed differently for the tests, with results with $I^2 = 0$ being evenly distributed in binary outcomes along the range of $p$-values, while they are more similar to the meta-analyses with larger heterogeneity for continuous outcome.\\
In the literature, the excess significance test is advised to be used in the case of no between study heterogeneity. Since there are no small $p$-values for such meta-analyses, one must conclude that there is no excess significance in the dataset. The obvious correspondence with the heterogeneity statistic however rather suggests that the test is another way to assess between study heterogeneity. Thompson test and R\"ucker's test are designed such that they take into account the between study heterogeneity when assessing small study effects. When no heterogeneity is given, Thompson's test results and Egger's test result should be equal, however, the $I^2$ statistic is computed by a more suitable estimator \citep{paulemandel} of $\tau^2$ as the estimator that Thompson and Sharp's test uses (method of moment estimator of \citet{tau.estimator}, which is more conservative by usually finding larger $\tau^2$). In the part of the histogram with $I^2 > 0$, the two tests are more reliable than the remaining tests, while the opposite is expected from simulations \citep{limitmeta} when $I^2 = 0$.\\


\subsection{Publication Bias Test Consistency}
It cannot be seen in Figure \ref{fig:test} if the tests used are finding small study effects for the same meta-analyses. A simple method to check the consistency of test results is to compare scatterplots and empirical Spearman correlations between the test statistics. This is done in Figure \ref{fig:test.agreement}. Here, there is no separation between IV and continuous outcomes. The upper left rectangle is displaying binary outcome results and the lower right continuous and IV outcomes results. Also, the $I^2$ statistic is included. Since no test statistic is used for excess significance tests, it is not shown here

\begin{figure}
<<echo=FALSE, fig.height = 7, fig.width = 8.5, warning=FALSE>>=
corrgram(cor.data, 
         upper.panel=panel.pts.new, lower.panel=panel.cor, 
         cex.labels = 1, cex = .25, cex.cor = 2, pch = 1,
         text.panel=panel.txt, cor.method = "spearman")
@
\caption{Pairs-plot for test statistics of small study effect and excess significance. The lower panel gives the Spearman correlations for the different test statistics, and the upper panel displays a scatterplot. The colors indicate magnitude and direction of the correlation coefficients. The rectangle with white borders displays the area within which both tests have absolute value < 1.64 (dots inside are statistically not significant by 0.1 $p$-value threshold).}
\label{fig:test.agreement}
\end{figure}

The observed patterns on the scatterplots differ, and some small study effect test statistics do align better than others. Regression based tests as Egger and Thompsons test which are methodically almost identical are closely aligned, which is reflected in large correlation coefficients. Continuous and IV outcome type tests align more closely than binary outcome tests. While correlation coefficients between binary outcome tests vary between each other, Harbord's test statistic has similar correlation coefficients with the other small study effect test statistics.\\
Because scatterplots and correlation coefficients can be misleading, also a Tukey mean-difference or Bland-Altman of transformed $p$-values plot is shown for four scenarios in Figure \ref{fig:mean.diff.test}:
\begin{itemize}
\item For Egger's and Thompson's tests, which is supposedly the most similar test and should show the least deviations and systematic errors
\item For Egger's and excess significance tests
\item For Harbord's and R\"ucker's tests
\item For Harbord's and excess significance tests
\end{itemize}

This can be justified since all tests are supposed to measure the evidence for publication bias. For the plots, the $p$-values of the tests are transformed on the entire continuous scale by a logit transformation $f(x)  = \log\frac{p}{1-p}$). The mean $p$-value (($f$($p$-value no. 1) + $f$($p$-value no. 2))/2) is then displayed against the difference between the $f$($p$-value). If no systematic errors and biases exist between the measurement methods, then 

\begin{itemize}
\item the mean of the differences should be around zero (no systematic error) 
\item the points should scatter independently on the $y$-axis and no general increase or decrease with the mean of the transformed $p$-values should be visible
\end{itemize}

Although this is not formally tested, there are likely systematic errors and bias between excess significance tests, although more so for continuous tests. -- In the appendix, it can be seen that this is reproducible with the remaining small study effect tests (not there yet) -- \\
The confidence intervals from Figure \ref{fig:mean.diff.test} can be used as limits of agreement, which gives, after back-transformation, around 0.9 for Egger's and Thompson's test, and around 0.99 for Harbord's and R\"ucker's tests. This suggests that correspondence between the tests is not very good in general. 

\begin{figure}
<<echo=FALSE, fig.height = 8.5, warning=FALSE>>=
grid.arrange(egger.thompson.plot, 
             egger.excess.plot,
             harbord.rucker.plot, 
             harbord.excess.plot, ncol = 1)
@
\caption{Mean - difference plots for logit transformed $p$-values. The mean of logit transformed $p$-values is displayed on the $x$-axis and the difference on the $y$-axis. Blue and red lines display the systematic error and the confidence intervals of the systematic error (limits of agreement).}
\label{fig:mean.diff.test}
\end{figure}

The previous results suggest that the results will also differ substantially after applying the common dichotomization of $p$-values. We expect that some proportion of the meta-analysis will only be significant for a certain test, and this is indeed observed in Table \ref{number.sig.tests}. It displays the percentage of meta-analyses with a certain number of significant test results.

<<echo = FALSE, results='asis'>>=
print(xtable(number.sig.tests, caption = "Counts Number of significant test results per meta-analysis, separated
       for outcome types. Last entry for continuous and IV outcomes is empty since one test less was 
       applied", label = "number.sig.tests", align = "lccc"), include.rownames = F, size = "footnotesize")
@

Here, it is difficult to say if continuous and IV or binary outcome types show better agreement, since more tests are applied in the binary case, and the continuous and IV outcome type tests have in general more positive findings. Very few meta-analyses are significant, irrespective of the test used, but around 67\% of the dataset is not significant, no matter which test is used. \\
When only looking at small study effect tests, \Sexpr{mean.min.bin}\% of binary outcome tests and \Sexpr{mean.min.else}\% of IV and continuous outcome tests had at least one significant result. After applying the Bonferroni correction for multiple testing, this shrinks to \Sexpr{mean.min.bin.b}\% for binary and \Sexpr{mean.min.else.b}\% for continuous and IV outcomes. \\
% To compare significant findings for small study effect tests and excess significance tests, we limit ourselves to comparison with Harbord's or Egger's test. \Sexpr{mean.min.bin2}\% of binary outcome analyses had at least one of the two test $p$-values being significant, and equivalently, \Sexpr{mean.min.else2}\% for continuous and IV outcomes. The numbers change to \Sexpr{mean.min.bin2.b}\% for binary outcomes and \Sexpr{mean.min.else2.b}\% for continuous and IV outcomes after applying the Bonferroni correction. \\
% \Sexpr{both.bin}\% have significant Harbord's test result and significant excess significance test result  (\Sexpr{both.bin.b}\% with Bonferroni). Of the continuous outcomes, we have \Sexpr{both.else}\% with Egger's test and \Sexpr{both.else.b}\% with Bonferroni correction. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Small Study Effects Adjustment}
<<echo=FALSE, warning=FALSE, message=FALSE>>=


#HISTOGRAMS:  

#Comparison of treatment effect p-values:
sig.zcor <- meta.f %>% mutate(z.fixef = est.z.fixef/se.est.z.fixef,
                              z.ranef = est.z.ranef/se.est.z.ranef,
                              z.reg = est.z.reg/se.est.z.reg,
                              z.copas = est.z.copas/se.est.z.copas) %>%  
  select(z.fixef, z.ranef, z.reg, z.copas) %>% gather(key = "method", value = "fisher.z") %>% 
  mutate(p.fisher.z = 2*(1-pnorm(abs(fisher.z)))) %>% 
  group_by(method) %>% 
  summarise(significant = length(which(abs(fisher.z) > 1.96)),
            p.significant = significant/length(fisher.z))
sig.zcor <- data.frame(method = sig.zcor$method,
                       label = paste(round(sig.zcor$p.significant,3)*100, "% significant", ", (n = ", sig.zcor$significant, ")", sep = ""))

method_names <- c(
  z.fixef = "Fixed Effects",
  z.ranef = "Random Effects",
  z.reg = "Regression",
  z.copas = "Copas"
)

adjustment.p.z <- meta.f %>% 
  mutate(z.fixef = est.z.fixef/se.est.z.fixef,
         z.ranef = est.z.ranef/se.est.z.ranef,
         z.reg = est.z.reg/se.est.z.reg,
         z.copas = est.z.copas/se.est.z.copas) %>%  
  select(z.fixef, z.ranef, z.reg, z.copas) %>% 
  gather(key = "method", value = "fisher.z") %>% 
  mutate(method = factor(method, levels = c("z.fixef", "z.ranef", "z.reg", "z.copas"))) %>% 
  mutate(p.fisher.z = 2*(1-pnorm(abs(fisher.z)))) %>% 
  ggplot(aes(x = p.fisher.z)) + 
  geom_histogram(boundary = 0, bins = 20) + 
  theme_bw() + 
  facet_wrap(~method, ncol = 2, labeller = as_labeller(method_names)) +
  geom_text(data = sig.zcor, aes(x = 0.5, y = 750, label = label), position = "dodge") + 
  xlab(expression(paste(italic(p),"-value of test statistic of the ", italic(z),"-score"))) +
  ggtitle(expression(paste(italic(z), " Score ", italic(p),"-value of Meta-Analysis and Adjustment Method")))
#--------------------------------------------------------------------------------------------------------------------#

#Comparison of SMD's:
sig.d <- meta.f %>% mutate(d.fixef = est.d.fixef/se.est.d.fixef,
                           d.ranef = est.d.ranef/se.est.d.ranef,
                           d.reg = est.d.reg/se.est.d.reg,
                           d.copas = est.d.copas/se.est.d.copas) %>%  
  select(d.fixef, d.ranef, d.reg, d.copas) %>% gather(key = "method", value = "smd") %>% 
  mutate(p.smd = 2*(1-pnorm(abs(smd)))) %>% 
  group_by(method) %>% 
  summarise(significant = length(which(abs(smd) > 1.96)),
            p.significant = significant/length(smd))
sig.d <- data.frame(method = sig.d$method,
                    label = paste(round(sig.d$p.significant,3)*100, "% significant", ", (n = ", sig.d$significant, ")", sep = ""))

method_names <- c(
  d.fixef = "Fixed Effects",
  d.ranef = "Random Effects",
  d.reg = "Regression",
  d.copas = "Copas"
)

adjustment.p.d <- meta.f %>% 
  mutate(d.fixef = est.d.fixef/se.est.d.fixef,
         d.ranef = est.d.ranef/se.est.d.ranef,
         d.reg = est.d.reg/se.est.d.reg,
         d.copas = est.d.copas/se.est.d.copas) %>%  
  select(d.fixef, d.ranef, d.reg, d.copas) %>% 
  gather(key = "method", value = "smd") %>% 
  mutate(method = factor(method, levels = c("d.fixef", "d.ranef", "d.reg", "d.copas"))) %>% 
  mutate(p.smd = 2*(1-pnorm(abs(smd)))) %>% 
  ggplot(aes(x = p.smd)) + 
  geom_histogram(boundary = 0, bins = 20) + 
  theme_bw() + 
  facet_wrap(~method, ncol = 2, labeller = as_labeller(method_names)) +
  geom_text(data = sig.d, aes(x = 0.5, y = 750, label = label), position = "dodge") + 
  xlab(expression(paste(italic(p),"-value of test statistic of Hedges ", italic(g)))) +
  ggtitle(expression(paste("Hedges ", italic(g), " ", italic(p),"-value of Meta-Analysis and Adjustment Method")))
#--------------------------------------------------------------------------------------------------------------------#

#Comparison of log hazard ratios:
sig.d <- meta.f %>% filter(outcome.flag == "IV") %>% 
  mutate(d.fixef = est.fixef/se.est.fixef,
         d.ranef = est.ranef/se.est.ranef,
         d.reg = est.reg/se.est.reg,
         d.copas = est.copas/se.est.copas) %>%  
  select(d.fixef, d.ranef, d.reg, d.copas) %>% gather(key = "method", value = "smd") %>% 
  group_by(method) %>% 
  summarise(significant = length(which(abs(smd) > 1.96)),
            p.significant = (significant + sum(is.na(smd)))/(length(smd)))
sig.d <- data.frame(method = sig.d$method,
                    label = paste(round(sig.d$p.significant,3)*100, "% significant", ", (n = ", sig.d$significant, ")", sep = ""))

method_names <- c(
  d.fixef = "Fixed Effects",
  d.ranef = "Random Effects",
  d.reg = "Regression",
  d.copas = "Copas"
)

adjustment.p.log.hazard.ratio <- meta.f %>% filter(outcome.flag == "IV") %>% 
  mutate(d.fixef = est.fixef/se.est.fixef,
         d.ranef = est.ranef/se.est.ranef,
         d.reg = est.reg/se.est.reg,
         d.copas = est.copas/se.est.copas) %>%  
  select(d.fixef, d.ranef, d.reg, d.copas) %>% gather(key = "method", value = "smd") %>% 
  mutate(method = factor(method, levels = c("d.fixef", "d.ranef", "d.reg", "d.copas"))) %>% 
  mutate(p.smd = 2*(1-pnorm(abs(smd)))) %>% 
  ggplot(aes(x = p.smd)) + 
  geom_histogram(boundary = 0, bins = 20) + 
  theme_bw() + 
  facet_wrap(~method, ncol = 2, labeller = as_labeller(method_names)) +
  geom_text(data = sig.d, aes(x = 0.5, y = 100, label = label), position = "dodge") + 
  xlab(expression(paste(italic(p),"-value of test statistic of Hedges ", italic(g)))) +
  ggtitle(expression(paste("Log IV outcome measures ", italic(p),"-value of Meta-Analysis and Adjustment Method")))
#--------------------------------------------------------------------------------------------------------------------#


#Plots separated by effect measure:


sig.level <- 0.1
meta.f <- meta.f %>% rowwise() %>% 
  mutate(egger.test = ifelse(pval1.egger < sig.level, 1, 0),
         thompson.test = ifelse(pval1.thompson < sig.level, 1, 0),
         begg.test = ifelse(pval1.begg < sig.level, 1, 0),
         
         schwarzer.test = ifelse(pval1.schwarzer < sig.level, 1, 0),
         rucker.test = ifelse(pval1.rucker < sig.level, 1, 0),
         harbord.test = ifelse(pval1.harbord < sig.level, 1, 0),
         peter.test = ifelse(pval1.peter < sig.level, 1, 0),
         I2 = max(c(0, 1 - (k-1)/Q)),
         i2f = factor(ifelse(I2 == 0, 1, 0)),
         se.stat.c = ifelse(outcome.flag == "DICH", stat.rucker, stat.thompson),
         se.stat = cut(se.stat.c,
                       breaks =  c(-7,1, 2, 4.5)))

meta.bin <- meta.f %>% filter(outcome.flag == "DICH")
meta.cont <- meta.f %>% filter(outcome.flag == "CONT")
meta.iv <- meta.f %>% filter(outcome.flag == "IV")

#Meta-Analysis and adjusted treatment effect estimate difference:

#Z-score:
diff.z.fixef.reg <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", regression) %>% 
  mutate(difference = fixef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) +
  xlab(expression(paste("Fixed effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.z.fixef.copas <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas) %>% 
  mutate(difference = fixef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) +
  xlab(expression(paste("Fixed effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.z.ranef.reg <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", regression) %>% 
  mutate(difference = ranef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + ylab("") + 
  xlab(expression(paste("Random effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.z.ranef.copas <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas) %>% 
  mutate(difference = ranef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + 
  xlab(expression(paste("Random effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

#SMD:
diff.d.fixef.reg <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", regression) %>% 
  mutate(difference = fixef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + 
  xlab(expression(paste("Fixed effects - adjusted Hedges ", g))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.d.fixef.copas <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", copas) %>% 
  mutate(difference = fixef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + 
  xlab(expression(paste("Fixed effects - adjusted Hedges ", g))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.d.ranef.reg <- meta.f  %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.smd", regression) %>% 
  mutate(difference = ranef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + guides(fill=guide_legend(title="test stat.")) + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) +  ylab("") +
xlab(expression(paste("Random effects - adjusted Hedges ", g))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.d.ranef.copas <- meta.f  %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.smd", copas) %>% 
  mutate(difference = ranef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + guides(fill=guide_legend(title="test stat.")) +
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + ylab("") +
xlab(expression(paste("Random effects - adjusted Hedges ", italic(g)))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#



#Log hazard ratios:
diff.log.hazard.ratio.fixef <- meta.iv %>% 
  mutate(fixef = est.fixef, ranef = est.ranef,
         copas = est.copas, regression = est.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.log.hazard.ratio", copas:regression)  %>% 
  ggplot(aes(x = (fixef - adjusted.log.hazard.ratio))) + 
  geom_histogram(binwidth = 0.1, center = 0) +
  facet_wrap(~method) + theme_bw() + xlab("Fixed effects - adjusted log effect") + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.log.hazard.ratio.ranef <- meta.iv %>% 
  mutate(fixef = est.fixef, ranef = est.ranef,
         copas = est.copas, regression = est.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.log.hazard.ratio", copas:regression)  %>% 
  ggplot(aes(x = (ranef - adjusted.log.hazard.ratio))) + 
  geom_histogram(binwidth = 0.1, center = 0) +
  facet_wrap(~method) + theme_bw() + xlab("Random effects - adjusted log effect") + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

sig.level <- 0.1
meta.f <- meta.f %>% 
	mutate(egger.test = ifelse(pval1.egger < sig.level, 1, 0),
				 thompson.test = ifelse(pval1.thompson < sig.level, 1, 0),
				 begg.test = ifelse(pval1.begg < sig.level, 1, 0),
				 
				 tes.d.test = ifelse(pval1.d.tes < sig.level, 1, 0),
				 
				 schwarzer.test = ifelse(pval1.schwarzer < sig.level, 1, 0),
				 rucker.test = ifelse(pval1.rucker < sig.level, 1, 0),
				 harbord.test = ifelse(pval1.harbord < sig.level, 1, 0),
				 peter.test = ifelse(pval1.peter < sig.level, 1, 0))

#Quantile, mean, largerthan and missing table of differences:
adjustment.diff.keep <- meta.f %>% 
  mutate(est.copas.f = case_when(sign(est.copas) == sign(est.fixef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.fixef) ~ -abs(est.copas)),
         est.reg.f = case_when(sign(est.reg) == sign(est.fixef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.fixef) ~ -abs(est.reg)),
         est.copas.r = case_when(sign(est.copas) == sign(est.ranef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.ranef) ~ -abs(est.copas)),
         est.reg.r = case_when(sign(est.reg) == sign(est.ranef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.ranef) ~ -abs(est.reg)),
         
         est.d.copas.f = case_when(sign(est.d.copas) == sign(est.d.fixef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.fixef) ~ -abs(est.d.copas)),
         est.d.reg.f = case_when(sign(est.d.reg) == sign(est.d.fixef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.fixef) ~ -abs(est.d.reg)),
         est.d.copas.r = case_when(sign(est.d.copas) == sign(est.d.ranef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.ranef) ~ -abs(est.d.copas)),
         est.d.reg.r = case_when(sign(est.d.reg) == sign(est.d.ranef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.ranef) ~ -abs(est.d.reg)),
         
         est.z.copas.f = case_when(sign(est.z.copas) == sign(est.z.fixef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.fixef) ~ -abs(est.z.copas)),
         est.z.reg.f = case_when(sign(est.z.reg) == sign(est.z.fixef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.fixef) ~ -abs(est.z.reg)),
         est.z.copas.r = case_when(sign(est.z.copas) == sign(est.z.ranef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.ranef) ~ -abs(est.z.copas)),
         est.z.reg.r = case_when(sign(est.z.reg) == sign(est.z.ranef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.ranef) ~ -abs(est.z.reg)),
         
         est.fixef = abs(est.fixef),
         est.ranef = abs(est.ranef),
         est.z.fixef = abs(est.z.fixef),
         est.z.ranef = abs(est.z.ranef),
         est.d.fixef = abs(est.d.fixef),
         est.d.ranef = abs(est.d.ranef)) 


adjustment.diff.keep <- adjustment.diff.keep %>% 
  mutate(bias.side = ifelse(est.fixef - est.reg.f > 0, 1, 0), #Decide if effect size declines or increases
         
         egger.test = ifelse(bias.side == 1, egger.test, 0),
         thompson.test = ifelse(bias.side == 1, thompson.test, 0),
         begg.test = ifelse(bias.side == 1, begg.test, 0),
         
         schwarzer.test = ifelse(bias.side == 1, schwarzer.test, 0),
         peter.test = ifelse(bias.side == 1, peter.test, 0),
         rucker.test = ifelse(bias.side == 1, rucker.test, 0),
         harbord.test = ifelse(bias.side == 1, harbord.test, 0))

adjustment.diff <- adjustment.diff.keep %>% 
  
  transmute(copas.z.f = est.z.fixef - est.z.copas.f,
            copas.z.r = est.z.ranef - est.z.copas.r,
            reg.z.f = est.z.fixef - est.z.reg.f,
            reg.z.r = est.z.ranef - est.z.reg.r,
            
            copas.d.f = est.d.fixef - est.d.copas.f,
            copas.d.r = est.d.ranef - est.d.copas.r,
            reg.d.f = est.d.fixef - est.d.reg.f,
            reg.d.r = est.d.ranef - est.d.reg.r) 

m.adjustment.diff <- as.matrix(adjustment.diff)

adjustmend.diff.iv <- meta.iv %>% 
  transmute(est.copas.f = case_when(sign(est.copas) == sign(est.fixef) ~  abs(est.copas),
                                    sign(est.copas) != sign(est.fixef) ~ -abs(est.copas)),
            est.copas.r = case_when(sign(est.copas) == sign(est.ranef) ~  abs(est.copas),
                                    sign(est.copas) != sign(est.ranef) ~ -abs(est.copas)),
            
            est.reg.f = case_when(sign(est.reg) == sign(est.fixef) ~  abs(est.reg),
                                  sign(est.reg) != sign(est.fixef) ~ -abs(est.reg)),
            est.reg.r = case_when(sign(est.reg) == sign(est.ranef) ~  abs(est.reg),
                                  sign(est.reg) != sign(est.ranef) ~ -abs(est.reg)),
            
            est.fixef = abs(est.fixef),
            est.ranef = abs(est.ranef)) %>% 
  
  transmute(copas.iv.f = est.fixef - est.copas.f,
            copas.iv.r = est.ranef - est.copas.r,
            
            reg.iv.f =  est.fixef - est.reg.f,
            reg.iv.r = est.ranef - est.reg.r) 

m.adjustment.diff.iv <- as.matrix(adjustmend.diff.iv)




adjustment.diff.quantile.table <- apply(m.adjustment.diff, 2, 
                                        FUN = function(column){quantile(x = column, 
                                                                        probs = c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = T)})
adjustment.diff.mean <- apply(m.adjustment.diff, 2, FUN = function(column) mean(column, na.rm = T))
adjustment.diff.largerzero <- (apply(m.adjustment.diff, 2, FUN = function(column) length(which(column > 0)))/apply(m.adjustment.diff, 2, FUN = function(column) length(column)))*100
adjustment.diff.equalzero <- (apply(m.adjustment.diff, 2, FUN = function(column) length(which(column == 0)))/apply(m.adjustment.diff, 2, FUN = function(column) length(column)))*100
adjustment.diff.largerequalzero <- (apply(m.adjustment.diff, 2, FUN = function(column) length(which(column >= 0)))/apply(m.adjustment.diff, 2, FUN = function(column) length(column)))*100
adjustment.diff.comp.errors <- c(4,4,0,0,0,0,0,0)
adjustment.diff.noest <- c(sum(meta.f$est.z.copas.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.z.copas.na[meta.f$outcome.flag != "IV"]),
                                sum(meta.f$est.z.reg.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.z.reg.na[meta.f$outcome.flag != "IV"]),
                                sum(meta.f$est.d.copas.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.d.copas.na[meta.f$outcome.flag != "IV"]),
                                sum(meta.f$est.d.reg.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.d.reg.na[meta.f$outcome.flag != "IV"]))
adjustment.diff.missing <- (adjustment.diff.comp.errors + adjustment.diff.noest)/apply(m.adjustment.diff, 2, FUN = function(column) length(column))*100



adjustment.diff.table <- rbind(adjustment.diff.quantile.table, mean = adjustment.diff.mean,
                               equalzero = adjustment.diff.equalzero,
                               largerequal = adjustment.diff.largerequalzero, larger = adjustment.diff.largerzero,
                               missing = adjustment.diff.missing)


adjustment.diff.quantile.table.iv <- apply(m.adjustment.diff.iv, 2, 
                                             FUN = function(column){quantile(x = column, 
                                                                             probs = c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = T)})
adjustment.diff.mean.iv <- apply(m.adjustment.diff.iv, 2, FUN = function(column) mean(column, na.rm = T))
adjustment.diff.largerzero.iv <- (apply(m.adjustment.diff.iv, 2, FUN = function(column) length(which(column >= 0)))/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100
adjustment.diff.equalzero.iv <- (apply(m.adjustment.diff.iv, 2, FUN = function(column) length(which(column == 0)))/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100
adjustment.diff.largerequalzero.iv <- (apply(m.adjustment.diff.iv, 2, FUN = function(column) length(which(column > 0)))/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100
adjustment.diff.comp.errors.iv <- c(0,0,0,0)
adjustment.diff.noest.iv <- c(sum(meta.iv$est.copas.na), sum(meta.iv$est.copas.na),
                                sum(meta.iv$est.reg.na), sum(meta.iv$est.reg.na))
adjustment.diff.missing.iv <- ((adjustment.diff.comp.errors.iv + adjustment.diff.noest.iv)/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100

adjustment.diff.table.iv <- rbind(adjustment.diff.quantile.table.iv, mean = adjustment.diff.mean.iv,
                                    equalzero = adjustment.diff.equalzero.iv,
                                    largerequal = adjustment.diff.largerequalzero.iv, larger = adjustment.diff.largerzero.iv,
                                    missing = adjustment.diff.missing.iv)

adjustment.diff.table <- cbind(adjustment.diff.table, (adjustment.diff.table.iv))

adjustment.diff.table <- round(t(adjustment.diff.table), 4)

rownames(adjustment.diff.table) <- c(paste("z:", c("Fixed - Copas", "Random - Copas", "Fixed - Regression", "Random - Regression")),
                                     paste("g:", c("Fixed - Copas", "Random - Copas", "Fixed - Regression", "Random - Regression")),
                                     paste("Log h.r.:", c("Fixed - Copas", "Random - Copas", "Fixed - Regression", "Random - Regression")))

colnames(adjustment.diff.table) <- c(colnames(adjustment.diff.table)[1:6], "= 0 (%)", ">= 0 (%)", "> 0 (%)", "No adj. est. (%)")

#--------------------------------------------------------------------------------------------------------------------#
#Meta-Analysis and adjusted treatment effect estimate difference, missing values:

#Z-score:
missing.z.fixef <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas:regression) %>% 
  mutate(difference = fixef - adjusted.z.score) %>% 
  filter(difference < -.75 | difference > .75) %>% 
  select(meta.id)




#--------------------------------------------------------------------------------------------------------------------#

missing.z.ranef <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas:regression) %>% 
  mutate(difference = ranef - adjusted.z.score) %>% 
  filter(difference < -.75 | difference > .75) %>% 
  select(meta.id)
  
  
  
#--------------------------------------------------------------------------------------------------------------------#

#SMD:
missing.d.fixef <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", copas:regression) %>% 
  mutate(difference = fixef - adjusted.smd) %>% 
  filter(difference < -1 | difference > 1) %>% select(meta.id)
  
  
#--------------------------------------------------------------------------------------------------------------------#

missing.d.ranef <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", copas:regression) %>% 
  mutate(difference = ranef - adjusted.smd) %>% 
  filter(difference < -1 | difference > 1) %>% select(meta.id)

missing.differences <- rbind(missing.z.fixef, missing.z.ranef, missing.d.fixef, missing.d.ranef)
missing.differences <- missing.differences %>% distinct(meta.id)

missing.differences <- merge(meta.f[,
        c("meta.id", "id", "comparison.nr", "subgroup.nr",
          "est.z.ranef", "est.z.fixef", "est.z.copas", "est.z.reg", 
          "est.d.fixef", "est.d.ranef", "est.d.copas", "est.d.reg")],
        missing.differences, by = "meta.id")

missing.differences[, c("est.z.ranef", "est.z.fixef", "est.z.copas", "est.z.reg", 
          "est.d.fixef", "est.d.ranef", "est.d.copas", "est.d.reg")] <- round(missing.differences[, c("est.z.ranef", "est.z.fixef", "est.z.copas", "est.z.reg", 
          "est.d.fixef", "est.d.ranef", "est.d.copas", "est.d.reg")], 2)

missing.differences <- missing.differences %>% mutate(meta.id = as.character(meta.id), id = as.character(id),
                                                      comparison.nr = as.character(comparison.nr), subgroup.nr = as.character(subgroup.nr))

colnames(missing.differences) <- c("meta.id", "id", "comparison.nr", "subgroup.nr",
                                   "z fixed", "z random", "z Copas", "z regression",
                                   "g fixed", "g random", "g Copas", "g regression")

#--------------------------------------------------------------------------------------------------------------------#
#Tukey mean difference plots for adjustment methods:


adjustment.mean.diff.orig <- adjustment.diff.keep %>% filter(est.copas.na != 1) %>% 
  mutate(mean = (est.copas + est.reg)/2, 
         difference = est.copas - est.reg) %>% filter(difference < 10 & mean < 20) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("Copas adjusted estimate - Regression adjusted estimate (original)" )))


adjustment.mean.diff.z <- adjustment.diff.keep %>% filter(est.z.copas.na != 1) %>% 
  mutate(mean = (est.z.copas + est.z.reg)/2, 
         difference = est.z.copas - est.z.reg) %>% filter(mean < 2.5 & difference < 5) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = 0.5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("Copas adjusted ", italic(z),"-score estimate - Regression adjusted ", italic(z),"-score estimate (original)" )))


adjustment.mean.diff.d <- adjustment.diff.keep %>% filter(est.d.copas.na != 1) %>% 
  mutate(mean = (est.d.copas + est.d.reg)/2, 
         difference = est.d.copas - est.d.reg) %>% filter(difference < 7 & mean < 10) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + #theme(legend.position = "bottom") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) + guides(fill=guide_legend(title=NULL)) +
  ggtitle(expression(paste("Copas adjusted Hedges ", italic(g)," estimate - Regression adjusted Hedges ", italic(g), " estimate (original)" )))

#--------------------------------------------------------------------------------------------------------------------#

#Missing study number by copas:
missing.copas.histogram <- meta.f %>% ggplot(aes(x = missing.copas/k)) + geom_histogram(binwidth = 0.05, boundary = 0) + theme_bw() + xlab("Missing study fraction")
missing.copas.zeros <- length(which(meta.f$missing.copas[!is.na(meta.f$missing.copas)]/meta.f$k[!is.na(meta.f$missing.copas)] == 0))
missing.copas.range.frac <- c(missing.copas.zeros, quantile(x = meta.f$missing.copas[!is.na(meta.f$missing.copas)]/meta.f$k[!is.na(meta.f$missing.copas)], c(0.05, 0.25, 0.5, 0.75, 0.95)), mean(meta.f$missing.copas[!is.na(meta.f$missing.copas)]/meta.f$k[!is.na(meta.f$missing.copas)],na.rm = T) )
missing.copas.range.count <- c(missing.copas.zeros, quantile(x = meta.f$missing.copas[!is.na(meta.f$missing.copas)], c(0.05, 0.25, 0.5, 0.75, 0.95)), mean(na.rm = T,meta.f$missing.copas[!is.na(meta.f$missing.copas)])) 
missing.copas.table <- rbind(missing.copas.range.frac, missing.copas.range.count)
rownames(missing.copas.table) <- c("Missing fraction", "Missing study number")
colnames(missing.copas.table) <- c("= 0", colnames(missing.copas.table)[2:6], "mean")

@

\subsection{Change in Effect Size after Adjustment} \label{sec:change.size}

Since there's evidence for publication bias when applying small study effect and excess significance tests, we apply small study effect adjustment methods to take into account that small studies are likely overestimating the treatment effect. \\
However, we will see that consequences of adjustment can be; 
\begin{itemize}
\item first, the absolute size of the treatment effect can be smaller than the classical meta-analysis treatment effect, which is expected if publication bias
is the cause for the small study effect.
\item secondly, the effect can be larger than before in meta-analysis. In this case, the small study effect must be not due to publication bias for large effects (``true'' publication bias), but due to bias for small effects (``null effect publication bias'' or even ``reversed effect publication bias'', sometimes refer erred to as ``Proteus phenomenon'' \citep{proteus}).
\end{itemize}

This is because, in contrast to the small study effect tests, the applied adjustment method does not pre-specify the direction in which bias is expected. \\
To compare the effects of adjustment between meta-analyses, the outcome measures are transformed to Hedges $g$ (standardized mean differences) and Fisher's $z$-score (see section \ref{sec:transformation.effectsizes} for details). The effect of adjustments can be compared to either random or fixed effects meta-analysis estimates. \\
Figure \ref{fig:adjustment.reg} displays the difference $\delta$ between the observed meta-analysis pooled treatment effect and the regression adjusted treatment effect, $\hat{\theta}_M - \hat{\theta}_\textrm{Adj.}$. The absolute value $|\hat{\theta}_M|$ is taken and the sign of $\hat{\theta}_\textrm{Adj.}$ is adjusted accordingly such that all effects are mirrored to one side. $\delta > 0$ is equivalent to $\hat{\theta}_\textrm{Adj} < \hat{\theta}_M$, a reduction of the original effect size. Additionally, the test statistic of heterogeneity adjusted small study effect tests (R\"ucker's and Thompson's test) is displayed with green color. Test statitics smaller $< 1$ are equivalent to no evidence for small study effect, test statistics between one and two to weak evidence, and above two they indicate evidence for small study effects. Some very large and very small differences have been omitted in the $z$-score and Hedges $g$ histograms; they are shown in Table \ref{missing.differences}. 

\begin{figure}
<<echo=FALSE, fig.height = 5.2, warning=FALSE>>=
grid.arrange(diff.z.fixef.reg, diff.z.ranef.reg, diff.d.fixef.reg, diff.d.ranef.reg,
             ncol = 2)
@
\caption{Histogram of the treatment effect differences between meta-analysis and regression adjusted meta-analysis. Negative differences indicate greater adjusted effect sizes than meta-analysis effect sizes. The bins are centered at zero and binwidth is equal to 0.1. Deeper green color indicates more evidence for small study effects.}
\label{fig:adjustment.reg}
\end{figure}

We see that more treatment effect sizes are negatively adjusted and diminished. Furthermore, we see that large adjustment is not necessarily connected with evidence for small study effects, but there is coincidence. Adjustment was larger when random effects meta-analysis effect is used as reference, because there larger weights are given to small studies.\\
The cases with no evidence (clear green color) and large adjustment indicate that between-study heterogeneity was not resolved by adjustment for small study effects. The regression adjustment method as described in section \ref{sec:regression.adjustment} adjusts for small study effects even if there is no evidence for small study effects. Thus, a large change in effect sizes does not necessarily have to be accompained by evidence for small study effects, because the unceratinity of the small study effect is not taken into account by the adjustment method. The uncertainity of the adjusted estimates is consequently larger in the clear green fraction of the histogramm than in the deeper green fraction.\\
Figure \ref{fig:adjustment.copas} shows the same for effect sizes adjusted by Copas; Copas selection model substitutes its estimates with random effect estimates when it finds no evidence for small study effects. Therefore, the effect of adjustment by Copas can be seen when comparing adjusted with random effects estimates. Again, we clearly see that more effect sizes are adjusted downwards. Additionally, there is more coincidence with the small study effect test statistics and adjustment, which is as expected. \\

\begin{figure}
<<echo=FALSE, fig.height = 4, warning=FALSE>>=
grid.arrange(diff.z.ranef.copas, diff.d.ranef.copas, ncol = 2)
@
\caption{Histogram of the treatment effect differences between meta-analysis and Copas adjusted meta-analysis. Negative differences indicate greater adjusted effect sizes than meta-analysis effect sizes. The bins are centered at zero and binwidth is equal to 0.1. Deeper green color indicates more evidence for small study effects.}
\label{fig:adjustment.copas}
\end{figure}

Table \ref{adjustment.difference} shows the quantiles and means for the various differences and additional information. When Hedge's $g$ is used as an effect measure, there are (substantially) more reduced effect sizes. The means in Table \ref{adjustment.difference} suggest that the average reduction is small (the means of Copas in the first two rows for $z$-score are caused by one very large value, see Table \ref{missing.differences}). To recall some other findings out of Table \ref{adjustment.difference}: 5\% or \Sexpr{floor(length(!is.na(meta.f$est.z.reg))*0.05)} cases have their $z$-score reduced by more than 0.15 by regression adjustment (and 5\% or \Sexpr{floor(length(!is.na(meta.f$est.z.reg))*0.05)} increased by -0.13, fixed effects reference). Also, Hedges $g$ is reduced by 0.39 compared to fixed effects estimates in 5\% or \Sexpr{floor(length(!is.na(meta.f$est.d.reg))*0.05)} meta-analyses (or increased by 0.24). 

<<echo=FALSE, results='asis'>>=
print(xtable(adjustment.diff.table, label = "adjustment.difference", caption = "Quantiles and Means of the differences between meta-analysis pooled treatment effects and small study adjusted treatment effects. The column with the names ``> 0'' give the percentages of estimates larger than zero or larger or equal zero. The column ``No adj. est.'' gives the percentage of missing estimates due to non-significant publication bias test (for Copas) and computational errors. The row names indicate which outcome measure, meta-analysis method and adjustment method is used. Abbreviations are used for z-score (= z) and Hedges g (= g).", align = "lcccccccccr"), include.rownames = T, size = "scriptsize",# sanitize.text.function=function(x){x},
      # table.placement = "p"
      )
@


\subsection{Change in Evidence for Treatment Effects} \label{sec:change.evidence}
Adjustment for small study effects in meta-analysis will not only provide new effect sizes, but also standard errors thereof. Thus, also the evidence for efficacy of a treatment can be obtained, which is usually summarized in a suitable test statistic or $p$-value. It is of interest if and how the evidence for treatment effects changes if adjusted for publication bias. There is an important difference between Copas selection model and regression adjustment. The regression adjustment method incorporates an additional parameter with corresponding uncertainity in the treatment effect estimate, while the application of the Copas algorithm does not so. Instead, the copas selection model derives the uncertainity of the estimate from the inverse of the fisher information matrix of the likelihood. Additionally, the Copas selection model algorithm provides only an estimate when it can detect a small study effect with reasonable precision at the first place (see \ref{sec:copas}).\\
The Wald test statistics $p$-value for fixed and random effects meta-analyses and Copas and regression adjusted treatment effects have been calculated. They are shown in Figure \ref{fig:adjustment.stat} for meta-analysis based on $z$-score, Hedges $g$ and other effect measures (\texttt{outcome.flag} = \texttt{IV}). \\
It can be seen that evidence for a treatment effect decreases after adjusting for publication bias. It does so more for Hedge's $g$ compared to $z$-scores, and for regression adjustment compared to Copas selection model. 
%Also, the evidence decreases for random effects meta-analyses compared to fixed effects meta-analyses. 
In the case of other effect measures (\texttt{outcome.flag} = \texttt{IV}) adjustment, the adjustment has only negligible impact on the $p$-values. \\
Also, Copas selection method has a very small impact on the evidence compared to random effects meta-analysis, at least using $z$-scores (again, we should rely on comparisons to random effects meta-analysis to evaluate Copas method). \\ 
% Interestingly, in contrast to the small study effect tests, there seems to be no large difference between adjusted continuous and binary meta-analysis test statistics --appendix--
%Figures \ref{fig:adjustment.stat.z} for the adjusted $z$-scores, \ref{fig:adjustment.stat.smd} for the adjusted Hedges $g$ and Cohen's d and \ref{fig:adjustment.stat.log.hazard.ratio} for the adjusted log hazard ratios.

\begin{figure}
<<echo=FALSE, fig.height = 9, warning=FALSE>>=
grid.arrange(adjustment.p.z,
             adjustment.p.d,
             adjustment.p.log.hazard.ratio, ncol = 1)
@
\caption{Histogram of the Wald test-statistic $p$-value of meta-analysis and adjusted pooled treatment effect, based on different treatment effect measures. The method is indicated in the header, bin width is set to 0.05. The significant proportion based on the threshold of 0.05 is displayed inside the figures.}
\label{fig:adjustment.stat}
\end{figure}

The Copas selection model also gives an estimate of the number of missing studies. It finds that \Sexpr{round(sum(meta.f$missing.Copas, na.rm = T))} are missing, which corresponds to \Sexpr{round((sum(meta.f$missing.copas, na.rm = T)/sum(meta.f$k))*100,1)}\% from all \Sexpr{round(sum(meta.f$k))} analysed studies. Figure \ref{fig:copas.missing} shows a histogram of the overall fraction of missing studies. Note that we have excluded \Sexpr{sum(is.na(meta.f$missing.Copas))} out of \Sexpr{dim(meta.f)[1]}, for which no Copas selection model estimate, but a random effects estimate was retained because the algorithm initially found no evidence for small study effects.

\begin{figure}
<<echo = FALSE, warning = FALSE>>=
missing.copas.histogram
@
\caption{Histogram of the fraction of missing studies from the total number of studies in a meta-analyses (only data shown where Copas estimate was obtained, thus \\$n =$ \Sexpr{sum(!is.na(meta.f$missing.Copas))})}
\label{fig:copas.missing}
\end{figure}

We can see that in some occasions, the method finds more than half of all studies are missing. In most occasions, the estimate of missing studies is zero, as can be seen in Table \ref{copas.missing}. The discrepancy between mean and median may indicate that the estimate of \Sexpr{round((sum(meta.f$missing.Copas, na.rm = T)/sum(meta.f$k))*100,1)}\% missing studies depends somewhat on these extreme cases. As can be written of from Table \ref{copas.missing}, 5\%, \ie \Sexpr{round(sum(!is.na(meta.f$missing.Copas))*0.05)} meta-analyses have 17.6 or more studies missing: in fact, these 5\% most extreme make up for \Sexpr{sum(meta.f$missing.Copas[which(meta.f$missing.Copas >= 17.60)], na.rm = T)}, more than 30\% of missing studies.

<<echo= FALSE, results='asis'>>=
print(xtable(missing.copas.table, caption = "Fraction of missing studies and estimates of missing studies with their zero counts (``= 0''), quantiles and means.",
             label = "copas.missing", align = "lrrrrrrr", digits = c(0,0,0,0,1,1,1,1)),
      size = "footnotesize")
@





<<echo=FALSE, results='asis'>>=
print(xtable(missing.differences, 
             caption = "Missing meta-analysis pooled treatment effect and adjusted treatment effects. Abbreviations are used for z-score (= z) and Hedges g (= g).", label = "missing.differences", align = "llcccrrrrrrrr"), 
      include.rownames = F, size = "tiny")
@



\subsection{Comparison of adjustment methods}
Regression adjusted estimates are compared to the estimates of Copas selection model if these are not equal to random effects meta-analysis. A Tukey mean difference plot can serve to reveal systematic differences and biases between the two measurement methods. We will compare estimates based on all three outcome measures, i.e. original measure, $z$-score and Hedges $g$ in Figure \ref{fig:adjustment.mean.diff}. Note that the \Sexpr{dim(meta.iv)[1]} IV outcome data is not included when using $z$-score and Hedges $g$.

\begin{figure}
<<echo = FALSE, fig.height = 7.5>>=
grid.arrange(adjustment.mean.diff.orig,
             adjustment.mean.diff.z,
             adjustment.mean.diff.d, ncol = 1)
@
\caption{Mean - difference plots for publication bias adjustment methods. The mean of the adjusted treatment effects is displayed on the $x$-axis and the difference on the $y$-axis. Blue and red lines display the systematic error and the confidence intervals of the systematic error (limits of agreement). Two values have been omitted in the middle plot for Hedges $g$ and one for $z$-score (see Table \ref{missing.differences}).}
\label{fig:adjustment.mean.diff}
\end{figure}

No formal tests are provided, but the at least there seems to be no clear bias or systematic error. The limits of agreement in Figure \ref{fig:adjustment.mean.diff} are large. We conclude thus that the impact of regression adjustment on the effect sizes is in general not substantially larger than the impact of Copas selection model in the subset of data where the estimate of the Copas selection model is not equal to a random effects estimate. There is however a small difference, with regression estimates to have a little bit a larger absolute value. There might be some bias between adjusted $z$-scores, where regression estimates seem to be somewhat smaller when the mean is a little above zero, and somewhat larger when the mean is a little below zero. \\











<<echo = FALSE, results = 'asis'>>=
print(xtable(test.agreement, align = "lcc", caption = "Overall proportion of agreement if significant or unsignificant, and for significance only. Horizontal lines separate binary, continuous and IV outcomes (order as in table). The reference significance test is the one with more significant results.",
             label = "test.agreement"),, size = "scriptsize")
@











