% LaTeX file for Chapter 03


<<'preamble03',include=FALSE, echo=FALSE>>=
library(knitr)
opts_chunk$set(fig.path='figure/ch03_fig',
               echo=TRUE, message=FALSE,
               fig.width=8, fig.height=3,
               self.contained = FALSE,
               out.width='\\textwidth-3cm',
               message=FALSE, fig.align='center',
               background="gray98", tidy=FALSE, #tidy.opts=list(width.cutoff=60),
               cache=TRUE
)
options(width=74)
@

<<'Data03', echo=FALSE, warning=FALSE, cache = TRUE>>= 
#Load data:
rm(list = ls())
PATH_HOME = path.expand("~") # user home
PATH = file.path(PATH_HOME, 'Data/PubBias')
PATH2 = file.path(PATH_HOME, 'PubBias')
FILE = 'cochrane_2019-07-04.csv'
PATH_DATA = file.path(PATH, 'data')
PATH_CODE = file.path(PATH2, 'code')
PATH_RESULTS = file.path(PATH2, 'results_new')
# PATH_FIGURES = file.path(PATH_RESULTS, 'figures')

source(file.path(PATH_CODE, 'PubBias_functions.R'))
require(corrgram)
require(nlme)

load(file.path(PATH_DATA, "PubBias_2019-07-19.RData"))
load(file.path(PATH_RESULTS, "data_used_for_analysis.RData"))
load(file.path(PATH_RESULTS, "meta_analyses_summary_complete.RData"))
load(file.path(PATH_RESULTS, "meta_id_vector.RData"))
load(file.path(PATH_RESULTS, "meta_id_I2.RData"))
load(file.path(PATH_RESULTS, "meta.bin.RData"))
load(file.path(PATH_RESULTS, "meta.cont.RData"))
load(file.path(PATH_RESULTS, "meta.iv.RData"))



@


\chapter{Results} \label{ch:Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<echo=FALSE, warning=FALSE, message = FALSE>>=
p.samplesize.z  <- data.ext2 %>% 
  filter(total1 + total2 > 15 & total1 + total2 < 300) %>% ungroup() %>%
  mutate(std.z = abs(z), sample.size = total1 + total2) %>%
  group_by(sample.size, outcome.desired) %>%
  summarize(median.effect = median(std.z, na.rm = T)) %>%
  ggplot(aes(x = sample.size, y = median.effect)) + 
  geom_point(size = 0.8) + facet_wrap(~outcome.desired) +
  theme_bw() + xlab("sample size") + ylab("median absolute z-score") +
  xlim(c(15, 300)) + ylim(c(0, 0.25)) +
  scale_x_continuous(limits = c(15, 300), breaks = c(15, 100, 200, 300))


labels <- c(RR = "log Risk Ratio", OR = "log Odds Ratio", MD = "Mean Difference", SMD = "Std. Mean Difference")

p.samplesize.effect.sep <- data %>% 
  filter(total1 + total2 > 15 & total1 + total2 < 300) %>%
  filter(outcome.desired == "efficacy") %>% 
  filter(id != "CD001293") %>%  #filter out this crazy review..
  filter(outcome.measure.merged == "RR" |
           outcome.measure.merged == "OR" |
           outcome.measure.merged == "MD" |
           outcome.measure.merged == "SMD") %>%
  group_by(outcome.measure.merged) %>%
  mutate(sample.size = total1 + total2, 
         scaled.effect = case_when(outcome.measure.merged == "RR" ~ abs(log(effect)),
                                   outcome.measure.merged == "OR" ~ abs(log(effect)),
                                   TRUE ~ abs(effect))) %>% 
  group_by(sample.size, outcome.measure.merged) %>%
  summarize(median.effect = median(scaled.effect, na.rm = T)) %>%
  ggplot(aes(x = sample.size, y = median.effect, group = outcome.measure.merged)) + geom_point(size = .5) + facet_wrap(~outcome.measure.merged, scales = "free", labeller = labeller(outcome.measure.merged = labels)) +
  theme_bw() + xlab("sample size") + ylab("median absolute effect size") +
  xlim(c(15, 300)) +
  scale_x_continuous(limits = c(15, 300), breaks = c(15, 100, 200, 300))
@

Meta analyis are based on results of primary studies. Therefore, in a first step, an exploratory plot of the median effect size and it's dependence on the study sample size can be shown. \\
The absolute value of the median $z$-score for a given sample size of a trial is shown in Figure \ref{z.samplesize}. The medians are calculated separately for efficacy and safety outcomes. It is clearly visible that the absolute value of the medians for efficacy decreases with increasing sample size. The sample size is much smaller for safety outcomes, such that it is not clear if the median effect sizes of safety outcomes do not decrease with increasing sample size, or the variation between medians is just too large to detect a decrease.\\
The pattern for efficacy outcomes is the same for all common outcome measures as shown in Figure \ref{effect.samplesize.separated}, where the original effect size measures ``log Odds Ratio'', ``log Risk Ratio'', ``Mean Difference'' and ``Std. Mean Difference'' are used (the most common measures in the dataset, ~ 98\%). 

\begin{figure}
<<echo=FALSE, warning=FALSE>>=
plot(p.samplesize.z)
@
\caption{Median of the absolute $z$-score across sample size plotted against the total sample size.}
\label{z.samplesize}
\end{figure}

\begin{figure}
<<echo=FALSE>>=
plot(p.samplesize.effect.sep)
@
\caption{Median of the absolute value of the original effect size across sample size plotted against the total sample size.}
\label{effect.samplesize.separated}
\end{figure}



\section{Publication Bias Test Results} \label{sec:publication.bias.tests}
<<echo=FALSE, warning=FALSE, message = FALSE>>=
#Agreement proportions of publication bias tests:

#Binary:
meta.bin <- meta.bin %>%ungroup() %>%  mutate(tes.d.schwarzer = ifelse(tes.d.test == schwarzer.test, "agree", "disagree"),
                                              tes.d.peter = ifelse(tes.d.test == peter.test, "agree", "disagree"),
                                              tes.d.rucker = ifelse(tes.d.test == rucker.test, "agree", "disagree"),
                                              tes.d.harbord = ifelse(tes.d.test == harbord.test, "agree", "disagree"),
                                              schwarzer.peter = ifelse(schwarzer.test == peter.test, "agree", "disagree"),
                                              schwarzer.rucker = ifelse(schwarzer.test == rucker.test, "agree", "disagree"),
                                              schwarzer.harbord = ifelse(schwarzer.test == harbord.test, "agree", "disagree"),
                                              rucker.peter = ifelse(rucker.test == peter.test, "agree", "disagree"),
                                              rucker.harbord = ifelse(rucker.test == harbord.test, "agree", "disagree"),
                                              harbord.peter = ifelse(harbord.test == peter.test, "agree", "disagree"))
agreement.bin <- meta.bin %>% ungroup() %>% summarise(tes.d.schwarzer = sum(tes.d.schwarzer == "agree")/n(),
                                                      tes.d.peter = sum(tes.d.peter == "agree")/n(),
                                                      tes.d.rucker = sum(tes.d.rucker == "agree")/n(),
                                                      tes.d.harbord = sum(tes.d.harbord == "agree")/n(),
                                                      schwarzer.peter = sum(schwarzer.peter == "agree")/n(),
                                                      schwarzer.rucker = sum(schwarzer.rucker == "agree")/n(),
                                                      schwarzer.harbord = sum(schwarzer.harbord == "agree")/n(),
                                                      rucker.peter = sum(rucker.peter == "agree")/n(),
                                                      harbord.peter = sum(harbord.peter == "agree")/n())
meta.bin <- meta.bin %>%ungroup() %>%  mutate(tes.d.schwarzer = ifelse(tes.d.test + schwarzer.test > 1, "agree", "disagree"),
                                              tes.d.peter = ifelse(tes.d.test + peter.test > 1, "agree", "disagree"),
                                              tes.d.rucker = ifelse(tes.d.test + rucker.test > 1, "agree", "disagree"),
                                              tes.d.harbord = ifelse(tes.d.test + harbord.test > 1, "agree", "disagree"),
                                              schwarzer.peter = ifelse(schwarzer.test + peter.test > 1, "agree", "disagree"),
                                              schwarzer.rucker = ifelse(schwarzer.test + rucker.test > 1, "agree", "disagree"),
                                              schwarzer.harbord = ifelse(schwarzer.test + harbord.test > 1, "agree", "disagree"),
                                              rucker.peter = ifelse(rucker.test + peter.test > 1, "agree", "disagree"),
                                              rucker.harbord = ifelse(rucker.test + harbord.test > 1, "agree", "disagree"),
                                              harbord.peter = ifelse(harbord.test + peter.test > 1, "agree", "disagree"))
agreement.sig.bin <- meta.bin %>% ungroup() %>% summarise(tes.d.schwarzer = sum(tes.d.schwarzer == "agree")/sum(tes.d.test),
                                                      tes.d.peter = sum(tes.d.peter == "agree")/sum(tes.d.test),
                                                      tes.d.rucker = sum(tes.d.rucker == "agree")/sum(tes.d.test),
                                                      tes.d.harbord = sum(tes.d.harbord == "agree")/sum(tes.d.test),
                                                      schwarzer.peter = sum(schwarzer.peter == "agree")/sum(schwarzer.test),
                                                      schwarzer.rucker = sum(schwarzer.rucker == "agree")/sum(schwarzer.test),
                                                      schwarzer.harbord = sum(schwarzer.harbord == "agree")/sum(schwarzer.test),
                                                      rucker.peter = sum(rucker.peter == "agree")/sum(rucker.test),
                                                      harbord.peter = sum(harbord.peter == "agree")/sum(peter.test))

binary.tests.agreement <- rbind(agreement.bin, agreement.sig.bin)
rownames(binary.tests.agreement) <- c("Agreement (overall)", "Agreement (significance)")
colnames(binary.tests.agreement) <- c("Excess significance, Schwarzer", "Excess significance, Peter",
                                      "Excess significance, Rucker", "Excess significance, Harbord",
                                      "Peter, Schwarzer", "Schwarzer, Rucker", "Schwarzer, Harbord",
                                      "Rucker, Peter", "Harbord, Peter")
#--------------------------------------------------------------------------------------------------------------------#


#Continuous:
meta.cont <- meta.cont %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test == egger.test, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test == begg.test, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test == begg.test, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test == egger.test, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test ==tes.d.test, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test == begg.test, "agree", "disagree"))
agreement.cont <- meta.cont %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/n(),
                                                         thompson.tes.d = sum(thompson.tes.d == "agree")/n(),
                                                         tes.d.begg = sum(tes.d.begg == "agree")/n(),
                                                         thompson.egger = sum(thompson.egger == "agree")/n(),
                                                         thompson.begg = sum(thompson.begg == "agree")/n(),
                                                         egger.begg = sum(egger.begg == "agree")/n())
meta.cont <- meta.cont %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test + egger.test > 1, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test + begg.test > 1, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test + begg.test > 1, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test + egger.test > 1, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test +tes.d.test > 1, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test + begg.test > 1, "agree", "disagree"))
agreement.sig.cont <- meta.cont %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/sum(tes.d.test),
                                                             thompson.tes.d = sum(thompson.tes.d == "agree")/sum(tes.d.test),
                                                             tes.d.begg = sum(tes.d.begg == "agree")/sum(tes.d.test),
                                                             thompson.egger = sum(thompson.egger == "agree")/sum(thompson.test),
                                                             thompson.begg = sum(thompson.begg == "agree")/sum(begg.test),
                                                             egger.begg = sum(egger.begg == "agree")/sum(begg.test))



cont.tests.agreement <- rbind(agreement.cont, agreement.sig.cont)
rownames(cont.tests.agreement) <- c("Agreement (overall)", "Agreement (significance)")
colnames(cont.tests.agreement) <- c( "Excess significance, Egger", "Excess significance, Thompson",
                                     "Excess significance, Begg","Thompson, Egger", "Thompson, Begg", "Egger, Begg")

#--------------------------------------------------------------------------------------------------------------------#


#IV:
meta.iv <- meta.iv %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test == egger.test, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test == begg.test, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test == begg.test, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test == egger.test, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test ==tes.d.test, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test == begg.test, "agree", "disagree"))
agreement.iv <- meta.iv %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/n(),
                                                         thompson.tes.d = sum(thompson.tes.d == "agree")/n(),
                                                         tes.d.begg = sum(tes.d.begg == "agree")/n(),
                                                         thompson.egger = sum(thompson.egger == "agree")/n(),
                                                         thompson.begg = sum(thompson.begg == "agree")/n(),
                                                         egger.begg = sum(egger.begg == "agree")/n())
meta.iv <- meta.iv %>% ungroup() %>% mutate(thompson.egger = ifelse(thompson.test + egger.test > 1, "agree", "disagree"),
                                                thompson.begg = ifelse(thompson.test + begg.test > 1, "agree", "disagree"),
                                                egger.begg = ifelse(egger.test + begg.test > 1, "agree", "disagree"),
                                                tes.d.egger = ifelse(tes.d.test + egger.test > 1, "agree", "disagree"),
                                                thompson.tes.d = ifelse(thompson.test +tes.d.test > 1, "agree", "disagree"),
                                                tes.d.begg = ifelse(tes.d.test + begg.test > 1, "agree", "disagree"))
agreement.sig.iv <- meta.iv %>% ungroup() %>%  summarise(tes.d.egger = sum(tes.d.egger == "agree")/sum(egger.test),
                                                             thompson.tes.d = sum(thompson.tes.d == "agree")/sum(thompson.test),
                                                             tes.d.begg = sum(tes.d.begg == "agree")/sum(begg.test),
                                                             thompson.egger = sum(thompson.egger == "agree")/sum(thompson.test),
                                                             thompson.begg = sum(thompson.begg == "agree")/sum(begg.test),
                                                             egger.begg = sum(egger.begg == "agree")/sum(begg.test))

iv.tests.agreement <- rbind(agreement.iv, agreement.sig.iv)
rownames(iv.tests.agreement) <- c("Agreement (overall)","Agreement (significance)")
colnames(iv.tests.agreement) <- c("Excess significance, Egger (IV)", "Excess significance, Thompson (IV)",
                                    "Excess significance, Begg (IV)", "Thompson, Egger (IV)",
                                    "Thompson, Begg (IV)", "Egger, Begg (IV)")

#--------------------------------------------------------------------------------------------------------------------#

#Merging:
test.agreement <- rbind(t(binary.tests.agreement), t(cont.tests.agreement), t(iv.tests.agreement))
#--------------------------------------------------------------------------------------------------------------------#

sig.level <- 0.1
meta.f <- meta.f %>% rowwise() %>% 
  mutate(i2f = factor(ifelse(I2 == 0, 1, 0)))

meta.bin <- meta.f %>% filter(outcome.flag == "DICH")
meta.cont <- meta.f %>% filter(outcome.flag == "CONT")
meta.iv <- meta.f %>% filter(outcome.flag == "IV")



dat_text <- data.frame(
  label = paste(c(sum(meta.bin$harbord.test), sum(meta.bin$peter.test), 
                  sum(meta.bin$rucker.test), sum(meta.bin$rucker.test.linreg), 
                  sum(meta.bin$schwarzer.test), sum(meta.bin$tes.d.test, na.rm = T)), "< 0.1,",
                round(c(mean(meta.bin$harbord.test), mean(meta.bin$peter.test), 
                        mean(meta.bin$rucker.test), mean(meta.bin$rucker.test.linreg), 
                        mean(meta.bin$schwarzer.test), mean(meta.bin$tes.d.test, na.rm = T)),2)*100, "%"),
  test.type   = c("pval1.harbord", "pval1.peter", "pval1.rucker", "pval1.rucker.linreg", "pval1.schwarzer", "pval1.d.tes"),
  i2f = factor(rep(0, times = 6)))

labels <- c(pval1.harbord = "Harbord", pval1.peter = "Peter", pval1.rucker = "Rucker (Thompson)", pval1.rucker.linreg = "Rucker (Egger)", pval1.schwarzer = "Schwarzer", pval1.d.tes = "Excess significance")

p.dist.bin <- meta.bin %>% ungroup() %>% 
  select(i2f, pval1.harbord, pval1.peter, pval1.rucker, pval1.rucker.linreg, pval1.d.tes, pval1.schwarzer) %>% 
  gather(key = "test.type", value = "p.value", pval1.harbord:pval1.schwarzer) %>% 
  ggplot(aes(x = p.value, fill = i2f)) + geom_histogram(boundary = 0, binwidth = 0.1) + theme_bw() + 
  facet_wrap(~test.type, labeller = labeller(test.type = labels)) + 
  geom_text(data = dat_text, mapping = aes(x = 0.5, y = 125, label = label), color = "black") + 
  theme(strip.text.x = element_text(size=7)) + ggtitle("Binary Outcomes") + 
  theme(legend.position = "none")

#--------------------------------------------------------------------------------------------------------------------#

dat_text <- data.frame(
  label = paste(c(sum(meta.cont$begg.test), sum(meta.cont$egger.test), sum(meta.cont$thompson.test), sum(meta.cont$tes.d.test, na.rm = T)), "< 0.1,",
                round(c(mean(meta.cont$begg.test), mean(meta.cont$egger.test), mean(meta.cont$thompson.test), mean(meta.cont$tes.d.test, na.rm = T)),2)*100, "%"),
  test.type   = c("pval1.begg", "pval1.egger", "pval1.thompson", "pval1.d.tes"),
  i2f = factor(rep(0, times = 4)))

labels <- c(pval1.begg = "Begg Mazumdar", pval1.egger = "Egger", pval1.thompson = "Thompson Sharp", pval1.d.tes = "Excess significance")
p.dist.cont <- meta.cont %>% ungroup() %>% 
  select(i2f, pval1.egger, pval1.thompson, pval1.begg, pval1.d.tes) %>% 
  gather(key = "test.type", value = "p.value", pval1.egger:pval1.d.tes) %>% 
  ggplot(aes(x = p.value, fill = i2f)) + geom_histogram(boundary = 0, binwidth = 0.1) + theme_bw() + 
  facet_wrap(~test.type, labeller = labeller(test.type = labels)) + 
  geom_text(data = dat_text, mapping = aes(x = 0.5, y = 60, label = label),  color = "black") + 
  theme(strip.text.x = element_text(size=7)) + ggtitle("Continuous Outcomes") + 
  theme(legend.position = "none")

#--------------------------------------------------------------------------------------------------------------------#

dat_text <- data.frame(
  label = paste(c(sum(meta.iv$begg.test), sum(meta.iv$egger.test), sum(meta.iv$thompson.test), sum(meta.iv$tes.d.test, na.rm = T)), "< 0.1,",
                round(c(mean(meta.iv$begg.test), mean(meta.iv$egger.test), mean(meta.iv$thompson.test), mean(meta.iv$tes.d.test, na.rm = T)),2)*100, "%"),
  test.type   = c("pval1.begg", "pval1.egger", "pval1.thompson", "pval1.d.tes"),
  i2f = factor(rep(0, times = 4)))

p.dist.iv <- meta.iv %>% ungroup() %>% 
  select(i2f, pval1.egger, pval1.thompson, pval1.begg, pval1.d.tes) %>% 
  gather(key = "test.type", value = "p.value", pval1.egger:pval1.d.tes) %>% 
  ggplot(aes(x = p.value, fill = i2f)) + geom_histogram(boundary = 0, binwidth = 0.1) + theme_bw() + 
  facet_wrap(~test.type, labeller = labeller(test.type = labels)) + 
  guides(fill=guide_legend(title=expression(paste(I^2, " Heterogeneity")))) + 
  geom_text(data = dat_text, mapping = aes(x = 0.5, y = 25, label = label),  color = "black")  + 
  scale_fill_discrete(labels = c("> 0", "= 0")) +
  theme(strip.text.x = element_text(size=7)) + 
  ggtitle("IV Outcomes") 
#--------------------------------------------------------------------------------------------------------------------#


################################################################################################
################################################################################################
#PUBLICATION BIAS TEST AGREEMENT
################################################################################################
################################################################################################

cor.data <- meta.f %>% select(stat.rucker, stat.harbord, stat.peter, stat.schwarzer, 
                             stat.egger, stat.thompson, stat.begg, I2)
colnames(cor.data) <- c("Rucker", "Harbord", "Peter", "Schwarzer", "Egger", "Thompson",
                        "Begg", "I-squared")

panel.pts.new <- function (x, y, corr = NULL, col.regions, cor.method, ...) 
{
  
  plot.xy(xy.coords(x, y), type = "p", ...)
  box(col = "lightgray")
  rect(xleft = -1.644854, ybottom = -1.644854, xright = 1.644854, ytop = 1.644854, density = NULL, angle = 45,
       col = NA, border = "white", lty = 1, lwd = 1.4, 
       ...)
  if (!is.null(corr)) 
    return()
  
}

# Tukey Mean difference plots;
#--------------------------------------------------------------------------------------------------------------------#
logit <- function(x) log(x/(1-x))

egger.thompson.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.egger)+ logit(pval1.thompson)))/2, 
         difference = logit(pval1.egger) - logit(pval1.thompson)) %>% 
  filter(difference < 10) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Egger) - logit(", italic(p), "-value Thompson)" ))) + geom_smooth(method = "lm", col = "green")
#--------------------------------------------------------------------------------------------------------------------#

egger.excess.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.egger)+ logit(pval1.d.tes)))/2, 
         difference = logit(pval1.egger) - logit(pval1.d.tes)) %>% 
  filter(difference > -Inf & difference < 10) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Egger) - logit(", italic(p), "-value excess significance)" ))) + geom_smooth(method = "lm", col = "green")
#--------------------------------------------------------------------------------------------------------------------#

harbord.rucker.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.harbord)+ logit(pval1.rucker)))/2, 
         difference = logit(pval1.harbord) - logit(pval1.rucker)) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .4) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Harbord) - logit(", italic(p), "-value Rucker)" ))) + geom_smooth(method = "lm", col = "green")
#--------------------------------------------------------------------------------------------------------------------#

harbord.excess.plot <- meta.f %>% 
  mutate(mean = ((logit(pval1.harbord)+ logit(pval1.d.tes)))/2, 
         difference = logit(pval1.harbord) - logit(pval1.d.tes)) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .4) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + #theme(legend.position = "bottom") +
  scale_color_manual(labels = c("systematic error", "95 % conf. int."), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("logit(", italic(p),"-value Harbord) - logit(", italic(p), "-value excess significance)" ))) + geom_smooth(method = "lm", col = "green")
#--------------------------------------------------------------------------------------------------------------------#

#Table with proportions of significant test results:
meta.f <- meta.f %>% 
mutate(test.sum.bin = tes.d.test + schwarzer.test + harbord.test + peter.test + rucker.test,
       test.sum.else = tes.d.test + egger.test + thompson.test + begg.test, 
       test.sum.bin2 = tes.d.test + harbord.test,
       test.sum.else2 = tes.d.test + egger.test, 
       test.min.bin = ifelse(schwarzer.test + harbord.test + peter.test + rucker.test > 0, 1, 0),
       test.min.else = ifelse(egger.test + thompson.test + begg.test > 0, 1, 0),
       test.min.bin2 = ifelse(test.sum.bin2 > 0, 1, 0),
       test.min.else2 = ifelse(test.sum.else2 > 0, 1, 0))


mean.min.bin <- round(mean(meta.f$test.min.bin, na.rm = T)*100, 1)
mean.min.else <- round(mean(meta.f$test.min.else, na.rm = T)*100, 1)

mean.min.bin2 <- round(mean(meta.f$test.min.bin2, na.rm = T)*100, 1)
mean.min.else2 <- round(mean(meta.f$test.min.else2, na.rm = T)*100, 1)

both.bin <- round(length(which(meta.f$test.sum.bin2 == 2))/
                    length(which(!is.na(meta.f$test.sum.bin2)))*100, 1)

both.else <- round(length(which(meta.f$test.sum.else2 == 2))/
                     length(which(!is.na(meta.f$test.sum.else2)))*100, 1)

summary.bin <- round(meta.f %>% 
  group_by(test.sum.bin) %>% summarise(n = n()) %>% select(n)/dim(meta.bin)[1] * 100, 1)

summary.else <- round(meta.f %>% group_by(test.sum.else) %>% 
                        summarise(n = n()) %>% 
                        select(n)/(dim(meta.cont)[1] + dim(meta.iv)[1]) * 100, 1)

number.sig.tests <- data.frame(number = seq(from = 0, to = 5),
                               binary = paste(summary.bin$n[-7], "%"),
                               cont = as.character(paste(c(summary.else$n[-c(6,7)], NA), "%")))
number.sig.tests <- as.matrix(number.sig.tests)
number.sig.tests[6,3][1] <- "-"



colnames(number.sig.tests) <- c("Count", "Binary Outcomes", "Continuous and IV")

meta.f <- meta.f %>% mutate(tes.d.test.b = ifelse(pval1.d.tes < 0.05, 1, 0),
                            harbord.test.b = ifelse(pval1.harbord < 0.05, 1, 0),
                            egger.test.b = ifelse(pval1.egger < 0.05, 1, 0),
                            test.sum.bin2 = tes.d.test.b + harbord.test.b,
                            test.sum.else2 = tes.d.test.b + egger.test.b, 
                            test.min.bin2 = ifelse(test.sum.bin2 > 0, 1, 0),
                            test.min.else2 = ifelse(test.sum.else2 > 0, 1, 0))

mean.min.bin2.b <- round(mean(meta.f$test.min.bin2, na.rm = T)*100, 1)
mean.min.else2.b <- round(mean(meta.f$test.min.else2, na.rm = T)*100, 1)

both.bin.b <- round(length(which(meta.f$test.sum.bin2 == 2))/
                    length(which(!is.na(meta.f$test.sum.bin2)))*100, 1)

both.else.b <- round(length(which(meta.f$test.sum.else2 == 2))/
                     length(which(!is.na(meta.f$test.sum.else2)))*100, 1)

meta.f <- meta.f %>% rowwise() %>% mutate(egger.test = ifelse(pval1.egger < 0.1/3, 1, 0),
                            thompson.test = ifelse(pval1.thompson < 0.1/3, 1, 0),
                            begg.test = ifelse(pval1.begg < 0.1/3, 1, 0),
                            
                            schwarzer.test = ifelse(pval1.schwarzer < 0.1/4, 1, 0),
                            rucker.test = ifelse(pval1.rucker < 0.1/4, 1, 0),
                            harbord.test = ifelse(pval1.harbord < 0.1/4, 1, 0),
                            peter.test = ifelse(pval1.peter < 0.1/4, 1, 0),
                            test.sum.bin = tes.d.test + schwarzer.test + harbord.test + peter.test + rucker.test,
                            test.sum.else = tes.d.test + egger.test + thompson.test + begg.test, 
                            test.min.bin = ifelse(schwarzer.test + harbord.test + peter.test + rucker.test > 0, 1, 0),
                            test.min.else = ifelse(egger.test + thompson.test + begg.test > 0, 1, 0))


mean.min.bin.b <- round(mean(meta.f$test.min.bin, na.rm = T)*100, 1)
mean.min.else.b <- round(mean(meta.f$test.min.else, na.rm = T)*100, 1)


#--------------------------------------------------------------------------------------------------------------------#
#Calculate harmonic mean of p-values of pb tests: 
meta.f <- meta.f %>% mutate(hmean = case_when(
  outcome.flag == "DICH" ~ 5/sum(pval1.rucker^-1 + pval1.harbord^-1 + pval1.schwarzer^-1 + pval1.peter^-1 + pval1.d.tes^-1),
  outcome.flag != "DICH" ~ 4/sum(pval1.egger^-1 + pval1.thompson^-1 + pval1.begg^-1 + pval1.d.tes^-1)))

harmonic.means <- c(length(which(meta.f$hmean < 0.1))/dim(meta.f)[1], 
length(which(meta.f$hmean[which(meta.f$outcome.flag == "DICH")] < 0.1))/dim(meta.f[which(meta.f$outcome.flag == "DICH"),])[1],
length(which(meta.f$hmean[which(meta.f$outcome.flag == "CONT")] < 0.1))/dim(meta.f[which(meta.f$outcome.flag == "CONT"),])[1],
length(which(meta.f$hmean[which(meta.f$outcome.flag == "IV")] < 0.1))/dim(meta.f[which(meta.f$outcome.flag == "IV"),])[1])
harmonic.means <- round(harmonic.means*100, 1)

#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#
#--------------------------------------------------------------------------------------------------------------------#


@



The meta-analyses fulfilling the criteria from Chapter \ref{ch:dataset}, section \ref{sec:Processing}, are analysed with one-sided small publication bias tests and excess significance tests. The direction in which bias is expected is the one on which more significant results of primary studies are (two-sided $p$-value < 0.05). The tests are applied on the original effect size measures, since the journal editors and the researchers also base their decisions on them. Different tests are applied depending on the outcome being binary or continuous or if the data is only partially available (\texttt{outcome.flag} = \texttt{IV}).\\
Multiple tests are applied in order to compare their results. A histogram of $p$-values for each test will summarize the overall evidence against the null-hypothesis of no publication bias, as displayed in Figure \ref{fig:test}.\\
The abbreviations in Figure \ref{fig:test}  are shortly explained with references to Chapter \ref{ch:methods}: \\
``Excess significance'' denotes the excess of significant $p$-values testing method from \citet{excess.significance}, see \ref{sec:excess.significance}. For continuous and \texttt{IV} outcomes, the names refer to: 

\begin{itemize}
\item Egger's test, weighted linear regression test described in Section \ref{sec:Egger}
\item Thompson and Sharp's test, weighted linear regression test adjusted for between-study heterogeneity, Section \ref{sec:Thompson}
\item Begg and Mazumdar's test, rank test described in Section \ref{sec:Begg}
\end{itemize}

For binary outcomes, the names refer to:
\begin{itemize}
\item Harbord's test, likelihood score based test (Section \ref{sec:Harbord})
\item Peter's test, weighted linear regression with inverse sample size as explanatory variable described in Section \ref{sec:Peter}
\item R\"ucker's test, test based on the arcsine transformation of proportions, in combination with Thompson and Sharp's regression test (Section \ref{sec:Rucker})
\item Schwarzer's test, rank based test using the expected event counts computed with the hypergeometric distribution (Section \ref{sec:Schwarzer})
\end{itemize}

\begin{figure}
<<echo=FALSE, fig.height = 9>>=
grid.arrange(p.dist.bin,
             p.dist.cont,
             p.dist.iv, ncol = 1)
@
\caption{Histogram of one-sided $p$-values for small study effect in direction of larger effect sizes. The testing method is indicated in the header, bin width is equal to 0.1. The proportion of meta-analyses with significant publication bias based on the threshold of 0.1 is displayed inside the figures.}
\label{fig:test}
\end{figure}

%Agreement table -> Chunck 1
The histograms in Figure \ref{fig:test} show that the tests mostly find evidence for publication bias in the dataset. The $p$-values of excess significance and Schwarzer's test are rather uniformly distributed, but notably, excess significance test, Schwarzer's test and rank tests have been shown to lack statistical power. The tests that are more suitable (regression based tests in general) all have proportions of significant $p$-values ($p$ > 0.1) clearly above 10 \% which would be the expected false positive rate.\\
In Figure \ref{fig:test}, the meta-analyses with an estimated $I^2$ of zero are depicted, because some methods are known to only be suitable when no heterogeneity is present (excess significance test and also Egger's test). Other tests are specially constructed to adjust for between study heterogeneity (Thompson and Sharp's test and R\"ucker's test). These tests find a smaller proportion of significant results in Figure \ref{fig:test}. However, this is also due to application to meta-analyses with no between-study heterogeneity, where the methods lack statistical power. Similarly, the evidence decreases somewhat when R\"ucker's test is extended by Thompson and Sharp's method to account for heterogeneity. The moderate decrease indicates however that the previous restriction to meta-analyses with $I^2 < 0.5$ is sufficient to remove meta-analyses with large heterogeneity and that the test results are not heavily influenced by unaccounted between-study heterogeneity.\\
The $p$-values of tests can be summarized by computing their harmonic mean \citep{harmonic.p}. In the case of binary tests, the $p$-values of R\"ucker's, Peters, Harbord's, Schwarzer's and excess significance tests are used, in the case of continuous and \texttt{outcome.flag} = \texttt{IV} outcomes, Egger's, Thompson and Sharp's, Begg and Mazumdar's and excess significance tests are used. This lead's to an overall of \Sexpr{harmonic.means[1]}\% significant results ($p_\textrm{harmonic}$ < 0.1, \Sexpr{harmonic.means[2]}\% \texttt{outcome.flag} = \texttt{DICH}, \Sexpr{harmonic.means[3]}\% \texttt{outcome.flag} = \texttt{CONT}, \Sexpr{harmonic.means[4]}\% \texttt{outcome.flag} = \texttt{IV}).






\subsection{Publication Bias Test Consistency}
It cannot be seen in Figure \ref{fig:test} if the tests used are finding small study effects for the same meta-analyses. A simple method to check the consistency of test results is to compare scatterplots and empirical Spearman correlations between the test statistics. This is done in Figure \ref{fig:test.agreement}. Here, there is no separation between \texttt{IV} and continuous outcomes. The upper left rectangle is displaying binary outcome results and the lower right continuous and \texttt{IV} outcomes results. Also, the $I^2$ statistic is included. Since no approximately normally distributed test statistic is used for excess significance tests, it is not shown here

\begin{figure}
<<echo=FALSE, fig.height = 7, fig.width = 8.5, warning=FALSE>>=
corrgram(cor.data, 
         upper.panel=panel.pts.new, lower.panel=panel.cor, 
         cex.labels = 2, cex = .25, cex.cor = 2, pch = 1,
         text.panel=panel.txt, cor.method = "spearman")
@
\caption{Pairs-plot for test statistics of small study effect and excess significance. The lower panel gives the Spearman correlations for the different test statistics, and the upper panel displays a scatterplot. The colors indicate magnitude and direction of the correlation coefficients. The rectangle with white borders displays the area within which both tests have absolute value < 1.64 (dots inside are statistically not significant by 0.1 $p$-value threshold).}
\label{fig:test.agreement}
\end{figure}

The observed patterns on the scatterplots differ, and some small study effect test statistics do align better than others. Regression based tests as Egger and Thompsons test which are methodically almost identical are closely aligned, which is reflected in large correlation coefficients. Continuous and\texttt{IV} outcome type tests align more closely than binary outcome tests. While correlation coefficients between binary outcome tests vary between each other, Harbord's test statistic has similar correlation coefficients with the other small study effect test statistics.\\
Because scatterplots and correlation coefficients can be misleading, also a Tukey mean-difference or Bland-Altman of transformed $p$-values plot is shown for four scenarios in Figure \ref{fig:mean.diff.test}:
\begin{itemize}
\item For Egger's and Thompson's tests, which is supposedly the most similar test and should show the least deviations and systematic errors
\item For Egger's and excess significance tests
\item For Harbord's and R\"ucker's tests
\item For Harbord's and excess significance tests
\end{itemize}

This can be justified since all tests are supposed to measure the evidence for publication bias. For the plots, the $p$-values of the tests are transformed on the entire continuous scale by a logit transformation $f(x)  = \log(\frac{p}{1-p}$). The mean $p$-value (($f$($p$-value no. 1) + $f$($p$-value no. 2))/2) is then displayed against the difference between the $f$($p$-value). If no systematic errors and biases exist between the measurement methods, then 

\begin{itemize}
\item the mean of the differences should be around zero (no systematic error) 
\item the points should scatter independently on the $y$-axis and no general increase or decrease with the mean of the transformed $p$-values should be visible (and the linear regression fit is flat)
\end{itemize}

There are likely systematic errors and bias between the tests, although the extent seems to vary. Most error seems to be between small study effect tests and excess significance tests, because the slope of the linear regression fit is likely positive. This means that the excess significance test finds less evidence in cases when both $p$-values are small and more evidence when both $p$-values are large, on average.\\
However, the confidence intervals from Figure \ref{fig:mean.diff.test} are very large. This suggests that additionally to the bias correspondence between the tests is not very good in general. 

\begin{figure}
<<echo=FALSE, fig.height = 8.5, warning=FALSE>>=
grid.arrange(egger.thompson.plot, 
             egger.excess.plot,
             harbord.rucker.plot, 
             harbord.excess.plot, ncol = 1)
@
\caption{Mean - difference plots for logit transformed $p$-values. The mean of logit transformed $p$-values is displayed on the $x$-axis and the difference on the $y$-axis. Blue and red lines display the systematic error and the confidence intervals of the systematic error (limits of agreement).
In green, a linear regression fit is shown with 95\% CI bands.}
\label{fig:mean.diff.test}
\end{figure}

The previous results suggest that the results will also differ substantially after applying the common dichotomization of $p$-values. Some proportion of the meta-analyses will only be significant using a single test, while being non-significant otherwise. This can be seen in Table \ref{number.sig.tests}. It displays the percentage of meta-analyses with a certain number of significant test results. Very few meta-analyses are give a significant result, independently of the test applied. Around 67\% of the dataset is not significant, no matter which test is used. When leaving away the excess significance test, \Sexpr{mean.min.bin}\% of binary outcome tests and \Sexpr{mean.min.else}\% of \texttt{IV} and continuous outcome tests had at least one significant result. 
%After applying the Bonferroni correction for multiple testing, this shrinks to \Sexpr{mean.min.bin.b}\% for binary and \Sexpr{mean.min.else.b}\% for continuous and \texttt{IV} outcomes. 
To compare significant findings for small study effect tests and excess significance tests, Harbord's or Egger's test results are compared with excess significance tests. \Sexpr{mean.min.bin2}\% of binary outcome analyses had at least one of the two test $p$-values being significant, and equivalently, \Sexpr{mean.min.else2}\% for continuous and \texttt{IV} outcomes. The numbers change to \Sexpr{mean.min.bin2.b}\% for binary outcomes and \Sexpr{mean.min.else2.b}\% for continuous and \texttt{IV} outcomes after applying the Bonferroni correction. \Sexpr{both.bin}\% have significant Harbord's test result and significant excess significance test result  (\Sexpr{both.bin.b}\% with Bonferroni). Of the continuous outcomes, we have \Sexpr{both.else}\% with Egger's test and \Sexpr{both.else.b}\% with Bonferroni correction. \\

<<echo = FALSE, results='asis'>>=
print(xtable(number.sig.tests, caption = "Counts Number of significant test results per meta-analysis, separated
       for outcome types. Last entry for continuous and IV outcomes is empty since one test less was 
       applied", label = "number.sig.tests", align = "lccc"), include.rownames = F, size = "footnotesize")
@

The precise proportions of agreement are provided in table \ref{test.agreement}. %Correspondence between excess significance tests and small study effects thus is considered rather random. 
Linear regression based tests agree more often with other linear regression based tests, and agreement between small study effect tests is in general well above 60 \% (at best, 95\% test agreement in significance for \texttt{IV} \texttt{outcome.flag}). The agreement on statistical significance between small study effect tests and excess significance tests ranges from 64\% to 4\% (for \texttt{IV} and rank tests). 

<<echo = FALSE, results = 'asis'>>=
print(xtable(test.agreement, align = "lcc", caption = "Overall proportion of agreement if significant or unsignificant, and for significance only. Horizontal lines separate binary, continuous and IV outcomes (order as in table). The reference significance test is the one with more significant results.",
             label = "test.agreement"),, size = "scriptsize")
@




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Small Study Effects Adjustment}
<<echo=FALSE, warning=FALSE, message=FALSE>>=

#Plots separated by effect measure:


sig.level <- 0.1
meta.f <- meta.f %>% rowwise() %>% 
  mutate(egger.test = ifelse(pval1.egger < sig.level, 1, 0),
         thompson.test = ifelse(pval1.thompson < sig.level, 1, 0),
         begg.test = ifelse(pval1.begg < sig.level, 1, 0),
         
         schwarzer.test = ifelse(pval1.schwarzer < sig.level, 1, 0),
         rucker.test = ifelse(pval1.rucker < sig.level, 1, 0),
         harbord.test = ifelse(pval1.harbord < sig.level, 1, 0),
         peter.test = ifelse(pval1.peter < sig.level, 1, 0),
         I2 = max(c(0, 1 - (k-1)/Q)),
         i2f = factor(ifelse(I2 == 0, 1, 0)),
         se.stat.c = ifelse(outcome.flag == "DICH", stat.rucker, stat.thompson),
         se.stat = cut(se.stat.c,
                       breaks =  c(-7,1, 2, 4.5)))

meta.bin <- meta.f %>% filter(outcome.flag == "DICH")
meta.cont <- meta.f %>% filter(outcome.flag == "CONT")
meta.iv <- meta.f %>% filter(outcome.flag == "IV")

#Meta-Analysis and adjusted treatment effect estimate difference:

#Z-score:
diff.z.fixef.reg <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", regression) %>% 
  mutate(difference = fixef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) +
  xlab(expression(paste("Fixed effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.z.fixef.copas <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas) %>% 
  mutate(difference = fixef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) +
  xlab(expression(paste("Fixed effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.z.ranef.reg <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", regression) %>% 
  mutate(difference = ranef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + ylab("") + 
  xlab(expression(paste("Random effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.z.ranef.copas <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas) %>% 
  mutate(difference = ranef - adjusted.z.score) %>% 
  filter(difference > -.75 & difference < .75) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + 
  xlab(expression(paste("Random effects - adjusted ", z, "-score"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

#SMD:
diff.d.fixef.reg <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", regression) %>% 
  mutate(difference = fixef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + 
  xlab(expression(paste("Fixed effects - adjusted std. mean difference"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.d.fixef.copas <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(se.stat, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", copas) %>% 
  mutate(difference = fixef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + theme(legend.position = "none") + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + 
  xlab(expression(paste("Fixed effects - adjusted std. mean difference"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.d.ranef.reg <- meta.f  %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.smd", regression) %>% 
  mutate(difference = ranef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + guides(fill=guide_legend(title="test stat.")) + 
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) +  ylab("") +
xlab(expression(paste("Random effects - adjusted std. mean difference"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.d.ranef.copas <- meta.f  %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.smd", copas) %>% 
  mutate(difference = ranef - adjusted.smd) %>% 
  filter(difference > -1 & difference < 1) %>% 
  ggplot(aes(x = difference, fill = se.stat)) + geom_histogram(binwidth = 0.1, center = 0) +
  theme_bw() + guides(fill=guide_legend(title="test stat.")) +
  scale_fill_manual(values  = c("seagreen1", "seagreen3", "green4")) + ylab("") +
xlab(expression(paste("Random effects - adjusted std. mean difference"))) + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#



#Log hazard ratios:
diff.log.hazard.ratio.fixef <- meta.iv %>% 
  mutate(fixef = est.fixef, ranef = est.ranef,
         copas = est.copas, regression = est.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.log.hazard.ratio", copas:regression)  %>% 
  ggplot(aes(x = (fixef - adjusted.log.hazard.ratio))) + 
  geom_histogram(binwidth = 0.1, center = 0) +
  facet_wrap(~method) + theme_bw() + xlab("Fixed effects - adjusted log effect") + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

diff.log.hazard.ratio.ranef <- meta.iv %>% 
  mutate(fixef = est.fixef, ranef = est.ranef,
         copas = est.copas, regression = est.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  gather(key = "method", value = "adjusted.log.hazard.ratio", copas:regression)  %>% 
  ggplot(aes(x = (ranef - adjusted.log.hazard.ratio))) + 
  geom_histogram(binwidth = 0.1, center = 0) +
  facet_wrap(~method) + theme_bw() + xlab("Random effects - adjusted log effect") + geom_vline(xintercept = 0, linetype = "dashed" )
#--------------------------------------------------------------------------------------------------------------------#

sig.level <- 0.1
meta.f <- meta.f %>% 
	mutate(egger.test = ifelse(pval1.egger < sig.level, 1, 0),
				 thompson.test = ifelse(pval1.thompson < sig.level, 1, 0),
				 begg.test = ifelse(pval1.begg < sig.level, 1, 0),
				 
				 tes.d.test = ifelse(pval1.d.tes < sig.level, 1, 0),
				 
				 schwarzer.test = ifelse(pval1.schwarzer < sig.level, 1, 0),
				 rucker.test = ifelse(pval1.rucker < sig.level, 1, 0),
				 harbord.test = ifelse(pval1.harbord < sig.level, 1, 0),
				 peter.test = ifelse(pval1.peter < sig.level, 1, 0))

#Quantile, mean, largerthan and missing table of differences:
adjustment.diff.keep <- meta.f %>% 
  mutate(est.copas.f = case_when(sign(est.copas) == sign(est.fixef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.fixef) ~ -abs(est.copas)),
         est.reg.f = case_when(sign(est.reg) == sign(est.fixef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.fixef) ~ -abs(est.reg)),
         est.copas.r = case_when(sign(est.copas) == sign(est.ranef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.ranef) ~ -abs(est.copas)),
         est.reg.r = case_when(sign(est.reg) == sign(est.ranef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.ranef) ~ -abs(est.reg)),
         
         est.d.copas.f = case_when(sign(est.d.copas) == sign(est.d.fixef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.fixef) ~ -abs(est.d.copas)),
         est.d.reg.f = case_when(sign(est.d.reg) == sign(est.d.fixef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.fixef) ~ -abs(est.d.reg)),
         est.d.copas.r = case_when(sign(est.d.copas) == sign(est.d.ranef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.ranef) ~ -abs(est.d.copas)),
         est.d.reg.r = case_when(sign(est.d.reg) == sign(est.d.ranef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.ranef) ~ -abs(est.d.reg)),
         
         est.z.copas.f = case_when(sign(est.z.copas) == sign(est.z.fixef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.fixef) ~ -abs(est.z.copas)),
         est.z.reg.f = case_when(sign(est.z.reg) == sign(est.z.fixef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.fixef) ~ -abs(est.z.reg)),
         est.z.copas.r = case_when(sign(est.z.copas) == sign(est.z.ranef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.ranef) ~ -abs(est.z.copas)),
         est.z.reg.r = case_when(sign(est.z.reg) == sign(est.z.ranef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.ranef) ~ -abs(est.z.reg)),
         
         est.fixef = abs(est.fixef),
         est.ranef = abs(est.ranef),
         est.z.fixef = abs(est.z.fixef),
         est.z.ranef = abs(est.z.ranef),
         est.d.fixef = abs(est.d.fixef),
         est.d.ranef = abs(est.d.ranef)) 


adjustment.diff.keep <- adjustment.diff.keep %>% 
  mutate(bias.side = ifelse(est.fixef - est.reg.f > 0, 1, 0), #Decide if effect size declines or increases
         
         egger.test = ifelse(bias.side == 1, egger.test, 0),
         thompson.test = ifelse(bias.side == 1, thompson.test, 0),
         begg.test = ifelse(bias.side == 1, begg.test, 0),
         
         schwarzer.test = ifelse(bias.side == 1, schwarzer.test, 0),
         peter.test = ifelse(bias.side == 1, peter.test, 0),
         rucker.test = ifelse(bias.side == 1, rucker.test, 0),
         harbord.test = ifelse(bias.side == 1, harbord.test, 0))

adjustment.diff <- adjustment.diff.keep %>% 
  
  transmute(copas.z.f = est.z.fixef - est.z.copas.f,
            copas.z.r = est.z.ranef - est.z.copas.r,
            reg.z.f = est.z.fixef - est.z.reg.f,
            reg.z.r = est.z.ranef - est.z.reg.r,
            
            copas.d.f = est.d.fixef - est.d.copas.f,
            copas.d.r = est.d.ranef - est.d.copas.r,
            reg.d.f = est.d.fixef - est.d.reg.f,
            reg.d.r = est.d.ranef - est.d.reg.r) 

m.adjustment.diff <- as.matrix(adjustment.diff)

adjustmend.diff.iv <- meta.iv %>% 
  transmute(est.copas.f = case_when(sign(est.copas) == sign(est.fixef) ~  abs(est.copas),
                                    sign(est.copas) != sign(est.fixef) ~ -abs(est.copas)),
            est.copas.r = case_when(sign(est.copas) == sign(est.ranef) ~  abs(est.copas),
                                    sign(est.copas) != sign(est.ranef) ~ -abs(est.copas)),
            
            est.reg.f = case_when(sign(est.reg) == sign(est.fixef) ~  abs(est.reg),
                                  sign(est.reg) != sign(est.fixef) ~ -abs(est.reg)),
            est.reg.r = case_when(sign(est.reg) == sign(est.ranef) ~  abs(est.reg),
                                  sign(est.reg) != sign(est.ranef) ~ -abs(est.reg)),
            
            est.fixef = abs(est.fixef),
            est.ranef = abs(est.ranef)) %>% 
  
  transmute(copas.iv.f = est.fixef - est.copas.f,
            copas.iv.r = est.ranef - est.copas.r,
            
            reg.iv.f =  est.fixef - est.reg.f,
            reg.iv.r = est.ranef - est.reg.r) 

m.adjustment.diff.iv <- as.matrix(adjustmend.diff.iv)




adjustment.diff.quantile.table <- apply(m.adjustment.diff, 2, 
                                        FUN = function(column){quantile(x = column, 
                                                                        probs = c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = T)})
adjustment.diff.mean <- apply(m.adjustment.diff, 2, FUN = function(column) mean(column, na.rm = T))
adjustment.diff.largerzero <- (apply(m.adjustment.diff, 2, FUN = function(column) length(which(column > 0)))/apply(m.adjustment.diff, 2, FUN = function(column) length(column)))*100
adjustment.diff.equalzero <- (apply(m.adjustment.diff, 2, FUN = function(column) length(which(column == 0)))/apply(m.adjustment.diff, 2, FUN = function(column) length(column)))*100
adjustment.diff.largerequalzero <- (apply(m.adjustment.diff, 2, FUN = function(column) length(which(column >= 0)))/apply(m.adjustment.diff, 2, FUN = function(column) length(column)))*100
adjustment.diff.comp.errors <- c(4,4,0,0,0,0,0,0)
adjustment.diff.noest <- c(sum(meta.f$est.z.copas.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.z.copas.na[meta.f$outcome.flag != "IV"]),
                                sum(meta.f$est.z.reg.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.z.reg.na[meta.f$outcome.flag != "IV"]),
                                sum(meta.f$est.d.copas.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.d.copas.na[meta.f$outcome.flag != "IV"]),
                                sum(meta.f$est.d.reg.na[meta.f$outcome.flag != "IV"]), 
                                sum(meta.f$est.d.reg.na[meta.f$outcome.flag != "IV"]))
adjustment.diff.missing <- (adjustment.diff.comp.errors + adjustment.diff.noest)/apply(m.adjustment.diff, 2, FUN = function(column) length(column))*100



adjustment.diff.table <- rbind(adjustment.diff.quantile.table, mean = adjustment.diff.mean,
                               equalzero = adjustment.diff.equalzero,
                               largerequal = adjustment.diff.largerequalzero, larger = adjustment.diff.largerzero,
                               missing = adjustment.diff.missing)


adjustment.diff.quantile.table.iv <- apply(m.adjustment.diff.iv, 2, 
                                             FUN = function(column){quantile(x = column, 
                                                                             probs = c(0.05, 0.25, 0.5, 0.75, 0.95), na.rm = T)})
adjustment.diff.mean.iv <- apply(m.adjustment.diff.iv, 2, FUN = function(column) mean(column, na.rm = T))
adjustment.diff.largerzero.iv <- (apply(m.adjustment.diff.iv, 2, FUN = function(column) length(which(column >= 0)))/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100
adjustment.diff.equalzero.iv <- (apply(m.adjustment.diff.iv, 2, FUN = function(column) length(which(column == 0)))/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100
adjustment.diff.largerequalzero.iv <- (apply(m.adjustment.diff.iv, 2, FUN = function(column) length(which(column > 0)))/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100
adjustment.diff.comp.errors.iv <- c(0,0,0,0)
adjustment.diff.noest.iv <- c(sum(meta.iv$est.copas.na), sum(meta.iv$est.copas.na),
                                sum(meta.iv$est.reg.na), sum(meta.iv$est.reg.na))
adjustment.diff.missing.iv <- ((adjustment.diff.comp.errors.iv + adjustment.diff.noest.iv)/apply(m.adjustment.diff.iv, 2, FUN = function(column) length(column)))*100

adjustment.diff.table.iv <- rbind(adjustment.diff.quantile.table.iv, mean = adjustment.diff.mean.iv,
                                    equalzero = adjustment.diff.equalzero.iv,
                                    largerequal = adjustment.diff.largerequalzero.iv, larger = adjustment.diff.largerzero.iv,
                                    missing = adjustment.diff.missing.iv)

adjustment.diff.table <- cbind(adjustment.diff.table, (adjustment.diff.table.iv))

adjustment.diff.table <- round(t(adjustment.diff.table), 4)

rownames(adjustment.diff.table) <- c(paste("z:", c("Fixed - Copas", "Random - Copas", "Fixed - Regression", "Random - Regression")),
                                     paste("g:", c("Fixed - Copas", "Random - Copas", "Fixed - Regression", "Random - Regression")),
                                     paste("IV:", c("Fixed - Copas", "Random - Copas", "Fixed - Regression", "Random - Regression")))

colnames(adjustment.diff.table) <- c(colnames(adjustment.diff.table)[1:6], "= 0 (%)", ">= 0 (%)", "> 0 (%)", "No adj. est. (%)")

#--------------------------------------------------------------------------------------------------------------------#
#Meta-Analysis and adjusted treatment effect estimate difference, missing values:

#Z-score:
missing.z.fixef <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas:regression) %>% 
  mutate(difference = fixef - adjusted.z.score) %>% 
  filter(difference < -.75 | difference > .75) %>% 
  select(meta.id)




#--------------------------------------------------------------------------------------------------------------------#

missing.z.ranef <- meta.f %>% 
  mutate(fixef = est.z.fixef, ranef = est.z.ranef,
         copas = est.z.copas, regression = est.z.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.z.score", copas:regression) %>% 
  mutate(difference = ranef - adjusted.z.score) %>% 
  filter(difference < -.75 | difference > .75) %>% 
  select(meta.id)
  
  
  
#--------------------------------------------------------------------------------------------------------------------#

#SMD:
missing.d.fixef <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(fixef) ~  abs(copas),
                           sign(copas) != sign(fixef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(fixef) ~  abs(regression),
                                sign(regression) != sign(fixef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", copas:regression) %>% 
  mutate(difference = fixef - adjusted.smd) %>% 
  filter(difference < -1 | difference > 1) %>% select(meta.id)
  
  
#--------------------------------------------------------------------------------------------------------------------#

missing.d.ranef <- meta.f %>% 
  mutate(fixef = est.d.fixef, ranef = est.d.ranef,
         copas = est.d.copas, regression = est.d.reg) %>% 
  mutate(copas = case_when(sign(copas) == sign(ranef) ~  abs(copas),
                           sign(copas) != sign(ranef) ~ -abs(copas)),
         regression = case_when(sign(regression) == sign(ranef) ~  abs(regression),
                                sign(regression) != sign(ranef) ~ -abs(regression)),
         fixef = abs(fixef),
         ranef = abs(ranef)) %>% 
  select(meta.id, fixef, ranef, copas, regression) %>% 
  gather(key = "method", value = "adjusted.smd", copas:regression) %>% 
  mutate(difference = ranef - adjusted.smd) %>% 
  filter(difference < -1 | difference > 1) %>% select(meta.id)

missing.differences <- rbind(missing.z.fixef, missing.z.ranef, missing.d.fixef, missing.d.ranef)
missing.differences <- missing.differences %>% distinct(meta.id)

missing.differences <- merge(meta.f[,
        c("meta.id", "id", "comparison.nr", "subgroup.nr",
          "est.z.ranef", "est.z.fixef", "est.z.copas", "est.z.reg", 
          "est.d.fixef", "est.d.ranef", "est.d.copas", "est.d.reg")],
        missing.differences, by = "meta.id")

missing.differences[, c("est.z.ranef", "est.z.fixef", "est.z.copas", "est.z.reg", 
          "est.d.fixef", "est.d.ranef", "est.d.copas", "est.d.reg")] <- round(missing.differences[, c("est.z.ranef", "est.z.fixef", "est.z.copas", "est.z.reg", 
          "est.d.fixef", "est.d.ranef", "est.d.copas", "est.d.reg")], 2)

missing.differences <- missing.differences %>% mutate(meta.id = as.character(meta.id), id = as.character(id),
                                                      comparison.nr = as.character(comparison.nr), subgroup.nr = as.character(subgroup.nr))

missing.differences <- missing.differences[,-1]
colnames(missing.differences) <- c("id", "comparison.nr", "subgroup.nr",
                                   "z fixed", "z random", "z Copas", "z reg.",
                                   "smd fixed", "smd random", "smd Copas", "smd reg.")


#--------------------------------------------------------------------------------------------------------------------#
#Change in evidence of treatment effect scatterplots:

mid <- median(meta.f$pval.se)
method.names <- c(z.reg = "Regression", z.copas = "Copas Selection Model")
adjusted.evidence.fixed <- meta.f %>% filter(z.reg > -40) %>% 
  select(pval.se, z.fixef, z.ranef, z.reg, z.copas) %>% 
  gather(key = "method", value = "test.stat", z.reg:z.copas) %>%
  ggplot(aes(x = test.stat, y = z.fixef, colour = pval.se)) + 
  facet_wrap(~ method, labeller = as_labeller(method.names)) + 
  scale_color_gradient2(midpoint=mid, low="firebrick4", mid = "gold",
                        high="chartreuse2", space ="Lab") +
  geom_point(size = .7) + theme_bw() + theme(legend.position = "none") + 
  geom_abline(slope = 1, intercept = 0, linetype = 2, size = .3) +
  xlab("Adjusted z test statistic") + ylab("Fixed effects test statistic")

adjusted.evidence.ranef <- meta.f %>% filter(z.reg > -40) %>% 
  select(pval.se, z.fixef, z.ranef, z.reg, z.copas) %>% 
  gather(key = "method", value = "test.stat", z.reg:z.copas) %>%
  ggplot(aes(x = test.stat, y = z.ranef, colour = pval.se)) + 
  facet_wrap(~ method, labeller = as_labeller(method.names)) + 
  scale_color_gradient2(midpoint=mid, low="firebrick4", mid = "gold",
                          high="chartreuse2", space ="Lab", name = expression(paste(italic(p),"-value"))) +
  geom_point(size = .7) + theme_bw() + guides(fill=guide_legend(title="p-value")) +
  geom_abline(slope = 1, intercept = 0, linetype = 2, size = .3) +
  xlab("Adjusted z test statistic") + ylab("Random effects test statistic")
#--------------------------------------------------------------------------------------------------------------------#
#Tukey mean difference plots for adjustment methods:


adjustment.mean.diff.orig <- adjustment.diff.keep %>% filter(est.copas.na != 1) %>% 
  mutate(mean = (est.copas + est.reg)/2, 
         difference = est.copas - est.reg) %>% filter(difference < 10 & mean < 20) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("Copas adjusted estimate - Regression adjusted estimate (original)" )))


adjustment.mean.diff.z <- adjustment.diff.keep %>% filter(est.z.copas.na != 1) %>% 
  mutate(mean = (est.z.copas + est.z.reg)/2, 
         difference = est.z.copas - est.z.reg) %>% filter(mean < 2.5 & difference < 5) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = 0.5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + theme(legend.position = "none") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) +
  ggtitle(expression(paste("Copas adjusted ", italic(z),"-score estimate - Regression adjusted ", italic(z),"-score estimate (original)" )))


adjustment.mean.diff.d <- adjustment.diff.keep %>% filter(est.d.copas.na != 1) %>% 
  mutate(mean = (est.d.copas + est.d.reg)/2, 
         difference = est.d.copas - est.d.reg) %>% filter(difference < 7 & mean < 10) %>% 
  ggplot(aes(x = mean, y = difference)) + geom_point(size = .5) + 
  geom_abline(aes(slope = 0, intercept = mean(difference, na.rm = T), col = "blue")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) + 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  geom_abline(aes(slope = 0,
                  intercept = mean(difference, na.rm = T) - 1.96*sqrt(var(difference, na.rm = T)), 
                  col = "red")) +
  # geom_smooth(method = "loess", se = F) + 
  theme_bw() + #theme(legend.position = "bottom") +
  scale_color_manual(labels = c("systematic error", "95 % confidence intervals"), 
                     values = c("red", "blue")) + guides(fill=guide_legend(title=NULL)) +
  ggtitle(expression(paste("Copas adjusted std. mean difference estimate - Regression adjusted std. mean difference estimate (original)" )))

#--------------------------------------------------------------------------------------------------------------------#

#Missing study number by copas:
missing.copas.histogram <- meta.f %>% ggplot(aes(x = missing.copas/k)) + geom_histogram(binwidth = 0.05, boundary = 0) + theme_bw() + xlab("Missing study fraction")
missing.copas.zeros <- length(which(meta.f$missing.copas[!is.na(meta.f$missing.copas)]/meta.f$k[!is.na(meta.f$missing.copas)] == 0))
missing.copas.range.frac <- c(missing.copas.zeros, quantile(x = meta.f$missing.copas[!is.na(meta.f$missing.copas)]/meta.f$k[!is.na(meta.f$missing.copas)], c(0.05, 0.25, 0.5, 0.75, 0.95)), mean(meta.f$missing.copas[!is.na(meta.f$missing.copas)]/meta.f$k[!is.na(meta.f$missing.copas)],na.rm = T) )
missing.copas.range.count <- c(missing.copas.zeros, quantile(x = meta.f$missing.copas[!is.na(meta.f$missing.copas)], c(0.05, 0.25, 0.5, 0.75, 0.95)), mean(na.rm = T,meta.f$missing.copas[!is.na(meta.f$missing.copas)])) 
missing.copas.table <- rbind(missing.copas.range.frac, missing.copas.range.count)
rownames(missing.copas.table) <- c("Missing fraction", "Missing study number")
colnames(missing.copas.table) <- c("= 0", colnames(missing.copas.table)[2:6], "mean")

@

\subsection{Change in Effect Size after Adjustment} \label{sec:change.size}

There are methods that can take into account the presence of publication bias in meta-analyses when estimating the overall treatment effect. The methods work in a semi-automatic manner; they will not only adjust for publication bias if smaller studies show larger effects, but also in the opposite case. The latter results in the adjusted overall treatment effect being \textit{larger} than the unadjusted, overall treatment effect. \\
To compare the effects of adjustment between meta-analyses of different outcomes, the outcome measures are transformed to standardized mean differences and Fisher's $z$-scores (see section \ref{sec:transformation.effectsizes} for details). When comparing to unadjusted effects, fixed or random effects meta-analysis estimates are used as references\\
Figure \ref{fig:adjustment.reg} displays the difference between the estimated meta-analysis treatment effect and the regression adjusted treatment effect \ref{sec:regression.adjustment}, $\hat{\theta}_M - \hat{\theta}_\textrm{Adj.}$. The absolute value $|\hat{\theta}_M|$ is taken and $\hat{\theta}_\textrm{Adj.}$ is negative if it's sign is different from the sign of the original $\theta$. Thus, a positive difference indicates a reduction of the original effect size, and the magnitude of the difference indicates the extent of the adjustment. \\
Additionally, the test statistics of heterogeneity adjusted publication bias tests (R\"ucker's and Thompson's test) are displayed with green color. Test statitics smaller $< 1$ (light green colored) are equivalent to no evidence for publication bias, test statistics between one and two to weak evidence, and above two they indicate evidence for publication bias (dark green). An adjusted effect with evidence for publication bias can be regarded as a more realistic estimate of the treatment effect. Some very large and very small differences have been omitted in the $z$-score and std. mean difference histograms; they are shown in Table \ref{missing.differences}. \\
Most often, adjustment leads to a reduction of overall treatment effect estimates, which can be seen by the size of the bins on the positive side of the histograms. Adjustment is stronger when random effects meta-analysis is used as reference, because it gives larger weights to small studies.\\
Contrary to naive expectation, we see cases with large negative adjustment, but no evidence for small study effects. This is because the linear regression parameter estimates are large, but estimated with high uncertainty, such that there will be few evidence for small study effects (publication bias), but nonetheless, the adjustment will be large. It is recommended to use the methods only in cases where there is clear evidence for publication bias. The color legend in Figure \ref{fig:adjustment.reg} thus gives a sense of the confidence that is put into the adjusted effects. \\

\begin{figure}
<<echo=FALSE, fig.height = 5.2, warning=FALSE>>=
grid.arrange(diff.z.fixef.reg, diff.z.ranef.reg, diff.d.fixef.reg, diff.d.ranef.reg,
             ncol = 2)
@
\caption{Histogram of the treatment effect differences between meta-analysis and regression adjusted meta-analysis. Negative differences indicate greater adjusted effect sizes than meta-analysis effect sizes. The bins are centered at zero and binwidth is equal to 0.1. Deeper green color indicates more evidence for small study effects.}
\label{fig:adjustment.reg}
\end{figure}

Additionally, some meta-analyses with positively adjusted, larger overall treatment effects after adjustment (left side of the histogram) have also evidence for publication bias (remember that publication bias was defined as the tendency of small studies to show \textit{larger} results). However, this is not wrong, because the effects and their variances have been transformed, and it is possible that the shape of the funnel plot changes upon transformation; Figure \ref{fig:funnel.plot.change} shows this for illustrative purposes. From the left to the right, the funnel plot is shown for mean differences (the original measure), standardized mean differences and Fisher's $z$ scores; there is no change upon adjustment using mean differences (blue line), reduction of the effect with std. mean differences and increase with Fisher's $z$ transformed correlation. Note that while the rank of the effect sizes is usually preserved after transformation, the relative size and especially the variance may vary. One effect of the Fisher's $z$-transformation is that the effect sizes are bounded on $[-1,1]$, and thus, very large effect sizes will influence the fit of the linear regression less than for example in std. mean differences, which are not bounded. In contrast, the variance of the correlation is directly tied to the sample size, which makes it a suitable proxy for study size (variance of the mean difference is in contrast strongly influenced by the standard deviations).

\begin{figure}
<<echo = FALSE, warning=FALSE>>=
par(mfrow = c(1,3))
example <- data.ext2 %>% filter(id == "CD003236", subgroup.nr == 2, outcome.nr == 1, comparison.nr == 1)
funnel(limitmeta(metagen(TE = effect, se = se, studlab = study.name, sm = "MD", data = example)), col.adjust = "blue", col.line = "blue")
funnel(limitmeta(metagen(TE = smd.pool, se = se.smd.pool, studlab = study.name, sm = "SMD", data = example)), col.adjust = "blue", col.line = "blue")
funnel(limitmeta(metacor(cor = z, n = total1 + total2, studlab = study.name, sm = "ZCOR", data = example)), col.adjust = "blue", col.line = "blue")
par(mfrow = c(1,1))
@
\caption{Funnel plots for a meta-analysis based on three different effect size measures: Mean differences, std. mean differences and Fisher's z transformed correlations and corresponding standard errors. Vertical dashed lines indicate meta-analysis estimates, the rhombus with the curved blue line the adjusted treatment effect.}
\label{fig:funnel.plot.change}
\end{figure}


Figure \ref{fig:adjustment.copas} shows the same for effect sizes adjusted by Copas; Copas selection model substitutes its estimates with random effect estimates when it finds no evidence for small study effects. Therefore, the effect of adjustment by Copas can better be seen when comparing adjusted with random effects meta analysis estimates. Again, we clearly see that more effect sizes are adjusted downwards. Additionally, there is more coincidence between the publication bias test statistics and adjustment, which is as expected. \\
Table \ref{adjustment.difference} shows quantiles and means for the various differences and the overall proportion of downward adjusted effect sizes. When std. mean difference is used as an effect measure, there are (substantially) more reduced effect sizes. The means in Table \ref{adjustment.difference} suggest that the average reduction is small. To recall some other findings out of Table \ref{adjustment.difference}: 5\% or \Sexpr{floor(length(!is.na(meta.f$est.z.reg))*0.05)} meta-analyses have their $z$-score reduced by more than 0.13 by regression adjustment (and 5\% or \Sexpr{floor(length(!is.na(meta.f$est.z.reg))*0.05)} increased by -0.11 or more, fixed effects reference). Also, std. mean difference is reduced by 0.39 compared to fixed effects estimates in 5\% or \Sexpr{floor(length(!is.na(meta.f$est.d.reg))*0.05)} meta-analyses (or increased by 0.24). 

\begin{figure}
<<echo=FALSE, fig.height = 4, warning=FALSE>>=
grid.arrange(diff.z.ranef.copas, diff.d.ranef.copas, ncol = 2)
@
\caption{Histogram of the treatment effect differences between meta-analysis and Copas adjusted meta-analysis. Negative differences indicate greater adjusted effect sizes than meta-analysis effect sizes. The bins are centered at zero and binwidth is equal to 0.1. Deeper green color indicates more evidence for small study effects.}
\label{fig:adjustment.copas}
\end{figure}

<<echo=FALSE, results='asis'>>=
print(xtable(adjustment.diff.table, label = "adjustment.difference", caption = "Quantiles and Means of the differences between meta-analysis pooled treatment effects and small study adjusted treatment effects. The column with the names ``> 0'' give the percentages of estimates larger than zero or larger or equal zero. The column ``No adj. est.'' gives the percentage of missing estimates due to non-significant publication bias test (for Copas) and computational errors. The row names indicate which outcome measure, meta-analysis method and adjustment method is used. Abbreviations are used for z-score (= z) and std. mean difference (= d).", align = "lcccccccccr"), include.rownames = T, size = "scriptsize",# sanitize.text.function=function(x){x},
      # table.placement = "p"
      )
@


<<echo=FALSE, results='asis'>>=
print(xtable(missing.differences, 
             caption = "Missing meta-analysis pooled treatment effect and adjusted treatment effects. Abbreviations are used for z-score (= z) and std. mean difference (= d).", label = "missing.differences", align = "lcccrrrrrrrr"), 
      include.rownames = F, size = "tiny")
@


\subsection{Comparison of adjustment methods}
Regression adjusted estimates are compared to the estimates of Copas selection model if these are not equal to random effects meta-analysis in Figure \ref{fig:adjustment.mean.diff}. A Tukey mean difference plot can serve to reveal systematic differences and biases between the two measurement methods. Note that only 26 out of \Sexpr{dim(meta.iv)[1]} \texttt{IV} outcome data is included when using $z$-score and std. mean differences, since not all effects could be transformed.

\begin{figure}
<<echo = FALSE, fig.height = 5>>=
grid.arrange(adjustment.mean.diff.z,
             adjustment.mean.diff.d, ncol = 1)
@
\caption{Mean - difference plots for publication bias adjustment methods. The mean of the adjusted treatment effects is displayed on the $x$-axis and the difference on the $y$-axis. Blue and red lines display the systematic error and the confidence intervals of the systematic error (limits of agreement). Two values have been omitted in the middle plot for std. mean difference and one for $z$-score (see Table \ref{missing.differences}).}
\label{fig:adjustment.mean.diff}
\end{figure}

No formal tests are provided, but the at least there seems to be no clear bias or systematic error. The limits of agreement in Figure \ref{fig:adjustment.mean.diff} are large. We conclude thus that the impact of regression adjustment on the effect sizes is in general not substantially larger than the impact of Copas selection model in the subset of data where the estimate of the Copas selection model is not equal to a random effects estimate. There is however a small difference, indicating that regression estimates have a little bit a larger absolute value. There might be some bias between adjusted $z$-scores, where regression estimates seem to be somewhat smaller when the mean is a little above zero, and somewhat larger when the mean is a little below zero. \\


\subsection{Change in Evidence for Treatment Effects} \label{sec:change.evidence}
Adjustment for small study effects in meta-analysis will provide new effect sizes and standard errors. The evidence against the null hypothesis of no treatment effect can be computed newly. The test statistics $\frac{\theta}{\se(\theta)}$ of random effects and fixed effects meta-analysis and the corresponding adjusted test statistics are shown in Figure \ref{fig:evidence.adjustment}. The original scale of the effects sizes is used, since it is the measure on which policy-makers assess the treatment efficacy.

\begin{figure}
<<echo = FALSE, warning = FALSE, fig.height = 6>>=
grid.arrange(adjusted.evidence.fixed,
             adjusted.evidence.ranef)
@
\caption{Test statistics of meta-analyses and adjustment methods. The color indicates the evidence for publication bias (\textit{p} value of R\\"ucker's or Egger's test), and the dotted line depicts the diagonal through the origin.}
\label{fig:evidence.adjustment}
\end{figure}

% Points lying on the diagonal through the origin indicate that there is no change in evidence after adjustment. In a hypothetical study with all treatment effects being artifacts of publication bias and perfect adjustment methods, all points should be distibuted on a vertical line at $x = 0$. A mitigation of evidence against the null hypothesis after adjustment thus is indicated by the adjusted test statistics being closer to zero. \\


It can be seen that the alignment on the diagonal changes depending on the adjustment method. The effect of adjustment by copas can be seen when comparing it to random effects meta-analysis. There, less of an effect of adjustment on the evidence can be seen. The adjustment is more likely to be more reliable than the unadjusted estimate when publication bias is strong. The dots with darker color are thus more likely to provide less biased estimates.\\
The decrease in evidence is larger when using adjustment with regression, which can be expected as the uncertainty of the additional parameter from the linear regression fit to estimate publication bias is included in the uncertainty of the estimate. In contrast, as the Copas selection model algorithm does a sensitivity analysis, the uncertainty of additional model parameters does not affect the estimate as they are not estimated but treated as fixed (see section \ref{sec:procedure} and section \ref{sec:copas}).\\
Large adjustment in effect sizes and test statistics is not necessarily accompanied by evidence for publication bias (color of the dots) in regression adjustment, as already discussed. But the dark dots can be found more often far away from the diagonal. Some cases which have more evidence for treatment efficacy after adjustment have evidence for publication bias, which is not expected, since the publication bias tests applied are one-sided and only test for bias towards large effects. \\
In some cases, the automatic detection of the side on which publication bias was expected (section \ref{sec:procedure}) might failed, because few significant effects are given (used to define side of bias) and the weighted mean effect is close to zero, in which case it is difficult to detect the side of publication bias beforehand. There are some adjustments with large increase in evidence and clear dark orange color, as in the plots on the right hand side in Figure \ref{fig:evidence.adjustment}. Two of them are investigated in detail. 
\begin{itemize}
\item z-value of regression adjustment 10.5 vs fixed effects z-value 5.7: When the funnel plot of these meta-analyses are investigated, it becomes clear that there is no reason to assume publication bias since the smaller studies are near to risk ratios equal to one, and larger studies show larger ratios. Publication bias tests disagree in their findings (R\"ucker's and Harbord's tests: 0.058 and 0.05, and Peters and Schwarzer's test: 0.467 and 0.79).
\item z-value of regression adjustment 9.24 vs fixed effects z-value 2.51: Asymmetry of the funnel plot could be confirmed when analyzing it with mean differences. The change to a larger treatment effect is rather due to large between study heterogeneity and the specific routine of the regression adjustment. Because it takes into account between-study heterogeneity (bias parameter times $\tau^2$), while fixed effects meta-analysis does not, it returns a larger treatment effect. The adjusted effect (MD = - 10.3) lies between the fixed effect estimate (-2.56) and the random effects estimate (-18.7).
\end{itemize}
In the case of Copas selection model and when both test statistics are large (> 10), the explanation lies in a specialty of the model. The adjusted treatment effect estimates are not larger than the unadjusted, but their standard errors are different and sometimes smaller, because they are obtained from the Fisher information matrix of the log-likelihood (see section \ref{sec:copas}). \\
The Copas selection model also allows to compute the number of missing studies, given that the model's assumptions are correct. It finds that \Sexpr{format(round(sum(meta.f$missing.Copas, na.rm = T)), big.mark = ",")} are missing, which corresponds to \Sexpr{round((sum(meta.f$missing.Copas, na.rm = T)/sum(meta.f$k))*100,1)}\% from all \Sexpr{format(round(sum(meta.f$k)), big.mark = ",")} analysed studies. Figure \ref{fig:copas.missing} shows a histogram of the overall fraction of missing studies. Note that random effects meta-analysis substitutes have been excluded (\Sexpr{sum(is.na(meta.f$missing.Copas))} out of \Sexpr{format(dim(meta.f)[1], big.mark = ",")}.

\begin{figure}
<<echo = FALSE, warning = FALSE>>=
missing.copas.histogram
@
\caption{Histogram of the fraction of missing studies from the total number of studies in a meta-analyses (only data shown where Copas estimate was obtained, thus \\$n =$ \Sexpr{sum(!is.na(meta.f$missing.Copas))})}
\label{fig:copas.missing}
\end{figure}

We can see that in some occasions, the method finds more than half of all studies are missing. In most occasions, the estimate of missing studies is zero, as can be seen in Table \ref{copas.missing}. The discrepancy between mean and median may indicate that the estimate of \Sexpr{round((sum(meta.f$missing.Copas, na.rm = T)/sum(meta.f$k))*100,1)}\% missing studies depends somewhat on these extreme cases. As can be written of from Table \ref{copas.missing}, 5\%, \ie \Sexpr{round(sum(!is.na(meta.f$missing.Copas))*0.05)} meta-analyses have 17.6 or more studies missing: in fact, these 5\% most extreme make up for \Sexpr{format(round(sum(meta.f$missing.Copas[which(meta.f$missing.Copas >= 17.60)], na.rm = T)), big.mark = ",")}, more than 30\% of missing studies.

<<echo= FALSE, results='asis'>>=
print(xtable(missing.copas.table, caption = "Fraction of missing studies and estimates of missing studies with their zero counts (``= 0''), quantiles and means.",
             label = "copas.missing", align = "lrrrrrrr", digits = c(0,0,0,0,1,1,1,1)),
      size = "footnotesize")
@



\section{Publication Bias over Time}
To test if publication bias in meta-analyses has decreased over time and newer meta-analyses have less bias, a generalized linear model has been set up. \\
The dependent variable is the standardized mean differences (therefore, from \texttt{outcome.flag} = \texttt{IV}, only the std. mean differences are used). As before, the std. mean difference is mirrored to one side by multiplying with the sign of the expected side of bias, \ie -1 or 1. The explanatory variable is the standard error, and the weights are the inverse of the variance of the std. mean difference. Additionally, random effects for the meta-analysis and for the review are added, the former being nested in the latter. Figure \ref{fig:smds} shows the complete dataset with the fit of the previously described model (dotted red line). The details and evidence for the small study effects in the generalized linear model are given in table \ref{anova.small.study}, the coefficients in table \ref{coefficients.small.study}

\begin{figure}
<<echo = FALSE, message = FALSE, warning = FALSE, fig.height=4>>=
##-------Prepare data--------#
reg.dat <- data.ext2 %>% filter(meta.id %in% meta.id.vector) #Filter meta-analysis data

#Mirror all effects on one positive side:
reg.dat <- reg.dat %>% group_by(meta.id) %>% 
  mutate(bias.side = bias.side.fct2(outcome = outcome.flag, 
                                    outcome.measure.merged = outcome.measure.merged, 
                                    lrr = lrr, var.lrr = var.lrr, smd = cohensd, 
                                    var.smd = var.cohensd, effect = effect, se = se),
  smd.pool = smd.pool * bias.side) #Mirror effect sizes on one side

#Get tau^2 based on smd's and introduce new study se:
reg.dat <- reg.dat %>% group_by(meta.id) %>% 
  mutate(tau.squared = metagen(TE = smd.pool, seTE = se.smd.pool)$tau^2,
         new.se.smd = sqrt(se.smd.pool^2 + tau.squared))

reg.dat <- reg.dat %>% filter(se.smd.pool != 0) %>% filter(!is.na(smd.pool)) %>% 
  filter(!is.na(se.smd.pool))

#Get review publication year:
reg.dat <- merge(reg.dat, data.review[,c("id", "year")], by = "id")
reg.dat <- reg.dat %>% filter(se.smd.pool < 3)


reg.dat <- reg.dat %>% ungroup() %>% 
  mutate(rev.year.center = scale(scale = F, center = T, x = year)[,1]) 
study.number.review.model <- dim(reg.dat)[1]
small.study.fit <- lme(fixed = smd.pool ~ se.smd.pool, random = list(~ 1 | id, ~ 1 | meta.id),
                       weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
plot(y = reg.dat$smd.pool, x = reg.dat$se.smd.pool, cex = .1, xlab = "standard error", ylab = "std. mean difference")
small.study.fit.coef <- summary(small.study.fit)$coefficients$fixed
abline(a = small.study.fit.coef, col = "red", lty = 2)

lmer.table <- function(model){
  sum <- summary(model)
  estimate <- round(sum$coefficients$fixed)
  ci <- intervals(model)
  tb <- data.frame(estimate = round(ci$fixed[,2], 3), CI = round(ci$fixed[,c(1,3)], 3))
  colnames(tb)[c(2,3)] <- c("2.5%CL", "97.5%CL")
  return(tb)
}
@
\caption{Std. mean differences of all meta-analyses plotted against their standard error, with the fit of a generalized linear model (dotted red line).}
\label{fig:smds}
\end{figure}

<<echo =FALSE, results='asis', warning=FALSE>>=
null.fit <- lme(fixed = smd.pool ~ 1, random = list(~ 1 | id, ~ 1 | meta.id),
                       weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
small.study.fit.anova <- anova(null.fit, small.study.fit)
small.study.fit.anova[2,9] <- "<0.00001"
print(xtable(small.study.fit.anova[,-1], caption = "Anova table for a generalized linear model for small study effects compared to the null model. 'small.study.fit' denotes the model with random intercepts for meta-analyses and reviews.", 
             label = "anova.small.study", digits = c(1,1,1,1,1,1,1,1,5)), 
      size = "scriptsize")
@

<<echo =FALSE, results='asis', warning=FALSE>>=
print(xtable(lmer.table(small.study.fit), align = "lccc", 
             caption = "Coefficients and 95\\% confidence limits of the generalized linear model. 'se.smd.pool' denotes the standard error of the std. mean difference.", label = "coefficients.small.study"),  size = "scriptsize")
@

It is possible to include meta-analysis specific random slopes. Before continuing, it is tested if incorporation of random slopes improves model fit. Table \ref{anova.random.slopes} shows the anova table for a model with and without random slopes.

<<echo =FALSE, results='asis', warning=FALSE>>=
small.study.rs.fit <- lme(fixed = smd.pool ~ se.smd.pool, 
                          random = list(~ 1 | id, ~ meta.id | meta.id),
                       weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
print(xtable(anova(small.study.fit, small.study.rs.fit)[,-1], caption = "Anova table for two generalized linear model fits. 'small.study.fit' denotes the model with random intercepts for meta-analyses and reviews, 'small.study.rs.fit' the model with additional random slopes per review.", 
             label = "anova.random.slopes", digits = c(1,1,1,1,1,1,1,1,1)), 
      size = "scriptsize")
@

Model diagnostics indicate that there is no benefit in including random slopes for the single meta-analysis. \\
To test if publication bias varies over time, we include the year of the publication of the review as an additional explanatory variable. Ultimately, it is of interest if there is an interaction between the small study effect (publication bias) and time, i.e. if the slope varies depending on the year. Because such a model is nested in a more simpler model where the study year is only an additive effect, all these models are fitted. Table \ref{anova.lme} displays model fit diagnostics.

<<results = 'asis', echo = FALSE>>=
review.year.fit <- lme(fixed = smd.pool ~ se.smd.pool + rev.year.center, 
                random = list(~ 1 | id, ~ 1 | meta.id),
                weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
review.year.int.fit <- lme(fixed = smd.pool ~ se.smd.pool * rev.year.center, 
                    random = list(~ 1 | id, ~ 1 | meta.id),
                weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
print(xtable(anova(small.study.fit, review.year.fit, review.year.int.fit)[,-1], caption = "Anova table for three generalized linear model fits. 'small.study.fit' denotes the model with the standard error as explanatory variable, 'review.year.fit' the model with standard error and the centered (- 2013) review year as explanatory variable, and 'review.year.int.fit' the model with interaction between the two.", 
             label = "anova.lme", digits = c(1,1,1,1,1,1,1,1,2)), size = "scriptsize")
@

Because neither AIC, BIC or the F-test do show any improvement of model fit, it does not seem as if publication bias has substantially decreased or increased over the years. The review publication year is however not directly tied to the publication year of the studies that are affected by publication bias. Thus, using another measure of time, the results may be different. Thus, the models are re-fitted with the study publication years. 

<<results = 'asis', echo = FALSE>>=
reg.dat <- reg.dat %>% filter(!is.na(study.year)) %>% ungroup() %>%  
  mutate(study.year.center = study.year - mean(study.year))
study.number.study.model <- dim(reg.dat)[1]
small.study.fit <- lme(fixed = smd.pool ~ se.smd.pool, random = list(~ 1 | id, ~ 1 | meta.id),
                       weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
study.year.fit <- lme(fixed = smd.pool ~ se.smd.pool + study.year.center, 
                random = list(~ 1 | id, ~ 1 | meta.id),
                weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
study.year.int.fit <- lme(fixed = smd.pool ~ se.smd.pool * study.year.center, 
                    random = list(~ 1 | id, ~ 1 | meta.id),
                weights = ~ se.smd.pool^2, method = "ML", data = reg.dat)
print(xtable(anova(small.study.fit, study.year.fit, study.year.int.fit)[,-1], caption = "Anova table for three generalized linear model fits. 'Small study fit' denotes the model with the standard error as explanatory variable, 'study.year.fit' the model with standard error and the centered (- 2000) study year as explanatory variable, and 'study.year.int.fit' the model with interaction between the two.", 
             label = "anova.lme", digits = c(1,1,1,1,1,1,1,1,6)), size = "scriptsize")
@

The analysis of model fit indicates that there is an improvement when using the study year as a covariate, but only as an additive effect. The coefficients of the model are given in Table \ref{coefficients.study.year.fit}. AIC and BIC are smaller and the $p$-value of the F-test is very small. However, not the size of the effect, but the slope of the linear regression fit would have to be decreased to ultimately conclude a decrease of publication bias over the years. 


<<echo =FALSE, results='asis', warning=FALSE>>=
lmer.table <- function(model){
  sum <- summary(model)
  estimate <- round(sum$coefficients$fixed)
  ci <- intervals(model)
  tb <- data.frame(estimate = round(ci$fixed[,2], 5), CI = round(ci$fixed[,c(1,3)], 5))
  colnames(tb)[c(2,3)] <- c("2.5%CL", "97.5%CL")
  return(tb)
}
print(xtable(lmer.table(study.year.fit), align = "lccc", 
             caption = "Coefficients and 95\\% confidence limits of the generalized linear model. 'se.smd.pool' denotes the standard error of the std. mean difference.", 
             label = "coefficients.study.year.fit", digits = c(4,4,4,4)),  size = "scriptsize")
@











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}
% <<echo=FALSE, fig.height = 9, warning=FALSE>>=
% #HISTOGRAMS:  
% 
% #Comparison of treatment effect p-values:
% sig.zcor <- meta.f %>% mutate(z.fixef = est.z.fixef/se.est.z.fixef,
%                               z.ranef = est.z.ranef/se.est.z.ranef,
%                               z.reg = est.z.reg/se.est.z.reg,
%                               z.copas = est.z.copas/se.est.z.copas) %>%
%   select(z.fixef, z.ranef, z.reg, z.copas) %>% gather(key = "method", value = "fisher.z") %>%
%   mutate(p.fisher.z = 2*(1-pnorm(abs(fisher.z)))) %>%
%   group_by(method) %>%
%   summarise(significant = length(which(abs(fisher.z) > 1.96)),
%             p.significant = significant/length(fisher.z))
% sig.zcor <- data.frame(method = sig.zcor$method,
%                        label = paste(round(sig.zcor$p.significant,3)*100, "% significant", ", (n = ", sig.zcor$significant, ")", sep = ""))
% 
% method_names <- c(
%   z.fixef = "Fixed Effects",
%   z.ranef = "Random Effects",
%   z.reg = "Regression",
%   z.copas = "Copas"
% )
% 
% adjustment.p.z <- meta.f %>%
%   mutate(z.fixef = est.z.fixef/se.est.z.fixef,
%          z.ranef = est.z.ranef/se.est.z.ranef,
%          z.reg = est.z.reg/se.est.z.reg,
%          z.copas = est.z.copas/se.est.z.copas) %>%
%   select(z.fixef, z.ranef, z.reg, z.copas) %>%
%   gather(key = "method", value = "fisher.z") %>%
%   mutate(method = factor(method, levels = c("z.fixef", "z.ranef", "z.reg", "z.copas"))) %>%
%   mutate(p.fisher.z = 2*(1-pnorm(abs(fisher.z)))) %>%
%   ggplot(aes(x = p.fisher.z)) +
%   geom_histogram(boundary = 0, bins = 20) +
%   theme_bw() +
%   facet_wrap(~method, ncol = 2, labeller = as_labeller(method_names)) +
%   geom_text(data = sig.zcor, aes(x = 0.5, y = 750, label = label), position = "dodge") +
%   xlab(expression(paste(italic(p),"-value of test statistic of the ", italic(z),"-score"))) +
%   ggtitle(expression(paste(italic(z), " Score ", italic(p),"-value of Meta-Analysis and Adjustment Method")))
% #--------------------------------------------------------------------------------------------------------------------#
% 
% #Comparison of SMD's:
% sig.d <- meta.f %>% mutate(d.fixef = est.d.fixef/se.est.d.fixef,
%                            d.ranef = est.d.ranef/se.est.d.ranef,
%                            d.reg = est.d.reg/se.est.d.reg,
%                            d.copas = est.d.copas/se.est.d.copas) %>%  
%   select(d.fixef, d.ranef, d.reg, d.copas) %>% gather(key = "method", value = "smd") %>% 
%   mutate(p.smd = 2*(1-pnorm(abs(smd)))) %>% 
%   group_by(method) %>% 
%   summarise(significant = length(which(abs(smd) > 1.96)),
%             p.significant = significant/length(smd))
% sig.d <- data.frame(method = sig.d$method,
%                     label = paste(round(sig.d$p.significant,3)*100, "% significant", ", (n = ", sig.d$significant, ")", sep = ""))
% 
% method_names <- c(
%   d.fixef = "Fixed Effects",
%   d.ranef = "Random Effects",
%   d.reg = "Regression",
%   d.copas = "Copas"
% )
% 
% adjustment.p.d <- meta.f %>% 
%   mutate(d.fixef = est.d.fixef/se.est.d.fixef,
%          d.ranef = est.d.ranef/se.est.d.ranef,
%          d.reg = est.d.reg/se.est.d.reg,
%          d.copas = est.d.copas/se.est.d.copas) %>%  
%   select(d.fixef, d.ranef, d.reg, d.copas) %>% 
%   gather(key = "method", value = "smd") %>% 
%   mutate(method = factor(method, levels = c("d.fixef", "d.ranef", "d.reg", "d.copas"))) %>% 
%   mutate(p.smd = 2*(1-pnorm(abs(smd)))) %>% 
%   ggplot(aes(x = p.smd)) + 
%   geom_histogram(boundary = 0, bins = 20) + 
%   theme_bw() + 
%   facet_wrap(~method, ncol = 2, labeller = as_labeller(method_names)) +
%   geom_text(data = sig.d, aes(x = 0.5, y = 750, label = label), position = "dodge") + 
%   xlab(expression(paste(italic(p),"-value of test statistic of std. mean differences"))) +
%   ggtitle(expression(paste("std. mean difference ", italic(p),"-value of Meta-Analysis and Adjustment Method")))
% #--------------------------------------------------------------------------------------------------------------------#
% 
% #Comparison of log hazard ratios:
% sig.d <- meta.f %>% filter(outcome.flag == "IV") %>% 
%   mutate(d.fixef = est.fixef/se.est.fixef,
%          d.ranef = est.ranef/se.est.ranef,
%          d.reg = est.reg/se.est.reg,
%          d.copas = est.copas/se.est.copas) %>%  
%   select(d.fixef, d.ranef, d.reg, d.copas) %>% gather(key = "method", value = "smd") %>% 
%   group_by(method) %>% 
%   summarise(significant = length(which(abs(smd) > 1.96)),
%             p.significant = (significant + sum(is.na(smd)))/(length(smd)))
% sig.d <- data.frame(method = sig.d$method,
%                     label = paste(round(sig.d$p.significant,3)*100, "% significant", ", (n = ", sig.d$significant, ")", sep = ""))
% 
% method_names <- c(
%   d.fixef = "Fixed Effects",
%   d.ranef = "Random Effects",
%   d.reg = "Regression",
%   d.copas = "Copas"
% )
% 
% adjustment.p.log.hazard.ratio <- meta.f %>% filter(outcome.flag == "IV") %>% 
%   mutate(d.fixef = est.fixef/se.est.fixef,
%          d.ranef = est.ranef/se.est.ranef,
%          d.reg = est.reg/se.est.reg,
%          d.copas = est.copas/se.est.copas) %>%  
%   select(d.fixef, d.ranef, d.reg, d.copas) %>% gather(key = "method", value = "smd") %>% 
%   mutate(method = factor(method, levels = c("d.fixef", "d.ranef", "d.reg", "d.copas"))) %>% 
%   mutate(p.smd = 2*(1-pnorm(abs(smd)))) %>% 
%   ggplot(aes(x = p.smd)) + 
%   geom_histogram(boundary = 0, bins = 20) + 
%   theme_bw() + 
%   facet_wrap(~method, ncol = 2, labeller = as_labeller(method_names)) +
%   geom_text(data = sig.d, aes(x = 0.5, y = 100, label = label), position = "dodge") + 
%   xlab(expression(paste(italic(p),"-value of test statistic of std. mean difference"))) +
%   ggtitle(expression(paste("Log IV outcome measures ", italic(p),"-value of Meta-Analysis and Adjustment Method")))
% #--------------------------------------------------------------------------------------------------------------------#
% 
% grid.arrange(adjustment.p.z,
%              adjustment.p.d,
%              adjustment.p.log.hazard.ratio, ncol = 1)
% @
% \caption{Histogram of the Wald test-statistic $p$-value of meta-analysis and adjusted pooled treatment effect, based on different treatment effect measures. The method is indicated in the header, bin width is set to 0.05. The significant proportion based on the threshold of 0.05 is displayed inside the figures.}
% \label{fig:adjustment.stat}
% \end{figure}




