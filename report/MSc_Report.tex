\documentclass[11pt,a4paper,twoside]{book}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\input{header.sty}   % packages, layout and standard macros


\usepackage{verbatim}
\usepackage{tikz}
\usepackage{amssymb}
\usetikzlibrary{trees}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\input{title}




\graphicspath{{./figure/}}
\DeclareGraphicsExtensions{.pdf,.png}
\setcounter{tocdepth}{1}



\thispagestyle{empty}
\begin{center}
  \vspace*{6cm}{\bfseries\Huge
  $p$-values:\\[5mm] their use, abuse and proper use \\[5mm]
  illustrated with seven facets 
  }
  \vfill
  \rm

  \LARGE
  M\"axli Musterli\\[12mm]
  
  \normalsize
  Version \today
\end{center}
\newpage
\thispagestyle{empty}~
\newpage
\pagenumbering{roman}

\thispagestyle{plain}\markboth{Contents}{Contents}
\tableofcontents
\setkeys{Gin}{width=.8\textwidth}

\chapter*{Preface}
\addtocontents{toc}{\protect \vspace*{13.mm}}
\addcontentsline{toc}{chapter}{\bfseries{Preface}}
\thispagestyle{plain}\markboth{Preface}{Preface}

Howdy!

\bigskip

\begin{flushright}
  Max Muster\\
  June 2018
\end{flushright}

\addtocontents{toc}{\protect \vspace*{10mm}}

\cleardoublepage
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% LaTeX file for Chapter 01



\chapter{Introduction}

Typically, we expect empirical scientific results to be distorted by random noise, such that the results are not equal to the real process that they describe. Additionally, one can reasonably assume that the results are also biased to some extent, as the statistical paradigm of precision versus bias would predict: The smaller the noise part of distortion, the more the result should be distorted by bias. However, the trade-off is not inevitable and can be minimized by scientific rigor and foresightfull experimental design. Large efforts have been made both in theory and practice to improve the quality of experiments and their analysis, such as defining experimental settings that prohibit influence of expectations of the researcher (e.g. blinding) or increase sample size in experiments. The introduction of randomized clinical trials is for example seen nowadays as a benchmark in clinical science, heavily improving the reliability of its findings. \\
It is more and more considered a new scientific field by itself to think of and argue about circumstances that improve the quality of scientific findings. Statistics is in some sense predestined to contribute to it, because probability and chance concepts are key to understanding of any empirical science. \\
There are also issues, in which statistics is not thought to be able to contribute much to understanding and to be of little help for solving them. For example, the selective attention that the agents in science are paying to new, strong and clear findings in each science is considered merely a psychological and social%, socioeconomic 
issue. It is often explained by the affinity of humans for stories rather than mere facts and for novelties. The issue that scientific findings get more attention and are more likely to be published, read and cited is known for many years, but has gained new traction in meta science when the so-called reproducibility crisis emerged in (2003 to 2005). Studies such as ... showed that even rigid prosecution of study protocols did often not lead to reproduction of previous results when experiments were repeated, which struck empirical science in its core. Most reasonably, some concluded that it was a large misunderstanding of statistical tools such as the p-value in the scientific community that lead to this situation (...). Although that some measures have been taken since then, and some progress is made, the non-reproducibility of scientific findings remain an acute threat to the relevance and reliability of science. Some even argue that certain scientific fields sooner or later will repeat the experience that for example psychology or medicine have made, and are yet to encounter their own reproducibility crisis (...). \\
Clearly, reproduction of experimental results is the gold standard of testing empirical science, because it reassures the universality and reliability by which certain procedures or effects appear if measured in the correct experimental context. However, there are also problems there. One can almost never fully exclude chance events to have played a major role in negating or reaffirming the experimental findings (which is of course always the case where noise is present in the data). Furthermore, reproduction studies are generally costly and the precise experimental conditions might be hard to reproduce because of lack of information or other reasons. \\
There is another way to assess the strength of scientific results if the experiment has been conducted more than once (however not with exactly identical protocol as in reproduction studies): Meta-analyses. In a meta-analyses, results of multiple (fairly identical) experiments (studies) are summarized to a single result. Very often, meta-analyses are not only regarded to be a synthesis of results, but of evidence, thus reflecting the overall and summarized evidence regarding to a scientific question. It is on purpose that selective attention to positive, strong results in the scientific literature have been mentioned beforehand, because it is clear how they will affect this second way of reassurance of scientific findings: The meta-analysis will again lead to irreproducible findings if it is based on a set of results that is itself biased. Thus, meta-analysis will in this case not reach its purpose of assessing reliability of science, but worsen the problem by reinforcing the confidence in the overestimated over-optimistic effects. However, and this is the main topic in this masters thesis, there are some ways to detect irregularities in the body of results that can provide indirect hints that a selective rather than a random sample from a hypothetical population of experiments is present. The abundance of the methods to link some features of the sample of experiments to publication bias, as the tendency of scientific literature to over-proportionally include large effects is generally called, also speaks for the relevance of the topic. A review in 2017 (...) for example identified 147 (!) conceptually differing methods.\\
This speaks as well for the inevitable difficulties that such methods encounter. First of all, it is most often impossible to estimate the real selection process, that is the rate by which smaller effects go unnoticed because of selective publication and reporting, because the number of missing results in the studies is not known at the first place and impossible to retrieve. Secondly, there is almost no real world data of complete and unbiased meta-analyses, such that evaluations of methods is dependent on simulations. So we have, after applying the methods, only indirect information of publication bias, and the extent to what it might influences scientific findings. Almost all the time, alternative explanations for the results of the methods might relieve the operators and publishers from the reproval of publication bias. However, given the large body of evidence for publication bias collected in other ways, those can probably be expelled most of the time, such that those methods indeed give a quantitative measure of publication bias. \\
In this masters thesis, the answers for two questions are investigated: What is the extent and effect of publication bias in clinical science? For this purpose, a dataset from the Cochrane Organization is analysed with commonly used techniques to detect and adjust for publication bias (more precisely, small study effects, as it will be seen later). The discussion of the results of the analysis will result in the light of the literature to publication bias. 
%Because the extent of the analysis that is operated here is, to my knowledge, unmatched so far, this dataset

%Before beginning with the main part of this masters thesis, I want to recall some of the most pointed evidence for publication bias that has been collected for publication bias so far. \\
 

% Meta-analysis is at the core of evidence based medicine because it allows to summarise evidence over multiple studies and provide a more broad view on success and effectivity of clinical treatments. The necessity of meta-analyses is also increased by the abundance of data and publications. Especially when the findings differ or even contradict between studies, meta-analysis is the only way to go if one wants to make decisions based on quantitative and scientific criteria.
% 
% \vspace{0mm}
% For this, meta-analyses do not only benefit research, but also clinical practice, and may lead to better health care and prevention. However, the usefulness of meta-analysis does not restrict to clinical science, but to any empirical and quantiative science. 
% 
% \vspace{0mm}
% Usually, a meta-analysis is part of a systematic review where researchers decided to summarise all research in a given field or more specifically, that concerns a given question. Meta-analysis can be applied to all studies that are approximately identical in their experimental setup and the way the outcome of the experiments is measured. In systematic reviews where meta-analyses are used, the conclusions are most often strongly based on the results and the interpretation of the meta-analysis.
% 
% \vspace{0mm}
% However, there are problems that potentially limit the validity of meta-analysis; the number of studies available can be incomplete or the results of the studies can be biased. Some of those problems can be solved or asserted by special statistical methods. 
% 
% \subsection{Small Study Effects or Publication Bias}
% When study sample size decreases, the probability of extreme and missleading results in a study increases. This becomes a problem if results are selectively published, and therefore available, based on their results. When this is the case, one speaks of a small study effect or of ``Publication bias''. 
% 
% \vspace{0mm}
% The issue has been discussed extensively in the last years, most often in the context of what has came to be known as the replication crisis. The reasons for small study effects are manifold, but originate most often in the myopical acting of agents in science and the lack of statistical education. Studies are reported by scientists, published by journals and noticed by readers more often if their findigs are positive and find e.g. a substantive difference or effect. When doing a meta-analysis, one again obtains biased results. 
% 
% \vspace{0mm}
% The reason why that is less of an issue for larger studies is that extreme results are in general less likely and that due to larger effort, a result is published although there has been no clear and positive findings.
% 
% \vspace{0mm}
% While there is generally no way to assert poor study quality, small study effect can in principle be asserted and corrected for statistically. This masters thesis will mainly be about statistical methods to detect and adjust for small study effects. It can furthermore be divided in two parts:
% \begin{itemize}
% \item Methodological part: Collection and discussion of statistical tests and correction methods for small study effects.
% \item Applied part: Application of the methods to studies of the Cochrane Library of systematic Reviews. Subsequent discussion of the implications
% of the results for clinical science.
% \end{itemize}
% 
% In contrast to simulation studies, it is not possible to assess critical properties of the methods such as the power of a test, since the truth is not known. But based on the amount of data, one can of course try to make extrapolation to tendencies in clinical science in general. Moreover, it is still interesting to see how the methods behave in general, especially with respect to each other. It may, as an example, be possible to answer the question which statistical test is most conservative and which pooling method is most optimistic on average. Comparison with results from simulations may allow to speculate about the reasons when simulation and real world results diverge.
% 
% 
% \subsection{Cochrane and the Cochrane Database of Systematic Reviews}
% The Cochrane Organization has specialized on systematic reviews in clinical science. It publishes and maintains a library with a large number of systematic reviews that are available in some countries to the public.
% 
% \vspace{0mm}
% The data analyzed in this thesis stems completely from the Cochrane Library of systematic Reviews (cite). 
% 
% \vspace{0mm}
% The reviews are arguably of good quality, since the authors are following elaborated guidelines, and there are control-mechanisms within the organisation that should prohibit conflicts of interests. This might further improve the validity and precision of findings and conclusions that have been made based on this data. 
% 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% LaTeX file for Chapter 02










\chapter{The Cochrane Dataset} 


\section{Cochrane Systematic Reviews}
The Cochrane Group has specialized on systematic reviews in clinical science. Certain knowledge of standards and principles of the Cochrane Group may help to assess the quality and the properties of the dataset. The following information stems from the Cochrane Handbook for Systematic Reviews \citep{cochrane.handbook}. \\
The definition of a systematic review is that it ``attempts to collate all empirical evidence that fits pre-specified eligibility criteria in order to answer a specific research question.'' Thus, the ``key properties of a review are'':

\begin{itemize}
\item``a clearly stated set of objectives with pre-defined eligibility criteria for studies''
\item ``an explicit, reproducible methodology''
\item ``a systematic search that attempts to identify all studies that would meet the eligibility criteria''
\item ``an assessment of the validity of the findings of the included studies, for example through the assessment of risk of bias''
\end{itemize}

At the end of a systematic review, ``a systematic presentation, and synthesis, of the characteristics and findings of the included studies'' is done. \\
53 Cochrane Review Groups prepare and maintain the reviews within specific areas of health care. A group consists of ``researchers, healthcare professionals and people using healthcare services (consumers)''. \\
The groups are supported by Method Groups, Centers and Fields. The Cochrane Method Groups aim to discuss and consult the groups in methodological questions concerning review preparation. The Centers play a main role in training and support of the Groups. The Fields are responsible for broad medical research areas and follow priorities in those areas by advice and control of the groups. \\
The first step in a review is writing a protocol, specifying the research question, the methods to be used in literature search and analysis and the eligibility criteria of the study. Changes in protocols are possible but have to be documented and the protocol is published in advance of the publication of the full review. The choices of methodology as well as the changes should not be made ``on the basis of how they affect the outcome of the research study''. \\
In order to avoid potential conflicts of interests, there is a code of conduct that all entities of the Cochrane Organization have to agree on: conflicts of interest must be disclosed and possibly be forwarded to the Cochrane Center, and participation of review authors in the studies used have to be acknowledged. Additionally, a Steering Group publishes a report of potential conflicts of interests based on information about external funding of Cochrane Groups. \\
In order for keeping the reviews up-to-date, they are revised in a two-year circle with exceptions. In addition to inclusion of new evidence in a field, the revision and maintenance process may as well includes change in analysis methods. This can reflect some advance in clinical science as for example new information about important subgroups, as well as new methods for conducting a Cochrane Review. However, there are no clear guidelines and the Cochrane Groups are free in the rate and extent of up-dating their reviews.

\subsection{Methods for Cochrane Reviews}
A research question defines the following points: ``the types of population (participants), types of interventions (and comparisons), and the types of outcomes that are of interest''. From the research question, usually the eligibility criteria follow. Usually, outcomes are not part of eligibility criteria, except for special cases such as adverse effect reviews. \\
The type of study is an important eligibility criterium. The Cochrane Collaboration focuses ``primarily on randomized controlled trials'', and also, the methods of study identification in literature search are focused on randomized trials. Furthermore, study characteristics such as blinding of study operators with respect to treatment and cluster-randomizing might be additional eligibility criteria which have to be chosen by the review authors. \\
After having specified the eligibility criteria, studies have to be collected. The central idea of systematic reviews, and also meta-analyses, is that the collected studies are a random sample of a population of studies, i.e. that they are representative and can be used to assess population properties. Therefore, the search process is crucial, as a selective search result may impose bias on the sample of studies available, making it a non-random sample. For this purpose, the Cochrane Groups are advised to go beyond MEDLINE !!cite!!, because a search restricted to it has been shown to deliver only 30\% to 80\% of available studies. ``Time and budget restraints require the review author to balance the thoroughness of the search with efficiency in use of time and funds and the best way of achieving this balance is to be aware of, and try to minimize, the biases such as publication bias and language bias that can result from restricting searches in different ways.'' It is important to note that not only studies, but also study reports are occasionally used in the reviews, as they may provide useful information. \\
There are different sources that are being used to search for studies. 
\begin{itemize}
\item The Cochrane Central Register of Controlled Trials is a source of reports of controlled trials. ``As of January 2008 (Issue 1, 2008), CENTRAL contains nearly 530,000 citations to reports of trials and other studies potentially eligible for inclusion in Cochrane reviews, of which 310,000 trial reports are from MEDLINE, 50,000 additional trial reports are from EMBASE and the remaining 170,000 are from other sources such as other databases and handsearching.'' It includes citations published in many languages, citations only available in conference proceedings, citations from trials registers and trials results registers.
\item MEDLINE. MEDLINE includes over 16 million references to journal articles. 5,200 journals publishing in 27 languages are indexed for MEDLINE. PubMed gives access to a free version of MEDLINE with up-to-date citations. NLM gateway such as the Health Services Research Project, Meeting Abstracts and TOXLINE Subset for toxicology citations allows for search in both databases together with additional data from the US National Library of Medicine.
\item EMBASE. 4,800 Journals publishing in 30 languages are indexed to EMBASE, which includes more than 11 million records from 1974 onward. EMBASE.com also includes 7 million unique records from MEDLINE (1966 up to date) together with its own records. Additionally, EMBASE Classic allows access to digitized records from 1947 to 1973. EMBASE and MEDLINE each have around 1,800 journals not indexed in the other database.
\item Regional or national and subject specific databases can additionally be consulted and often provide important information. Financial considerations may limit the use of such databases.
\item General search engines such as Google Scholar, Intute and Turning Research into Practice (TRIP) database can be used.
\item Citation Indexes. The database lists articles published in around 6,000 Journals with articles in which they have been cited and is available online as SciSearch. This form of search is known as cited reference searching.
\item Dissertation sources. Dissertations are often listed in MEDLINE or EMBASE but one is advised to also search in specific dissertation sources.
\item Grey Literature Databases. Approximately 10\% of the results in the Cochrane Database stems from conference abstracts and other grey literature. The Institute for Scientific and Technical Information in France provides access to entries of the previously closed System for Information on Grey Literature database of the European Association for Grey Literature Exploitation). Another source is the Healthcare Management Information Consortium (HMIC) database containing records from the Library and Information Services department of the Department of Health (DH) in England and the King's Fund Information and Library Service. The National Technical Information Service (NTIS) gives access to the results of US and non-US government-sponsored research, as well as technical report for most published results. References from newsletters, magazines and technical and annual reports in behavioral science, psychology and health are provided in the PsycEXTRA database which is linked to PsycINFO database.
\end{itemize}


\subsection{Structure and Content}
The dataset consists of 5016 systematic reviews from the Cochrane Library with 52995 studies and 463820 results. A result compares a clinical or medical intervention or treatments to a control. Each study provides (multiple) results of clinical interventions. \\
In Table \ref{barbiturate.row}, two results from a systematic review about effects of barbiturates are shown as they are given in the dataset. As can be seen, further specifications are provided by the variables in the columns. \\
The comparison variable specifies \textit{what kind} of treatments or interventions are compared, the outcome variable \textit{how} it is compared, and the subgroup variable (not indicated in table) if the result belongs to a certain subgroup. Here, the result is of a binary type, so the counts of events in the barbiturate treatment group and the total number of participants are given in columns ``Events'' and ``Total'' and the number of events in the control group ``Events\_c'' and participants ``Total\_c''. A event is here death at the end of follow up.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Jun 17 16:15:38 2019
\begin{table}[ht]
\centering
\begingroup\scriptsize
\begin{tabular}{lllrrrr}
  \hline
Study & Comparison & Outcome & Events & Total & Events\_c & Total\_c \\ 
  \hline
Bohn 1989 & Barbiturate vs no barbiturate & Death at the end of follow-up & 11 & 41 & 11 & 41 \\ 
  Ward 1985 & Barbiturate vs no barbiturate & Death at the end of follow-up & 14 & 27 & 13 & 26 \\ 
   \hline
\end{tabular}
\endgroup
\caption{Example of two results as given in the dataset. Events denotes the count of events in the treatment group while Events c the count of events in the group compared to. Further descriptive variables have been ommitted} 
\label{barbiturate.row}
\end{table}


A complete listing of the variables of a result is given in Table \ref{variable}. They can roughly be separated into variables that \textit{specify the review} in which the result is contained and variables that \textit{specify the result} itself (separated by a horizontal line in Table \ref{variable}):

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{l l}
      \textbf{Variable} & \textbf{Description}\\
      \hline
      \textbf{file.nr} & The number of the file from which the review data has been \\&gathered. This file corresponds to a file available in the. \\& Cochrane library\\
      \textbf{doi} & Digital object identifier. A unique id of the review such that  \\ &the full text of the review can be found on the web.\\
      \textbf{file.index} & Internal index of the file in the Cochrane library.\\
      \textbf{file.version} & Denotes the version of the review, since the reviews are \\ &occasionally updated.\\
      %\multicolumn{2}{c}{textbf{Study level variables}}\\ 
      \hline
      \textbf{study.name} & Name of the study to which the result belongs\\
      \textbf{study.year} & Year in which the study was published\\
      \hline
      \textbf{comparison.name/.nr} & Specification of the interventions compared in the study  \\ &and a unique number for the comparison\\
      \textbf{outcome.name/.nr} & Specification by which outcome the interventions are compared\\ &and a unique number for the outcome\\
      \textbf{subgroup.name/.nr} & Potentially indication of affiliation to subgroups and a \\ &unique number for the subgroup\\
      \textbf{outcome.measure} & Indication of the quantification method of the effect \\ &(of one intervention compared to the other).\\
      \textbf{effect} & Measure of the effect given in the quantity denoted by \\ &``outcome measure''.\\
      \textbf{se} & Standard error of the measure of the effect,\\
      \textbf{events1/events2} & The counts of patients with an outcome \textit{if}\\ &measurement/outcome is binary or dichotomous \\ &2 (1 for treatment group and 2 for control group).\\
      \textbf{total1/total2} & Number of patients in groups.\\
      \textbf{mean1/mean2} & Mean of patient measurements \textit{if} outcome is continuous.\\
      \textbf{sd1/sd2} & Standard deviation of mean \textit{if} \\ &outcome is continuous.
    \end{tabular}
  \caption{Dataset variable names and descriptions  \label{variable}}

  \end{center}
\end{table}

Results are part of studies that are again part of a (systematic) review. This structure of a review is shown in Figure \ref{review}. 

\begin{figure}
\tikzstyle{every node}=[draw=black,thick,anchor=west]
\tikzstyle{selected}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed,fill=gray!50]
\begin{tikzpicture}
[grow = right, anchor = west, 
  growth parent anchor=east, % added code
  parent anchor=east]
  \node {Review} [edge from parent fork right]
    child { node {Comparison 2}
      child { node {Outcome 2}}
      child { node {Outcome 1}
        child { node {Subgroup 2}}
        child { node {Subgroup 1}
          child  { node {Result 2}}
          child  { node {Result 1}}
          }}
    }
    child [missing] {}		
    child { node {Comparison 1  }};
\end{tikzpicture}
\caption{Structure of a hypothetical review with two different comparisons\label{review.structure}}
\label{review.structure}
\end{figure}

\vspace{0mm}
The structure of a review will now be outlined based on an example of the dataset. Let us consider the previously mentioned barbiturate and head injury review. The aim was to ``assess the effects of barbiturates in reducing mortality, disability and raised ICP (intra-cranial pressure) in people with acute traumatic brain injury'' as well as to ``quantify any side effects resulting from the use of barbiturates''. \\
%Since there are arguments for and against use of barbiturates, the authors of the review did a comprehensive literature search and collected all available study findings. 
The review comprises five studies in total. Three of them compared barbiturate to placebo, one compared barbiturate to Mannitol and one Pentobarbital to Thiopental. The studies have different outcomes, for example, death or death and severe disability at follow up, but also dropout counts or adverse effects (secondary outcomes). 
We have continuous (e.g. mean body temperature) and binary outcome data (e.g. death/no death). One study split up outcomes for patients with and without haematoma, which would be subgroups. Thus, it is important not to confuse results with studies. A study can contribute multiple results to a systematic review, for example, primary and secondary outcomes and adverse effects. 

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Jun 17 16:15:38 2019
\begin{table}[ht]
\centering
\begingroup\footnotesize
\begin{tabular}{lll}
  \hline
Study & Comparison & Outcome \\ 
  \hline
Bohn 1989 & Barbiturate vs no barbiturate & Death at the end of follow-up \\ 
  Bohn 1989 & Barbiturate vs no barbiturate & Death or severe disability at the end of follow-up \\ 
  Eisenberg 1988 & Barbiturate vs no barbiturate & Death at the end of follow-up \\ 
  Eisenberg 1988 & Barbiturate vs no barbiturate & Uncontrolled ICP during treatment \\ 
  Eisenberg 1988 & Barbiturate vs no barbiturate & Hypotension during treatment \\ 
  Perez-Barcena 2008 & Pentobarbital vs Thiopental & Death at the end of follow-up (6 months) \\ 
  Perez-Barcena 2008 & Pentobarbital vs Thiopental & Death or severe disability at the end of follow-up (6 months) \\ 
  Perez-Barcena 2008 & Pentobarbital vs Thiopental & Uncontrolled ICP during treatment \\ 
  Perez-Barcena 2008 & Pentobarbital vs Thiopental & Hypotension during treatment \\ 
  Schwartz 1984 & Barbiturate vs Mannitol & Death at the end of follow-up (1 year) \\ 
  Schwartz 1984 & Barbiturate vs Mannitol & Death at the end of follow-up (1 year) \\ 
  Schwartz 1984 & Barbiturate vs Mannitol & Uncontrolled ICP during treatment \\ 
  Ward 1985 & Barbiturate vs no barbiturate & Death at the end of follow-up \\ 
  Ward 1985 & Barbiturate vs no barbiturate & Death or severe disability at the end of follow-up \\ 
  Ward 1985 & Barbiturate vs no barbiturate & Mean ICP during treatment \\ 
  Ward 1985 & Barbiturate vs no barbiturate & Mean arterial pressure during treatment \\ 
  Ward 1985 & Barbiturate vs no barbiturate & Hypotension during treatment \\ 
  Ward 1985 & Barbiturate vs no barbiturate & Mean body temperature during treatment \\ 
   \hline
\end{tabular}
\endgroup
\caption{Barbiturate and head injury review. In the columns, study names, comparison and outcome measure of the results are given} 
\label{barbiturates}
\end{table}


Information about missing values in the dataset is given in Table \ref{missing}. For variables as research subject, outcome and subgroup name and event counts there are no missing values. The relative amount of missing values is very low except for study years. For continuous outcomes, the cases have been counted were no treatment effect and standard error is available: neither mean values and standard deviations, nor mean differences and standard error. Also Study years before 1920 and after 2019 are declared as missing, as well as sample sizes below zero.

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Jun 17 16:15:38 2019
\begin{table}[ht]
\centering
\begingroup\footnotesize
\begin{tabular}{lr}
  \hline
  \hline
Missing mean values and mean differences & 984 \\ 
  Missing standard deviations and standard errors & 1300 \\ 
  Missing sample sizes & 12173 \\ 
  Missing study year & 44649 \\ 
   \hline
\end{tabular}
\endgroup
\caption{Number of missing variables and measurements in the dataset} 
\label{missing}
\end{table}


The studies that are included in the reviews and have been published are most often from the years after 1980 (5\% quantile = 1982). The median of the publication years is 2003, the mean 2001 and the quartiles are 1996 and 2008. Only a handful ($n = 18$) have been published in 2018, none in 2019. \\
%No information is available concerning unpublished results and studies.
The top treatment effect measure (risk ratio, mean difference, hazard ratio etc.) abundances are summarized in Table \ref{outcome.measure.frequencies}. One can conclude of the table that roughly 30 \% of outcomes in the dataset are continuous and the rest being some sort of discrete or binary outcomes, most often binary (> 65\%).

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Jun 17 16:15:38 2019
\begin{table}[ht]
\centering
\begingroup\footnotesize
\begin{tabular}{lll}
  \hline
Outcome measure & n & Percentage \\ 
  \hline
Risk Ratio & 232583 & 50.1\% \\ 
  Mean Difference & 102315 & 22.1\% \\ 
  Odds Ratio & 49372 & 10.6\% \\ 
  Std. Mean Difference & 40535 & 8.7\% \\ 
  Peto Odds Ratio & 19122 & 4.1\% \\ 
  Hazard Ratio & 6566 & 1.4\% \\ 
  Risk Difference & 6234 & 1.3\% \\ 
  Rate Ratio & 2283 & 0.5\% \\ 
  other & 4810 & 1\% \\ 
   \hline
\end{tabular}
\endgroup
\caption{Frequencies of outcome measures among results. n denotes the total number 
             of results with the outcome measure and percentage the percentage of the outcome measure,} 
\label{outcome.measure.frequencies}
\end{table}



The sample sizes among results vary to some extent. There are 5\% of treatment group sample sizes that are smaller than 8, the 5\% quantile. The first quartile is 22, the median 48, the mean 302.03 and the third quartile 119. The large difference between median and mean is caused by very large groups with over 2,000,000 participants. Analogously, the quantiles of the total sample size are: 5\% quantile = 15, first quartile = 44, median = 94 and third quartile = 229. The mean is 617.81. \\
The mean and median number of results per review are 12.42 and 7. There are 417 reviews with five or fewer results, and the quartiles are 16 and 102. Similarly, the number of reviews with a maximum of two studies included is 836, the mean study number is 12.42, the median 7 and the interquartile range 4 and 15. The discrepancy between mean and median is due to large reviews with a high number of studies and results, most extreme in \citet{largest.review} which is a systematic review about antibiotic prophylaxis for preventing infection after cesarean section, with 95 studies and 1497 results in total. \\
For results to be suitable for usage in meta-analysis, they have to be identical with respect to comparison and outcome. More specifically, the studies in the dataset that have the same comparison, outcome and subgroup can be pooled in a meta-analysis, since their research subject and experimental setup can be considered sufficiently homogeneous. Importantly, this distinction is also used by the Cochrane Organization itself, i.e. the meta-analyses are identical to the meta-analyses done in the systematic reviews.\\%Another approach that is not made here is to analyze all subgroups together, which is possible if enough studies are given.
The dataset is divided in meta-analyses with identical experimental setup. The size of a meta-analysis denotes how many results are included in a group.
Table \ref{repr.groups} shows the number of meta-analysis with size $\geq n$ results. Practically, this number of meta analyses can be performed, with each having at least $n$ results:

% latex table generated in R 3.5.1 by xtable 1.8-3 package
% Mon Jun 17 16:15:38 2019
\begin{table}[ht]
\centering
\begingroup\footnotesize
\begin{tabular}{lll}
  \hline
n & Number of groups & Cumulative sum of groups \\ 
  \hline
1 & 102344 & 186300 \\ 
  2 & 31686 & 83956 \\ 
  3 & 16072 & 52270 \\ 
  4 & 9628 & 36198 \\ 
  5 & 6444 & 26570 \\ 
  6 & 4230 & 20126 \\ 
  7 & 2961 & 15896 \\ 
  8 & 2114 & 12935 \\ 
  9 & 1592 & 10821 \\ 
  10 & 1238 & 9229 \\ 
  11 & 921 & 7991 \\ 
  12 & 702 & 7070 \\ 
  13 & 585 & 6368 \\ 
  14 & 455 & 5783 \\ 
  15 & 5328 & 5328 \\ 
   \hline
\end{tabular}
\endgroup
\caption{Cumulative number of groups with number of reproduction trials >= n} 
\label{repr.groups}
\end{table}


\section{Data Tidying and Processing} \label{sec:Processing}
The original dataset was provided by .. (some words how the dataset was obtained and processed by C.R.). \\
The information in the previous pages are from a tidied and processed version of this dataset. \\

\subsection{Modification of old variables}
In some cases, reasonable assumptions led to small changes in the originally provided variables:
\begin{itemize}
\item \textbf{study.year}: It was assumed that studies that are declared to have been published before 1920 ($n$ = 30664) are mis-specified, as well as after 2019 ($n$ = 384), so these have been set to NA. 
\item \textbf{mean1} and \textbf{mean2}: When both ``mean1'' and ``mean2'' are equal to zero or NA, ``outcome.measure.new'' is equal to ``(Std.) Mean Difference'', but a mean difference is given in ``effect'', ``mean1'' is set equal to ``effect'' ($n$ = 1593).
\end{itemize}


\subsection{Newly Introduced Variables}
Some new variables are added to the obtained dataset:
\begin{itemize}
\item \textbf{outcome.measure.new}: Outcome measure specifications given in ``outcome.measure'' were standardized whenever they were supposed to denote the same outcome measure. An example would be the formally different notations for odds ratios: ``Odds Ratio'', ``odds ratio'', ``OR'', which were all denoted as ``Odds Ratio''.
\item \textbf{outcome.type}: A simplification of ``outcome.measure.new''. The outcome.type ``bin'' indicates if ``outcome.measure.new'' is either one of ``Risk Ratio'', ''Odds Ratio'', ``Risk difference'' or ``Peto Odds Ratio''. ``cont'' is equivalent to ``outcome.measure.new'' equal to "Std. Mean Difference'' or ``Mean Difference''. ``surv'' equal to ``Hazard Ratio'' and ``rate'' equal to ''Rate Ratio'' or ``Rate Difference''.
\item \textbf{lrr} and \textbf{var.lrr}: log risk ratio and variance of the log risk ratio for outcome.type ``bin''.
\item \textbf{smd} and \textbf{var.smd}: Hedges $g$ and the variance of Hedges $g$ for outcome.type ``cont''. If not computable from ``mean1'', ``mean2'', ``sd1'' and ``sd2'', and ``outcome.measure.new'' was equal to ``Std. Mean Difference'', it was set equal to ``effect'' (and ``var.smd'' equal to ``se'' squared, $n$ =  6938).
\item \textbf{smd.ordl} and \textbf{var.smd.ordl}: Cohen's $d$ and its variance as obtained by transformation of a log odds ratio for outcome.type ``bin''.
\item \textbf{cor.Pearson} and \textbf{var.cor.Pearson}: Pearson correlation coefficient and variance as obtained from the $d$ (for outcome.type ``bin'') or $g$ (for outcome.type ``cont'') to $r$ transformation.
\item \textbf{z} and \textbf{var.z}: Fisher's z score and it's variance obtained from the Pearson correlation $r$ to $z$ transformation.
\item \textbf{pval.single}: $p$-value against the null hypothesis of no treatment effect, derived by a $t$-test for outcome.type ``cont'' or Wald test for outcome.type ``bin''.
\item \textbf{events1c} and \textbf{events2c}: Correction of ``events1'' and ``events2'' zero event counts or event counts = patient number. When no events occurred, 0.5 was added, and when all patients experienced the event, 0.5 was subtracted. When one of ``events'' had zero counts while the other had maximum counts, no adjustment occurred.
\item \textbf{meta.id}: Meta-analysis ID variable to uniquely identify any potential meta-analysis in the dataset. Consistent to what has been discussed before, all results that share a common comparison, outcome and subgroup (optional, subgroups not given in any case) may be combined in a meta-analysis.
% \item \textbf{}:
% \item \textbf{}:
\end{itemize}


\subsection{Eligibility criteria for Publication Bias Test and Adjustment} \
Initially, the analysis is restricted to binary, continuous and survival outcomes. More formally:
\begin{itemize}
\item \textbf{Outcome measures}: The analysis has been restricted on the following outcome measures (from ``outcome.measure.new''): ``Odds Ratio'', ``Risk Ratio'', ``Risk difference'', ``Peto Odds Ratio'', ``Std. Mean Difference'', ``Mean Difference'', ``Hazard Ratio'' 
($n$ = 456727 out of a total of 463820 results)
\end{itemize}

The suggestions of criteria for eligibility for publication bias tests of \citet{Ioannidis2007} have largely been followed:
\begin{itemize}
\item \textbf{Sample size}: A meta-analysis is comprised of at least ten studies ($n$ = 5797 remaining). 
\item \textbf{Heterogeneity}: The $I^2$ statistic of a given meta-analysis is smaller than 0.5, thus, the proportion of between study variance of the overall variance is smaller than 0.5 ($n$ = 4038 remaining).
\item \textbf{Study size}: The ratio between largest variance of an estimate and smallest variance of an estimate is larger than four ($n$ = 4006 remaining).
\item \textbf{Significance}: At least one treatment effect has a $p$-value below the significance threshold 0.05 ($n$ = 2673 remaining)
\end{itemize}

After having excluded meta-analyses with less than ten studies, additional criteria for excluding inconvenient meta-analyses are:
\begin{itemize}
\item \textbf{Zero events}: In the case of binary outcomes, meta-analyses with zero events in any study and any group are excluded ($n$ = 141 out of meta-analyses with at least ten studies).
\item \textbf{Missing means and sd's}: In the case of continuous outcomes, meta-analyses with means and sd's in any group being equal to zero are excluded ($n$ = 306 out of meta-analyses with at least ten studies).
\item \textbf{Single patient data}: Meta-analyses that are comprised of single patient data are excluded ($n$ = 8 out of meta-analyses with at least ten studies)
\end{itemize}

--Make Flowchart--

\subsubsection{Exclusions due to Computational Errors}
In exceptional cases, the applied meta-analysis methods and publication bias tests and adjustment algorithms failed due to various reasons. Here, those cases are listed and if known, the reasons why computation failed are given.\\
\textbf{Binary Outcomes}: Out of the meta-analyses that are of ``outcome.type'' bin, and have at least ten studies, two studies hat to be excluded:
\begin{itemize}
\item meta.id = 62301 (file.nr = 1531, comparison.nr = 8, outcome.nr = 6, subgroup.nr = 1):
%A subgroup analysis for women with increased adverse pregnancy outcomes, where the effect of vitamin C supplements is tested on pre-eclampsia: 
Largest study (se = 0.1) has risk ratio 1 and smallest study (se = 1) risk ratio 0.07. Copas publication bias adjustment methods has likelihood optimization issues.
\item meta.id = 94519 (2519, 14, 1, 2): Largest study (se = 0.05) has risk ratio 0.9 and smallest study (se = 0.4) risk ratio 3. Again Copas publication bias adjustment methods has likelihood optimization issues.
\end{itemize}

There were no issues with ``outcome.type'' ``cont'' and ``surv''. For the subsequent information, it is important to know that meta-analyses are only repeated with Hedges $g$, Pearson correlation coefficient and Fisher's Z-scores, if the outcome type is not survival and all of the above criteria are met ($n$ = 0). \\
\textbf{Pearson Correlation Coefficient}: Three studies had to be excluded when analyzing the Pearson correlation coefficients:
\begin{itemize}
\item meta.id = 157083 (file.nr = 5061, comparison.nr = 1, outcome.nr = 5, subgroup.nr = 2): A meta-analysis with all results from the same study, and equal sample size (5 each group). The Copas selection model algorithm issued an error when optimizing the likelihood.
\item meta.id = 159329 (5183, 3, 13): One very small study with group sizes of 2 and 1 patients and small risk ratio of 0.222 (compared to the largest of 1.05, se = 0.04). Also, the Copas selection model algorithm issued an error when optimizing the likelihood.
\item meta.id = 182298 (6211, 1, 8, -): One very small study with 8 patients in each group and 0 event counts in both. The Copas selection model algorithm issued an error when optimizing the likelihood.
\end{itemize}

\textbf{Fisher's Z score}: The same issues as encountered for Pearson correlation coefficients (meta.id = 157083 and 159329). The variance of the z score $s_z^2$ can be calculated as:  $s_z^2 = \frac{1}{n-3}$, $n$ being the total sample size. Thus, studies that have total sample size $n \geq 3$ are discarded in the meta-analyses ($n$ = 72 studies). 2 meta-analyses have then less than 10 studies left for pooling: 
\begin{itemize}
\item meta.id = 10671: (file.nr = 5061, comparison.nr = 1, outcome.nr = 5, subgroup.nr = 2), 7 studies left.
\item meta.id = 43324: (1019, 1, 3, 2), 9 studies left.
\end{itemize}

\subsubsection{The Publication Bias Test and Adjustment Dataset}
The dataset that was ultimately tested and adjusted for publication bias comprises 2673 meta-analyses and 42789 results. ..


% \begin{itemize}
% \item meta.id = 5229 (file.nr = 137, comparison.nr = 1, outcome.nr = 3, subgroup.nr = 1): ``Luken 1979'' (3), ``Miller-Fisher 1978'' (3), ``MÃ¼ller Forell 1978'' (2).
% \item meta.id = 136563 (4218, 1, 1, 1): ``Roberts 2010'' (3)
% \item meta.id = 136566 (4218, 1, 3, 1): ``Roberts 2010'' (3)
% \end{itemize}




%which leads to a reduction of the meta-analyses that can be calculated based on the Fisher's z score by $n$ = \Sexpr{}
























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% LaTeX file for Chapter 03

%<<echo=FALSE>>=
% library(knitr)
% opts_chunk$set(
%      fig.path='figure/ch03_fig',
%      self.contained=FALSE,
%      cache=TRUE
%  )
%@






\chapter{Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Small study effects}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# Scale for 'x' is already present. Adding another scale for 'x', which\\\#\# will replace the existing scale.\\\#\# Scale for 'x' is already present. Adding another scale for 'x', which\\\#\# will replace the existing scale.}}\end{kframe}
\end{knitrout}

The median $z$-score for a given sample size of a trial is shown in Figure \ref{z.samplesize}. It is clearly visible that the absolute value decreases with increasing sample size, i.e. that the effect size is becoming smaller. The trend flattens off after $\sim$ sample size = 400 (not shown). 
-- Appendix --

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-10-1} 

\end{knitrout}
\caption{Median of the absolute $z$-score plotted against the total sample size.}
\label{z.samplesize}
\end{figure}

Only for illustration, the same trend is reproduced in Figure \ref{effect.samplesize.separated} with a similar method, using the original effect size measures ``Odds Ratio'', ``Risk Ratio'', ``Mean Difference'' and ``Std. Mean Difference'' (the most commonly used in the dataset). The ``normalized effect'' is the original effect size, normalized with respect to all other effect sizes of the same measure (i.e. subtraction of mean and division through standard error of the mean).

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-11-1} 

\end{knitrout}
\caption{Median of the absolute value of the normalized, original effect size plotted against the total sample size.}
\label{effect.samplesize.separated}
\end{figure}

%Show absolute effect size depending on scaled time? Chunck 3


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Small Study Effect Tests}


The results of the test for small study effects for the meta-analyses fullfilling the criteria from chapter \ref{chapter01}, section \ref{sec:Processing}, are presented in Figures \ref{fig:test.bin}, \ref{fig:test.cont} and \ref{fig:test.surv}. The histogram of the $p$-values appears most often skewed to the right, indicating evidence for small study effects. Because different tests are applied depending on outcome type, the results are displayed separately for binary, continuous and survival outcome types (as previously defined in the dataset variable ``outcome.type'', see chapter \ref{ch:chapter01}, section \ref{sec:Processing}. \\
For continuous and survival outcomes, the names refer to: 

\begin{itemize}
\item Egger's test, the weigthed linear regression test as described in section \ref{sec:Egger}
\item Thompson and Sharp's test, the weighted linear regression test adjusted for between-study heterogeneity, section \ref{sec:Thompson}
\item Begg and Mazumdar's test, the rank test described in section \ref{sec:Begg}
\end{itemize}

For binary outcomes, the names refer to:
\begin{itemize}
\item Harbord's test, the likelihood score based test (section \ref{sec:Harbord})
\item Peter's test, the weighted linear regression with inverse sample size as explanatory variable (study size proxy) described in section \ref{sec:Peter}
\item R\"ucker's test, the test based on the arcsine transformation of proportions, in combination with Thompson and Sharp's regression test (section \ref{sec:Rucker})
\item Schwarzer's test, the rank based test using the expected event counts computed with the hypergeometric distribution (section \ref{sec:Schwarzer})
\end{itemize}


\begin{figure}[!ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-13-1} 

\end{knitrout}
\caption{Histogram of the $p$-values for small study effect in binary outcome meta-analyses. The testing method is indicated in the header, binwidth is equal to 0.1. The significant proportion based on the threshold of 0.1 is displayed inside the figures.}
\label{fig:test.bin}
\end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-14-1} 

\end{knitrout}
\caption{Histogram of the $p$-values for the small study effect in continuous outcome meta-analyses. The testing method is indicated in the header, binwidth is equal to 0.1. The significant proportion based on the threshold of 0.1 is displayed inside the figures.}
\label{fig:test.cont}
\end{figure}

\begin{figure}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-15-1} 

\end{knitrout}
\caption{Histogram of the $p$-values for the small study effect in survival outcome meta-analyses. The testing method is indicated in the header, binwidth is equal to 0.1. The significant proportion based on the threshold of 0.1 is displayed inside the figures.}
\label{fig:test.surv}
\end{figure}



\subsection{}




















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

















% 
% <<echo=FALSE, warning=FALSE>>=
% #Test Results: Binary
% test.bin <- meta.bin %>% ungroup() %>% summarize(egger.test = mean(egger.test),
%                                                           schwarzer.test = mean(schwarzer.test),
%                                                           rucker.test = mean(rucker.test),
%                                                           harbord.test = mean(harbord.test),
%                                                           peter.test = mean(peter.test))
% test.bin <- test.bin %>% gather(key = "test.type", value = "mean")
% 
% p.bin <- meta.bin %>% ungroup() %>% 
%   select(egger.test, schwarzer.test, rucker.test, harbord.test, peter.test) %>% 
%   gather(key = "test.type", value = "null.hypothesis") %>%  
%   mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
%   ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
%   theme_bw() + xlab(label = NULL) +
%   annotate("text", x = test.bin$test.type, y = 1000, 
%            label = paste(round(test.bin$mean, 2)*100, "% rejected"), 
%            color = "white")
% 
% 
% 
% test.sig.bin <- meta.bin %>% filter(sig.fixef.bin == 1) %>% ungroup() %>% summarize(egger.test = mean(egger.test),
%                                                               schwarzer.test = mean(schwarzer.test),
%                                                               rucker.test = mean(rucker.test),
%                                                               harbord.test = mean(harbord.test),
%                                                               peter.test = mean(peter.test))
% test.sig.bin <- test.sig.bin %>% gather(key = "test.type", value = "mean")
% 
% p1 <- meta.bin %>% ungroup() %>% filter(sig.fixef.bin == 1) %>% 
% select(egger.test, schwarzer.test, rucker.test, harbord.test, peter.test) %>% 
%   gather(key = "test.type", value = "null.hypothesis") %>% 
%   mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
%   ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
%   theme_bw() + ggtitle("Significant Pooled Effects")+ xlab(label = NULL) + theme(legend.position="none") +
%   annotate("text", x = test.sig.bin$test.type, y = 1750, 
%            label = paste(round(test.sig.bin$mean, 2)*100, "% rejected"), 
%            color = "white")
% 
% test.nonsig.bin <- meta.bin %>% filter(sig.fixef.bin == 0) %>% ungroup() %>% summarize(egger.test = mean(egger.test),
%                                                                                                 schwarzer.test = mean(schwarzer.test),
%                                                                                                 rucker.test = mean(rucker.test),
%                                                                                                 harbord.test = mean(harbord.test),
%                                                                                                 peter.test = mean(peter.test))
% test.nonsig.bin <- test.nonsig.bin %>% gather(key = "test.type", value = "mean")
% 
% p2 <- meta.bin %>% 
%   filter(sig.fixef.bin == 0) %>% ungroup() %>% 
%   select(egger.test, schwarzer.test, rucker.test, harbord.test, peter.test) %>% 
%   gather(key = "test.type", value = "null.hypothesis") %>% 
%   mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
%   ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
%   theme_bw() + ggtitle("Non-significant Pooled Effects")+ xlab(label = NULL) +theme(legend.position="none") +
%   annotate("text", x = test.nonsig.bin$test.type, y = 650, 
%            label = paste(round(test.nonsig.bin$mean, 2)*100, "% rejected"), 
%            color = "white")
% 
% range.pb.difference.bin <- range(test.sig.bin$mean - test.nonsig.bin$mean)
% 
% #Test Results: Continuous
% test.cont <- meta.cont %>% ungroup() %>% summarize(egger.test = mean(egger.test),
%                                                             begg.test = mean(begg.test),
%                                                             thomson.test = mean(thomson.test))
% 
% test.cont <- test.cont %>% gather(key = "test.type", value = "mean")
% 
% p.cont <- meta.cont %>% ungroup() %>% 
%   select(egger.test, thomson.test, begg.test) %>% 
%   gather(key = "test.type", value = "null.hypothesis") %>% 
%   mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
%   ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
%   theme_bw() + xlab(label = NULL) +
%   annotate("text", x = test.cont$test.type, y = 750, 
%            label = paste(round(test.cont$mean, 2)*100, "% rejected"), 
%            color = "white")
% 
% 
% test.sig.cont <- meta.cont %>% filter(sig.fixef.cont == 1) %>% ungroup() %>% summarize(egger.test = mean(egger.test),
%                                                                                                 begg.test = mean(begg.test),
%                                                                                                 thomson.test = mean(thomson.test))
% 
% test.sig.cont <- test.sig.cont %>% gather(key = "test.type", value = "mean")
% 
% p3 <- meta.cont %>% 
%   filter(sig.fixef.cont == 1) %>% ungroup() %>% 
%   select(egger.test, thomson.test, begg.test) %>% 
%   gather(key = "test.type", value = "null.hypothesis") %>% 
%   mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
%   ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
%   theme_bw() + ggtitle("Significant Pooled Effects")+ xlab(label = NULL) + theme(legend.position="none") +
%   annotate("text", x = test.sig.cont$test.type, y = 600, 
%            label = paste(round(test.sig.cont$mean, 2)*100, "% rejected"), 
%            color = "white")
% 
% test.nonsig.cont <- meta.cont %>% filter(sig.fixef.cont == 0) %>% ungroup() %>% summarize(egger.test = mean(egger.test),
%                                                                                                    begg.test = mean(begg.test),
%                                                                                                    thomson.test = mean(thomson.test))
% 
% test.nonsig.cont <- test.nonsig.cont %>% gather(key = "test.type", value = "mean")
% 
% p4 <- meta.cont %>% 
%   filter(sig.fixef.cont == 0) %>% ungroup() %>% 
%   select(egger.test, thomson.test, begg.test) %>% 
%   gather(key = "test.type", value = "null.hypothesis") %>% 
%   mutate(null.hypothesis = factor(ifelse(null.hypothesis == 1, "rejected", "not rejected"))) %>% 
%   ggplot(aes(x = test.type, fill = null.hypothesis)) + geom_bar() + coord_flip() + 
%   theme_bw() + ggtitle("Non-significant Pooled Effects")+ xlab(label = NULL) + theme(legend.position="none") +
%   annotate("text", x = test.nonsig.cont$test.type, y = 100, 
%            label = paste(round(test.nonsig.cont$mean, 2)*100, "% rejected"), 
%            color = "white")
% 
% range.pb.difference.cont <- range(test.sig.cont$mean - test.nonsig.cont$mean)
% 
% ########################################################################################################################################
% ########################################################################################################################################
% 
% #Agreement proportions of publication bias tests:
% 
% #Binary:
% n.bin <- dim(meta.bin)[1]
% meta.bin <- meta.bin %>%ungroup() %>%  mutate(egger.schwarzer = ifelse(egger.test == schwarzer.test, "agree", "disagree"),
%                                         egger.peter = ifelse(egger.test == peter.test, "agree", "disagree"),
%                                         egger.rucker = ifelse(egger.test == rucker.test, "agree", "disagree"),
%                                         egger.harbord = ifelse(egger.test == harbord.test, "agree", "disagree"),
%                                         schwarzer.peter = ifelse(schwarzer.test == peter.test, "agree", "disagree"),
%                                         schwarzer.rucker = ifelse(schwarzer.test == rucker.test, "agree", "disagree"),
%                                         schwarzer.harbord = ifelse(schwarzer.test == harbord.test, "agree", "disagree"),
%                                         rucker.peter = ifelse(rucker.test == peter.test, "agree", "disagree"),
%                                         rucker.harbord = ifelse(rucker.test == harbord.test, "agree", "disagree"),
%                                         harbord.peter = ifelse(harbord.test == peter.test, "agree", "disagree"))
% 
% agreement.bin <- meta.bin %>% ungroup() %>% summarise(egger.schwarzer = sum(egger.schwarzer == "agree")/n(),
%                                             egger.peter = sum(egger.peter == "agree")/n(),
%                                             egger.rucker = sum(egger.rucker == "agree")/n(),
%                                             egger.harbord = sum(egger.harbord == "agree")/n(),
%                                             schwarzer.peter = sum(schwarzer.peter == "agree")/n(),
%                                             schwarzer.rucker = sum(schwarzer.rucker == "agree")/n(),
%                                             schwarzer.harbord = sum(schwarzer.harbord == "agree")/n(),
%                                             rucker.peter = sum(rucker.peter == "agree")/n(),
%                                             harbord.peter = sum(harbord.peter == "agree")/n())
% 
% correlation.bin <- meta.bin %>%ungroup() %>%  summarise(egger.schwarzer = cor(pval.egger.bin, pval.schwarzer.bin),
%                                               egger.peter = cor(pval.egger.bin, pval.peter.bin),
%                                               egger.rucker = cor(pval.egger.bin, pval.rucker.bin),
%                                               egger.harbord = cor(pval.egger.bin, pval.harbord.bin),
%                                               schwarzer.peter = cor(pval.schwarzer.bin, pval.peter.bin),
%                                               schwarzer.rucker = cor(pval.schwarzer.bin, pval.rucker.bin),
%                                               schwarzer.harbord = cor(pval.schwarzer.bin, pval.harbord.bin),
%                                               rucker.peter = cor(pval.rucker.bin, pval.peter.bin),
%                                               harbord.peter = cor(pval.harbord.bin, pval.peter.bin))
% 
% binary.tests.agreement <- rbind(agreement.bin, correlation.bin)
% rownames(binary.tests.agreement) <- c("Test Agreement","P-value Correlation")
% 
% 
% #Continuous:
% n.cont <- dim(meta.cont)[1]
% meta.cont <- meta.cont %>% ungroup() %>% mutate(thomson.egger = ifelse(thomson.test == egger.test, "agree", "disagree"),
%                                           thomson.begg = ifelse(thomson.test == begg.test, "agree", "disagree"),
%                                           egger.begg = ifelse(egger.test == begg.test, "agree", "disagree"))
% 
% agreement.cont <- meta.cont %>% ungroup() %>%  summarise(thomson.egger = sum(thomson.egger == "agree")/n(),
%                                               thomson.begg = sum(thomson.begg == "agree")/n(),
%                                               egger.begg = sum(egger.begg == "agree")/n())
% 
% correlation.cont <- meta.cont %>% ungroup() %>% summarise(thomson.egger = cor(pval.thomson.cont, pval.egger.cont),
%                                                 thomson.begg = cor(pval.thomson.cont, pval.begg.cont),
%                                                 egger.begg = cor(pval.egger.cont, pval.begg.cont))
% 
% cont.tests.agreement <- rbind(agreement.cont, correlation.cont)
% rownames(cont.tests.agreement) <- c("Test Agreement","P-value Correlation")
% 
% ########################################################################################################################################
% ########################################################################################################################################
% 
% 
% #Trimfill missing studies fractions statistics:
% trimfill.cont.mean <- meta.cont %>% ungroup %>% summarise(mean = mean(missing.trim.cont)) %>% select(mean)
% trimfill.cont.median <- meta.cont %>% ungroup %>% summarise(median = median(missing.trim.cont)) %>% select(median)
% 
% trimfill.bin.mean <- meta.bin %>% ungroup %>% summarise(mean = mean(missing.trim.bin)) %>% select(mean)
% trimfill.bin.median <- meta.bin %>% ungroup %>% summarise(median = median(missing.trim.bin)) %>% select(median)
% 
% ########################################################################################################################################
% ########################################################################################################################################
% 
% change.ranef.bin <- meta.bin %>% ungroup() %>% 
%   select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>% 
%   group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>% 
% 	mutate(n = n/sum(n)) 
% 
% p.change.ranef.bin <- meta.bin %>% ungroup() %>% 
%   select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>%  
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>% 
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Binary outcomes")+ 
%   theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.ranef.bin[change.ranef.bin$sig.change == 3,]$correction.method, y = 1000, 
%          label = paste(round(change.ranef.bin[change.ranef.bin$sig.change == 3,]$n*100, 2), "% changed to nonsig."), 
%          color = "white")
% 
% change.ranef.cont <- meta.cont %>% ungroup() %>% 
%   select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%   
%   group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>% 
% 	mutate(n = n/sum(n))
% 
% p.change.ranef.cont <- meta.cont %>% ungroup() %>% 
%   select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>%  
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>% 
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Continuous outcomes") +
%   theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.ranef.cont[change.ranef.cont$sig.change == 3,]$correction.method, y = 500, 
%          label = paste(round(change.ranef.cont[change.ranef.cont$sig.change == 3,]$n*100, 2), "% changed to nonsig."), 
%          color = "white")
% 
% change.ranef <- meta %>% ungroup() %>% 
%   select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%   
%   group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>% 
% 	mutate(n = n/sum(n))
% 
% p.ranef.change <- meta %>% ungroup() %>% 
%   select(sig.change.ranef.reg, sig.change.ranef.copas, sig.change.ranef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>%  
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>% 
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + 
%   theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.ranef[change.ranef$sig.change == 3,]$correction.method, y = 1500, 
%          label = paste(round(change.ranef[change.ranef$sig.change == 3,]$n*100, 2), "changed to nonsig."), 
%          color = "white")
% # Significance after correction plots, separated for meta-analyses with significant publication bias test.
% #Fixed effects meta-analysis significance
% #Binary outcomes:
% change.fixef.bin <- meta.bin %>% ungroup() %>% 
%   select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%   
%   group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>% 
% 	mutate(n = n/sum(n))
% 
% p.change.fixef.bin <- meta.bin %>% ungroup() %>% 
%   select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>%  
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%   
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Binary outcomes")+  
%   theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.fixef.bin[change.fixef.bin$sig.change == 3,]$correction.method, y = 800, 
%          label = paste(round(change.fixef.bin[change.fixef.bin$sig.change == 3,]$n*100, 2), "changed to nonsig."), 
%          color = "white")
% 
% #Continuous outcomes:
% change.fixef.cont <- meta.cont %>% ungroup() %>% 
%   select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>% 
%   mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>% 
%   group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>% 
% 	mutate(n = n/sum(n))
% 
% p.change.fixef.cont <- meta.cont %>% ungroup() %>% 
%   select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>%  
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>% 
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + ggtitle("Continuous outcomes")+ 
%   theme_bw() + xlab(label = NULL) + scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.fixef.cont[change.fixef.cont$sig.change == 3,]$correction.method, y = 300, 
%          label = paste(round(change.fixef.cont[change.fixef.cont$sig.change == 3,]$n*100, 2), "changed to nonsig."), 
%          color = "white")
% 
% #Overall:
% change.fixef <- meta %>% ungroup() %>% select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%
% 	group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>% 
% 	mutate(n = n/sum(n))
% 
% p.fixef.change <- meta %>% ungroup() %>% 
%   select(sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change") %>%  
%   mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + 
%   theme_bw() + xlab(label = NULL) + guides(fill=guide_legend(title=NULL)) +
% 	scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.fixef[change.fixef$sig.change == 3,]$correction.method, y = 1500, 
%          label = paste(round(change.fixef[change.fixef$sig.change == 3,]$n*100, 2), "% changed to nonsig."), 
%          color = "white")
% 
% #Separated for meta-analyses with significant publication bias test.
% change.fixef.bias <- meta %>% 
%   select(thomson.test, rucker.test, sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change", sig.change.fixef.reg:sig.change.fixef.trimfill) %>% 
%   filter(thomson.test == 1) %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%
% 	group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>% 
% 	mutate(n = n/sum(n))
% 
% p.change.fixef.bias <- meta %>% 
%   select(thomson.test, rucker.test, sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change", sig.change.fixef.reg:sig.change.fixef.trimfill) %>% 
%   filter(thomson.test == 1) %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>% 
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + 
%   theme_bw() + xlab(label = NULL) + guides(fill=guide_legend(title=NULL)) +
% 	scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.fixef.bias[change.fixef.bias$sig.change == 3,]$correction.method, y = 250, 
%            label = paste(round(change.fixef.bias[change.fixef.bias$sig.change == 3,]$n*100, 2), "% changed to nonsig."), 
%            color = "white")
% 
% change.fixef.unbias <- meta %>% 
%   select(thomson.test, rucker.test, sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change", sig.change.fixef.reg:sig.change.fixef.trimfill) %>% 
%   filter(thomson.test == 0) %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%
% 	group_by(sig.change, correction.method) %>% count() %>% group_by(correction.method) %>%  
% 	mutate(n = n/sum(n))
% 
% p.change.fixef.unbias <- meta %>% 
%   select(thomson.test, rucker.test, sig.change.fixef.reg, sig.change.fixef.copas, sig.change.fixef.trimfill) %>% 
%   gather(key = "correction.method", value = "sig.change", sig.change.fixef.reg:sig.change.fixef.trimfill) %>% 
%   filter(thomson.test == 0) %>% 
% 	mutate(sig.change = factor(ifelse(sig.change == 0 |sig.change == 2, 0, sig.change))) %>%
%   ggplot(aes(x = correction.method, fill = sig.change)) + geom_bar() + coord_flip() + 
%   theme_bw() + xlab(label = NULL) + guides(fill=guide_legend(title=NULL)) +
% 	scale_fill_discrete(labels= c("unchanged", "change.to.sig", "change.to.nonsig")) +
%   annotate("text", x = change.fixef.unbias[change.fixef.unbias$sig.change == 3,]$correction.method, y = 1300, 
%            label = paste(round(change.fixef.unbias[change.fixef.unbias$sig.change == 3,]$n*100, 2), "% changed to nonsig."), 
%            color = "white")
% 
% 
% ########################################################################################################################################
% ########################################################################################################################################
% 
% p.missing.copas <- meta %>% ggplot(aes(x = missing.copas/n)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20)
% p.missing.trim <- meta %>% ggplot(aes(x = missing.trim)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20)
% @
% 
% There are tests that can be applied to find out if small study effects are present in the meta analysis. For the precise description, see the methods section. Application of the tests is only recommended if there are ten or more studies \citep{cochrane.handbook} that can be used, so all meta-analyses with less than ten studies have been excluded.
% 
% \vspace{0mm}
% There are modifications to make tests more appropriate in case of binary outcomes, therefore the results have been separated in continuous and dichotomous outcome test results. In Figure \ref{bias.results.cont} the proportion of test results that led to rejection of the null hypothesis of no small study effect based on the 5 \% level are shown for continuous outcomes ($n$ = n.cont)
% The same is shown in Figure \ref{bias.results.bin} for dichotomous outcome measures ($n$ = n.bin). The amount of studies varies from 5\% (Schwarzer's Test) to 13 \% (Egger's Test) for binary outcomes and 9\% (Begg and Mazumdar's Test) to 25 \% (Egger's Test) for continuous outcomes. 
% 
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% plot(p.cont)
% @
% \caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (continuous outcomes).}
% \label{bias.results.cont}
% \end{figure}
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% plot(p.bin)
% @
% \caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (dichotomous outcomes).}
% \label{bias.results.bin}
% \end{figure}
% 
% There is no substantive change in the fraction of positive test results, depending on if the pooled treatment effect size estimate is significant or not. Most substantively, the fraction of positive Egger Test results increases for significant pooled treatment effects by range.pb.difference.bin[2] (the minimal increase is range.pb.difference.bin[1] for Schwarzer's Test).
% 
% %Show plots of significant meta-analyses separated? Chunck 4
% 
% Furthermore one can look if the frequencies of tests that reject the null hypotheses change over time (mean publication year of the studies included in the meta analyses). The proportion of the test results are shown in Figure \ref{pub.bias.time.overtime}. The Figure suggests that the frequency of publication bias remains constant over time. The mean publication years have been restricted such that at least 180 meta-analyses are available per year, such that random fluctuation is restricted to some extent. The significance threshold for the $p$-values used is 0.05, and the small study effect test used is Thomson's test (with the arcsine variance stabilizing transformation function used in the case of binary outcomes).
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% meta %>% filter(mean.publication.year < 2013 & mean.publication.year > 1990) %>% 
%   ggplot(aes(x = mean.publication.year, fill = factor(thomson.test), stat(count))) + geom_density(na.rm = T, position = "fill") + 
%   labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
% @
% \caption{Proportion of test results where the null hypothesis of no small study effect is rejected over time (mean publication.year).}
% \label{pub.bias.overtime}
% \end{figure}
% 
% 
% The agreement of the tests, i.e. the proportion of meta-analyses where the rest results are equal between tests, is shown in Table \ref{agreement.bin} and Table \ref{agreement.cont}, again separated for outcome types. Agreement in tests for binary outcomes is better than continuous outcomes, with some variation between tests (binary outcomes: 83 to 91\%). Correlation varies more between tests, both for continuous and binary outcome tests.
% 
% <<results = 'asis', echo = FALSE, warning = FALSE>>=
% print(xtable(x = t(binary.tests.agreement), caption = "Proportion of tests that agree in rejection ar acceptance of the null hypothesis that there is no small study effect (dichotomous outcomes)", label = "agreement.bin",  align = "lll", digits = 2), include.rownames = T, size = "footnotesize")
% @
% 
% 
% <<results = 'asis', echo = FALSE, warning = FALSE>>=
% print(xtable(x = t(cont.tests.agreement), caption = "Proportion of tests that agree in rejection ar acceptance of the null hypothesis that there is no small study effect (continuous outcomes)", label = "agreement.cont",  align = "lll", digits = 2), include.rownames = T, size = "footnotesize")
% @
% 
% \vspace{0mm}
% Test performance depends on the sample size, despite having restricted sample size to a minimum of 10 studies. The p-values of the Thompson and Sharp tests are shown with respect to the sample size of the meta-analysis in Figure \ref{pvalues.samplesize}. %(Suggests that 10 is likely too small)
% In the case of binary outcomes, the arcsine variance stabilizing function has been applied prior to use of Thompson and Sharp's test. A trend towards more rejections for larger sample sizes can be seen.
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% meta %>% filter(n < 40) %>% ggplot(aes(stat(count), x = n, fill = factor(thomson.test))) + 
%   geom_density(position = "fill")
% @
% \caption{P-values of Thomson and Sharp's test for small study effects and their corresponding sample size.}
% \label{pvalues.samplesize}
% \end{figure}
% 
% 
% 
% \vspace{0mm}
% One can use the proportion of added studies by the trim-and-fill method from the overall number of studies to further investigate the extent of small study effects. The mean fraction of trimmed comparisons for binary outcomes is round(trimfill.bin.mean,2) and the median round(trimfill.bin.median,2). 
% % A histogram with those fractions is shown in Figure \ref{trimfill.cont} for continuous outcomes and \ref{trimfill.bin} for dichotomous outcomes. In both cases, the method very commonly adds supposedly unpublished studies to the meta-analyses. 
% In Figure \ref{trimfill.pvalues.bin} and Figure \ref{trimfill.pvalues.cont}, the relationship between fraction of added studies by trim-and-fill and the hypothesis test decisions of the small study effects tests is shown for continuous and dichotomous outcomes. In the case of Peters test dor dichotomous outcomes, there is less agreement with the trim-and fill method than in the case of Thomson and Sharp's test for continuos outcomes in the sense that the fraction of meta-analyses with rejected null hypothesises increases more clearly when there are more studies added by trim-and-fill. 
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% meta.bin %>% filter(missing.trim.bin < 0.5) %>% 
%   ggplot(aes(x = missing.trim.bin, fill = factor(peter.test), stat(count))) + 
%   geom_density(na.rm = T, position = "fill") + 
%   labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
% @
% \caption{Fraction of added studies for small study effect correction by trim and fill and the corresponding small study effect test decision (based on 5\% significance level) of Peters test and dichotomous outcomes}
% \label{trimfill.pvalues.bin}
% \end{figure}
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% meta.cont %>% filter(missing.trim.cont < 0.5) %>% 
%   ggplot(aes(x = missing.trim.cont, fill = factor(thomson.test), stat(count))) + 
%   geom_density(na.rm = T, position = "fill") + 
%   labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
% @
% \caption{Fraction of added studies for small study effect correction by trim and fill and the corresponding small study effect test decision (based on 5\% significance level) of Thomson and Sharp's test and continuous outcomes}
% \label{trimfill.pvalues.cont}
% \end{figure}
% 
% 
% \subsection{Small Study Effect Correction}
% Multiple methods are available to correct for the effects of small study effects in order to get an unbiased estimate. Three of them will be applied to the meta-analyses shown previously that have ten or more study results and are therefore eligible for testing for publication bias. 
% 
% \vspace{0mm}
% The extent to what the results of the meta-analysis results are changed can be investigated. Because statistical significance is often used to decide if there is a treatment effect, a non-significant corrected effect size estimate can indicate that an observed treatment effect has been accepted because of small study effects. Therefore, the cases have been counted in which 
% \begin{enumerate}
% \item Significance or non-significance of pooled estimate of meta-analysis did not change after correction for small study effects.
% \item Significance of pooled estimate of meta-analysis did change to non-significance after correction for small study effects.
% \item Non-significance of pooled estimate of meta-analysis did change to significance after correction for small study effects.
% \end{enumerate}
% 
% The results of this can be seen in Figure \ref{significance.change.fixed} for all three methods, comparing the significance of the corrected pooled effect size estimate with the significance of the pooled effect size estimate of the fixed effects meta-analysis. The same for significance of random effects meta-analysis is shown in Figure \ref{significance.change.random}. The significance threshold was chosen such that the $p$-value had to be < 0.05 for rejection of the null hypothesis of no treatment effect. The correction methods were trim-and-fill, copas selection model and regression with random effects and shrinkage of within-study-variance methods. More details to the applied correction methods and their application are in the methods section \ref{methods}. Notably, the correction methods has been applied to all meta-analyses, thus also for such that had no significant small study effect test result. 
% 
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% plot(p.fixef.change)
% @
% \caption{Change in signficance of fixed effects meta-analysis pooled estimate after correction.}
% \label{significance.change.fixed}
% \end{figure}
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% plot(p.ranef.change)
% @
% \caption{Change in signficance of random effects meta-analysis pooled estimate after correction.}
% \label{significance.change.random}
% \end{figure}
% 
% Since it has been previously seen in Figure \ref{test.results} that the results of small study effects vary considerably between continuous outcomes, the results in significance change from fixed effects meta-analysis can be seen separately in Figure \ref{significance.change.fixed.sep} for continuous and binary outcomes. The change in significance from random effects meta-analysis to significance of corrected estimate can be seen in Figure \ref{significance.change.fixed.sep}
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% grid.arrange(p.change.fixef.bin, p.change.fixef.cont, ncol = 1)
% @
% \caption{Change in signficance of fixed effects meta-analysis pooled estimate after correction, separated for continuous and binary outcomes.}
% \label{significance.change.fixed.sep}
% \end{figure}
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% grid.arrange(p.change.ranef.bin, p.change.ranef.cont, ncol = 1)
% @
% \caption{Change in signficance of random effects meta-analysis pooled estimate after correction, separated for continuous and binary outcomes.}
% \label{significance.change.random}
% \end{figure}
% 
% Because the real amount of publication bias in the dataset is not known, the correction method can also be applied only to meta-analyses that have publication bias according to the small study effect tests in the previous section. Because the test developed by \citet{thomson.sharp} has been applied to both binary and continuous outcome meta-analyses (in the case of binary outcomes to arcsine variance stabilized proportions), it is used as a criterium to distinguish biased from unbiased meta-analyses. The proportions of significance tests of pooled treatment effects that turned from significant to non-significant, non-significant to significant etc. are shown in Figure \ref{significance.change.fixed.sep}. Fixed effects meta-analysis has been used to determine signficance of the uncorrected estimate.
% 
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% grid.arrange(p.change.fixef.bias, p.change.fixef.unbias, ncol = 1)
% @
% \caption{Change in signficance of random effects meta-analysis pooled estimate after correction, separated for continuous and binary outcomes.}
% \label{significance.change.random}
% \end{figure}
% 
% Similarly, the number of missing studies per meta-analysis, i.e. those which have not been included because of small study effects, are estimated by the copas and trim-and-fill method and their empirical distribution is shown in histograms in Figure \ref{missing.studies.distribution}. For visualisation, the fraction of unpublished studies from the total fraction of available studies is shown.
% 
% \begin{figure}
% <<echo=FALSE, warning=FALSE>>=
% grid.arrange(p.missing.copas, p.missing.trim, ncol = 1)
% @
% \caption{Fraction of missing studies estimated of the number of total studies included in the meta-analysis for copas selection and trim-and-fill method.}
% \label{missing.studies.distribution}
% \end{figure}
% 
% 
% 
% 
% % # <<results = 'asis', Echo = FALSE>>=
% % # rejection.bin <- meta.bin %>% summarize(egger.rejection = mean(egger.test),
% % #                                             schwarzer.rejection = mean(schwarzer.test),
% % #                                             rucker.rejection = mean(rucker.test),
% % #                                             harbord.rejection = mean(harbord.test),
% % #                                             peter.rejection = mean(peter.test))
% % # print(xtable(rejection.bin), label = "bias.results", caption = "Cumulative number of groups with number of reproduction trials >= n", align = "llll", digits = 0), include.rownames = F, size = "footnotesize")
% % # @
% 
% 
% 
% 
% 
% 
% % 
% % For continuous outcomes, three tests are available: Eggers (based on linear regression), Thompson and Sharp (weighted linear regression) and Begg and Mazumdar (rank based) test. The following three figures show the distribution of p-values of the corresponding tests. Note that only meta analyses with more than 10 comparisons have been included. 
% % 
% % \vspace{0mm}
% % Since each histogram of p-values has 20 bins, the content of the bin with the smallest p-values is equal to the number of meta-analyses whose reporting bias test reports a p-value < 0.05. The fraction of those analyses in which we would reject the null-hypothesis based on the 5 \% threshold can therefore be assessed by eye, and would be for example for Eggers test somewhat less than one third of all analyses. 
% % 
% % \begin{figure}
% % <<echo=FALSE>>=
% % data %>% filter(outcome.measure == "Mean Difference" | outcome.measure == "Std. Mean Difference") %>% #filter(file.nr < 503) %>% 
% %   filter(sd1 > 0 & sd2 > 0 ) %>% filter(!is.na(sd1) & !is.na(sd2)) %>% 
% %   filter(mean1 != 0 | mean2 != 0 ) %>% filter(!is.na(mean1) & !is.na(mean2)) %>% 
% %   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
% %   mutate(n = n()) %>% filter(n > 9) %>% 
% %   summarize(pval = metabias(metacont(n.e = total1, mean.e = mean1, sd.e = sd1, n.c = total2, mean.c = mean2, sd.c = sd2), 
% %                             method = "linreg")$p.val) %>% 
% %   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
% %   theme_bw() + labs(title = "Eggers Reporting Bias Test P-values for Continuous Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% % @
% % \caption{Histogram of p-values for Eggers reporting bias test (linear regression based) for continuous outcome meta analysis.}
% % \label{egger.cont}
% % \end{figure}
% % 
% % 
% % \begin{figure}
% % <<echo=FALSE>>=
% % data %>% filter(outcome.measure == "Mean Difference" | outcome.measure == "Std. Mean Difference") %>% #filter(file.nr < 503) %>% 
% %   filter(sd1 > 0 & sd2 > 0 ) %>% filter(!is.na(sd1) & !is.na(sd2)) %>% 
% %   filter(mean1 != 0 | mean2 != 0 ) %>% filter(!is.na(mean1) & !is.na(mean2)) %>% 
% %   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
% %   mutate(n = n()) %>% filter(n > 9) %>% 
% %   summarize(pval = metabias(metacont(n.e = total1, mean.e = mean1, sd.e = sd1, n.c = total2, mean.c = mean2, sd.c = sd2), 
% %                             method = "mm")$p.val) %>% 
% %   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
% %   theme_bw() + labs(title = "Thomson Sharp Reporting Bias Test P-values for Continuous Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% % @
% % \caption{Histogram of p-values for Thompsom and Sharp reporting bias test (weighted linear regression based) for continuous outcome meta analysis.}
% % \label{thomson.cont}
% % \end{figure}
% % 
% % \begin{figure}
% % <<echo=FALSE>>=
% % data %>% filter(outcome.measure == "Mean Difference" | outcome.measure == "Std. Mean Difference") %>% #filter(file.nr < 503) %>% 
% %   filter(sd1 > 0 & sd2 > 0 ) %>% filter(!is.na(sd1) & !is.na(sd2)) %>% 
% %   filter(mean1 != 0 | mean2 != 0 ) %>% filter(!is.na(mean1) & !is.na(mean2)) %>% 
% %   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
% %   mutate(n = n()) %>% filter(n > 9) %>% 
% %   summarize(pval = metabias(metacont(n.e = total1, mean.e = mean1, sd.e = sd1, n.c = total2, mean.c = mean2, sd.c = sd2), 
% %                             method = "rank")$p.val) %>% 
% %   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
% %   theme_bw() + labs(title = "Begg and Mazumdar Reporting Bias Test P-values for Continuous Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% % @
% % \caption{Histogram of p-values for Begg and Mazumdar reporting bias test (rank based) for continuous outcome meta analysis.}
% % \label{begg.cont}
% % \end{figure}
% % 
% % 
% % For binary outcomes, Peters and Harbords reporting bias test have been chosen. Also here, only meta-analyses with more than 10 comparisons are included.
% % 
% % \begin{figure}
% % <<echo=FALSE>>=
% % data %>% filter(outcome.measure == "Risk Ratio" | outcome.measure == "Odds Ratio") %>% filter(file.nr != 3014) %>% 
% %   filter(events1 > 0 | events2 > 0) %>% 
% %   filter(total1 - events1 > 0 | total2 - events2 > 0) %>%
% %   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
% %   mutate(n = n()) %>% filter(n > 9) %>% 
% %   summarize(pval = metabias(metabin(event.e = events1, n.e = total1, event.c = events2, n.c = total2, sm = "OR"), method = "peters")$p.val) %>% 
% %   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
% %   theme_bw() + labs(title = "Peters Reporting Bias Test P-values for Binary Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% % @
% % \caption{Histogram of p-values for Peters reporting bias test (rank based) for continuous outcome meta analysis.}
% % \label{peters.bin}
% % \end{figure}
% % 
% % 
% % \begin{figure}
% % <<echo=FALSE>>=
% % data %>% filter(outcome.measure == "Risk Ratio" | outcome.measure == "Odds Ratio") %>% filter(file.nr != 3014) %>% 
% %   filter(events1 > 0 | events2 > 0) %>% 
% %   filter(total1 - events1 > 0 | total2 - events2 > 0) %>%
% %   group_by(file.nr, outcome.nr, subgroup.nr) %>% 
% %   mutate(n = n()) %>% filter(n > 9) %>% 
% %   summarize(pval = metabias(metabin(event.e = events1, n.e = total1, event.c = events2, n.c = total2, sm = "OR"), method = "score")$p.val) %>% 
% %   ggplot(aes(x = pval)) + geom_histogram(col = "gray15", fill = "dodgerblue", bins = 20) +
% %   theme_bw() + labs(title = "Harbord Reporting Bias Test P-values for Binary Outcome Meta-Analyses") + xlab("P-value") + ylab("Frequency")
% % @
% % \caption{Histogram of p-values for Harbord reporting bias test (rank based) for continuous outcome meta analysis.}
% % \label{harbord.bin}
% % \end{figure}
% 
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 1
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% % Since most of the times the study publication year is available for a given result, the fraction of significant treatment effects found is shown over time in Figure \ref{study.significance.overtime}. The $p$-value was chosen to be 0.05  only times where a reasonbly large number of effect estimates is available is shown in order to reduce random fluctuation ($n$ > 800). 
% % 
% % \begin{figure}
% % <<echo=FALSE, warning=FALSE>>=
% % data.ext %>% filter(!is.na(sig.type)) %>%  ggplot(aes(x = study.year, fill = factor(sig.type), stat(count))) + geom_density(na.rm = T, position = "fill") + 
% %   labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))+ scale_x_continuous(limits = c(1970, 2017))
% % @
% % \caption{Mean of the absolute value of the normalized effect size plotted against the total sample size.}
% % \label{study.significance.overtime}
% % \end{figure}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 2
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% % The separation of studies can also be made based on the significance of heterogeneity between them when pooling them by means of a meta-analysis. Significant heterogeneity between studies corresponds to a rejection of the null-hypothesis that all the study treatment effect estimates share the same underlying distribution. The test used to assess heterogeneity was based on the between-study heterogeneity estimate $Q$ estimated as \citet{tau.estimator} suggested. This is shown in Figure \ref{primary.secondary.significance.sep.sig}.
% % 
% % \begin{figure}
% % <<echo=FALSE, warning=FALSE>>=
% % grid.arrange(sig.meta.Q1, nonsig.meta.Q, ncol= 1)
% % @
% % \caption{Overall fraction of studies whose treatment effect estimate was significant when pooled by means of meta-analysis, separated by significance of study treatment effect estimate. The fractions have been calculated
% % by fixed-effects, random-effects and Hartung and Knapp adjusted random-effects meta-analysis}
% % \label{primary.secondary.significance.sep.sig}
% % \end{figure}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 3
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% % A second way to analyse meta-analyses is a cumulative meta-analysis that should reveal shifts in treatment effect sizes over time. This can again be done for the entire dataset, i.e. for all effect estimates and their ordering in time. It is important to scale the effect estimates here with respect to the estimates of the replication studies that are about the same subject, have the same outcome measure, etc. Also, only effect estimates that can be compared to other estimates before are included, i.e. only study results with one or more replica are included. Time needs to be scaled and normalized in order to compare multiple time-trends in effect size to each other and gain insights in the overall trend. This is done in Figure \ref{effects.overtime.separated}, separared for odds rati, risk ratio, and mean and standardized difference outcome measures. In order to reduce the spread over time of effect sizes, only studies between 1970 and 2019 were included, as well as studies with a minimal group size of 12 participants (each group).
% % 
% % \begin{figure}
% % <<echo=FALSE>>=
% % data.ext %>% filter(outcome.measure.new == "Mean Difference" | outcome.measure.new == "Std. Mean Difference" | 
% %                 outcome.measure.new == "Risk Ratio" | outcome.measure.new == "Odds Ratio") %>% 
% %   filter(total1 > 11 & total2 > 11) %>% filter(!is.na(study.year) & study.year > 1970 & study.year < 2019) %>% 
% %   group_by(meta.id) %>% mutate(n = n()) %>% filter(n > 1) %>% 
% %   mutate(scaled.effect = abs(scale(effect)), scaled.time = scale(study.year)) %>% 
% %   ggplot(aes(x = scaled.time, y = scaled.effect)) + geom_point(size = .5, alpha = 0.2)  + facet_wrap(~outcome.measure.new) +
% %   geom_smooth() + theme_bw() +  ylab("absolute scaled effect")  + ylab("scaled time")
% % @
% % \caption{Absolute values of scaled effect sizes over scaled time, separated for outcome measures.}
% % \label{effects.overtime.separated}
% % \end{figure}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 4
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% % The same plots can be shown separately for meta-analyses with significant and non-significant pooled effect sizes. This is done in Figure \ref{bias.results.cont.sep} for continuous outcomes and \ref{bias.results.bin.sep} for binary outcomes.
% % 
% % \begin{figure}
% % <<echo=FALSE, warning=FALSE>>=
% % grid.arrange(p3, p4, ncol = 2)
% % @
% % \caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (continuous outcomes).}
% % \label{bias.results.cont.sep}
% % \end{figure}
% % 
% % \begin{figure}
% % <<echo=FALSE, warning=FALSE>>=
% % grid.arrange(p1, p2, ncol = 2)
% % @
% % \caption{Proportion where the null hypothesis of no small study effect is rejected based on the 5\% significance level for different tests (dichotomous outcomes).}
% % \label{bias.results.bin.sep}
% % \end{figure}
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 5
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% % #Proportion of significant pooled estimates based on proportion of single pooled estimates
% % mly %>% filter(NA.sig.type == 0) %>% ggplot(aes(x = mean.sig.type)) + geom_histogram()
% % 
% % mly %>% filter(NA.sig.type == 0) %>% ggplot(aes(x = mean.sig.type, fill = factor(sig.ranef), stat(count))) + 
% %   geom_density(na.rm = T, position = "fill") + 
% %   labs(fill = "Significance") + scale_fill_discrete(labels= c("Yes", "No"))
% % 
% % mly %>% filter(NA.sig.type == 0) %>% ggplot(aes(x = mean.sig.type, fill = factor(sig.ranef), stat(count))) + 
% %   geom_histogram(position = "fill")
% % 
% % mly %>% filter(NA.sig.type == 0) %>% ggplot(aes(x = mean.sig.type, fill = factor(sig.ranef))) + 
% %   geom_histogram()
% 
% % \begin{figure}
% % <<echo=FALSE, warning=FALSE>>=
% % plot(p.secondary.over.meansig)
% % @
% % \caption{Fraction of significant meta-analysis, dependent on the fraction of significant single results.}
% % \label{secondary.significance.over.meansig}
% % \end{figure}
% 
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 1
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 1
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 1
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 1
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 1
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %Chunck 1
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% LaTeX file for Chapter 04


\chapter{Methods}
\section{Introduction and Notation}
The analysis has already been said to be restricted on clinical or health care interventions. The interventions are restricted to comparisons of two treatment arms by some measure of sanity or worsening of health. The difference in this measure between the grouops is referred to as the treatment effect. Where it is not particularly mentioned, the term treatment effect refers to any effect measure such as log risk ratio, log hazard ratio, Cohen's $d$ or Hedges $g$, Fisher's z score or Pearsons correlation coefficient. \\
Let us consider a meta-analysis with $n$ study treatment effects ($n > 1$, but typically small). A study is indexed by $i$, and it's treatment effect by  $\theta_i$. The observed treatment effect is $\hat{\theta}_i$. The pooled treatment effect of a meta-analysis will be denoted as $\theta_M$, and consequently, the observed pooled treatment effect as $\hat{\theta}_M$. Furthermore, each treatment effect is typically measured with some standard error $s_i$ and an estimate of $s_i$ is denoted as $\hat{s}_i$. The $\hat{}$ sign thus indicates if it is an estimate.\\
For continuous outcomes, let $m_t$ be the mean of the treatment group, $m_c$ the mean of the control group, and equivalently $\textrm{sd}_t$ and $\textrm{sd}_c$ the corresponding standard deviations. $n_t$ and $n_c$ are the total number of participants in the groups.
In the case of binary outcomes, let $e_t$ be the count of events in the treatment arm $e_c$ the count of events in the control group. The observed counts in a study $i$ are referred to as $e_{t,i}$ and analogously $e_{c,i}$.

\section{Effect Measures and $p$-values}
\subsection{Continuous Outcomes}
For given $(m_t, m_c), (\textrm{sd}_t, \textrm{sd}_c)$ and $(n_t, n_c)$, one can compute mean difference as well as a standardized mean difference (here: Hedges $g$) and  a standard error thereof. Note that the definition of Hedges $g$ and its standard error $s$ varies among the literature, the following applies for this report:

\begin{align}
s &= \sqrt{\frac{(n_t - 1)\textrm{sd}_t^2 + (n_c - 1)\textrm{sd}_c^2}{n_t + n_c - 2}} & g &= \frac{m_t - m_c}{s} \label{eq:hedges.g}
\end{align}

The mean difference $\theta$ and its standard error $s$ can similarly be obtained by

\begin{align}
\theta &= m_t - m_c & s &= \sqrt{\textrm{sd}_t^2/n_t + \textrm{sd}_c^2/n_t}
\end{align}

Both estimators take into account that the two groups might have unequal variances. A $p$-value to test the null hypothesis that the mean between group is equal is commonly obtained with the students $t$ test. The $t$ statistic is obtained, using $s$ and $g$ from \ref{eq:hedges.g}, by

\begin{align}
t &= g/(s\sqrt{(1/n_t)+(1/n_c)}) \nonumber
\end{align}
and the $p$-value can be obtained with the cumulative student's $t$-distribution $F$ with $n_t + n_c - 2$ degrees of freedom:

\begin{align}
p &= 2(1-F(|t|)) \nonumber
\end{align}

The t-test is known to be not very reliable if combined sample size is small ($n < 30$), see for example \citet{t.test}.

\subsection{Binary Outcomes}
Two commonly used effect measures for binary outcome data are risk ratios and odds ratios between treatment and control group. 
The methods presented here can also be found, for example, in \cite[34]{Intro.meta}.
Let $\theta$ be the logarithm to base 10 of the odds ratio. $\theta$ and its variance $s^2$ can be obtained by 

\begin{align}
\hat{\theta} &= \log(\frac{e_t*(n_c - e_c)}{e_c*(n_t - e_t)}) \nonumber \\
\hat{s}^2 &= 1/e_t + 1/(n_t - e_t) + 1/e_c + 1/(n_c - e_c) \nonumber
\end{align}

Plugging in the observed counts will give the corresponding estimates. The logarithm of the risk ratio $\theta$ and its variance $s^2$ is similarly defined as

\begin{align}
\theta &= \log(\frac{e_t/n_t}{e_c/n_c}) \label{eq:risk.ratio} \\
s^2 &= 1/e_t + 1/n_t + 1/e_c + 1/e_t \label{eq:risk.ratio.variance}
\end{align}

Using likelihood theory, one could show that the estimators are maximum likelihood estimators and that one can use the asymptotic normal distribution of the maximum likelihood estimator to calculate a $p$-value, e..g. \cite[98]{held2014}. \\
Thus, with $\Phi$ as the cumulative standard normal distribution, we get
\begin{align}
p &= 2*(1-\Phi(|\hat{\theta}/\hat{s}|) \nonumber
\end{align}
, a $p$-value for the corresponding estimates, which summarizes the evidence against $\hat{\theta}$ being zero (i.e. the true risk/odds ratio being 1). \\
Binary and continuous effect measures can be converted into each other, as described in section \ref{sec:transformation.effectsizes}.

\subsection{Survival Outcomes}
Time-to-event data with censoring has to be analyzed by special means. One frequently used method to take into account right-censoring is the Cox proportional hazards regression model \citep{Cox}. Because the method itself is not applied in this thesis, but only the resulting estimates of the parameters are used, the reader is referred to the extensive literature covering this topic (e.g. \citet{Surv}). \\
The so-called hazard ratio estimated by cox p.h. regression is the ratio of the instantaneous risk of experiencing the event between two groups. Because it is a maximum likelihood estimator, one can again use its Wald test statistic to test for equal hazards. Let $\hat{\theta}$ be an estimate of the log hazard ratio and $\hat{s}$ an estimate of the standard error of it. As before
\begin{align}
p &= 2*(1-\phi(|\hat{\theta}/\hat{s}|) \nonumber
\end{align}
will give a $p$-value for the evidence against the null hypothesis.



\section{Fixed and Random Effects Meta-Analysis} \label{sec:meta.analysis}
The fixed effects meta-analysis estimator of the pooled treatment effect is a mean of the single treatment effect estimators, weighted by their standard errors \citep{fixed.effects.rosenthal}. Let $w_i = 1/s_i^2$ be the weights, and $\theta_M$ be the pooled estimator and $s_M^2$ its variance. Then 

\begin{align}
\theta_M &= \frac{\sum_{i = 1}^n w_i \theta_i}{\sum_{i = 1}^n w_i} &
s_M^2 &= \frac{1}{\sum_{i = 1}^n w_i} \label{eq:fixed.effects}
\end{align}

This estimator minimizes the variance between the effects. An estimate $\hat{\theta}_M$ can be obtained by plugging in the observed treatment effects and variances $\hat{\theta}_i, \hat{s}_i^2$. The underlying idea is that we assume $\theta_i \sim N(\theta_M, s_i^2)$, $\theta_M$ being the true effect, all $\theta_i$ being distributed around an equal mean. \\
The random effects model \citep{whitehead} assumes instead that 
\begin{align}
\theta_i \sim N(\mu_i, s_i^2) &&
\mu_i \sim N(\theta_M, \tau^2) \label{eq:random.effects} 
\end{align}

Marginally, we have $\theta_i$ being distributed around a common mean $\theta_M$ with additional variance $\tau^2$:

\begin{align}
\theta_i | \mu_i \sim N(\theta_M, s_i^2 + \tau^2) \nonumber %\label{eq:random.effects.marginal}
\end{align}

$\tau^2$ is often referred to as a population variance or between-study variance, whereas $s_i^2$ can be interpreted as sampling error. The pooled treatment effect estimate $\theta_M$ of the random effects model and its variance is obtained by replacing the weights $w_i$ in equation \ref{eq:fixed.effects} with $w_i = 1/(s_i^2 + \tau^2)$. \\
The model is superior to the fixed effects model whenever the standard errors of the treatment effects alone are unlikely to fully account for the entire variability observed between studies. Note that as $\tau^2$ increases, each $\theta_i$ will eventually get equal weights, irrespective of it's sampling error $s_i^2$.\\
The estimation of $\tau^2$ has been subject to some debate in the statistical literature. Oftentimes, the method of moment estimator of \citet{tau.estimator} is used.
We use the measure of heterogeneity, $Q$, and divide by $C$ after having subtracted the degrees of freedom $n-1$:

\begin{align}
Q &= \sum_{i = 1}^n w_i(y_i - \theta_M)^2 & C &= \sum_{i = 1}^n w_i - \frac{\sum_{i = 1}^n w_i^2}{\sum_{i = 1}^n w_i}\label{eq:Q.heterogeneity} \\
\tau^2 &= \max(0, \frac{Q - (n-1)}{C}) \label{eq:Tau.definition}
\end{align}

Again, $w_i, \theta_i ...$ have to be replaced by their estimates in order to get an estimate $\hat{\tau}^2$. \\
The Paule-Mandel estimator \citep{paulemandel} is considered to have most often better properties than the method of moments estimator (e.g. \citet{tau.estimator.evaluation}).
Since we defined $w_i = 1/(s_i^2 + \tau^2)$, it als holds that

\begin{align}
w_i \textrm{Var}(\theta_i) &= 1 & \textrm{Var}(\sqrt{w_i}\theta_i) = 1 \nonumber
\end{align}

For any $w_i$, the variance can be estimated and equated to its expected value:

\begin{align}
s^2(w_i\theta_i) &= \frac{\sum_{i = 1}^n w_i(\theta_i - \theta_M)^2}{n-1} & \frac{\sum_{i = 1}^n w_i(\theta_i - \theta_M)^2}{n-1} = 1 \label{eq:paulemandel}
\end{align}

$\theta_M$ can be estimated using equation \ref{eq:fixed.effects}, the only problem is to estimate $\tau^2$. It can be obtained through an iterative process, using a newly defined function

\begin{align}
F(\tau^2) &= \sum_{i = 1}^n w_i(\theta_i - \theta_M)^2 - (n-1) \nonumber
\end{align}

In view of equation \ref{eq:paulemandel}, $\tau^2$ must be such that $F(\tau^2) = 0$. Then, we start with a arbitrary $\tau^2$ and repeatedly add a term $\tau_0^2$ to update $\tau^2$ until $F(\tau^2 + \tau_0^2)$ is close to zero (using $\tau^2 + \tau_0^2$ for $\hat{w}_i, \hat{\theta}_M$). Using a truncated Taylor series expansion, one can obtain the partial derivative after $\tau^2$, which is a reasonable choice for $\tau_0^2$. \\
%Using $\tau^2 + \tau_0^2$ for $\hat{w}_i, \hat{theta}$, we can update $F(\tau^2)$ and check convergence to zero. \\
The estimation of $\tau^2$ is accompanied by uncertainty. A common procedure is to test if it is there is significant heterogeneity between the studies \cite[109]{Intro.meta}. Compute $Q$, as given in \ref{eq:Q.heterogeneity}, based on one of the estimators of $\tau^2$. It is assumed that $Q$ follows a central Chi-squared distribution with $n -1$ degrees of freedom under the null hypothesis of equally distributed effect sizes. Thus, the expected value of $Q$ is $n-1$, and the excess dispersion is $Q - n + 1$. The $p$-value against the null hypothesis of equally distributed effect sizes is $1 - F(Q)$, using $F$ as the cumulative distribution function of the Chi-squared distribution with d.f. = $n-1$. \\
An advantage of the $\tau^2$ is that it is directly linked to the variability in the data. Additionally, one can use the $I^2$ statistic to see what portion the between study variance has of the overall variance. It is computed as

\begin{align}
I^2 &= (Q - n + 1)/Q \nonumber%\label{I2.proportion}
\end{align}

%Large values for $\hat{I}^2$ are of great importance when analyzing e.g. small study effects. \\
Importantly, all proposed methods above assume normally distributed effect sizes and proper estimates $\hat{s}$ of the true standard error. This assumptions are not met for very small sample sizes and very few event counts. Alternatively, the mantel-haenszel method for risk and odds ratios (see e.g. \citet{mantel.haenszel}) could be used in the latter case. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Small Study Effects Tests}
The tests that will be presented on the following pages are a common way to detect publication bias. Importantly, they are however not interpretable directly as evidence for publication bias, but this is left for the discussion chapter. The tests are also often referred to as funnel plot asymmetry tests, because of the popularity of a recent test that has been used frequently to test and adjust for publication bias, which goes under the name of trim-and-fill \citet{trimfill}. However, it is known for some time that the method has disadvantageous properties, therefore, it will not be discussed here, as well as the funnel plot (the radial plot will be used as an alternative).\\
A test for small study effects will test in some ways if the size of the estimated treatment effect of a study depends on some measure of its size. An association between study size and treatment effect size can be interpreted as an artifact of publication bias.

\subsection{Continuous Outcome Tests}
For a continuous outcomes that are normally distributed, the sample mean and variance are independent of each other \cite[120]{meta.w.R}. Thus, the estimated standard errors of treatment effects can be used as a proxy for study size, as they should in principle not be tied to the effect size.

\subsubsection{Begg and Mazumdar: Rank Correlation Test} \label{sec:Begg}
\citet{begg.ties} proposed a rank based test to test the null hypothesis of no correlation between effect size and variance.
A standardized effect size $\theta_i^\star$ can be computed as in \ref{begg.stand.eff}. $s_i^{2\star}$ is the variance of $\theta_i - \theta_M$ as defined in \ref{begg.stand.var}, $\theta_M$ being the fixed effects pooled treatment effect (\ref{eq:fixed.effects}. 

\begin{align}
\theta_i^\star &= (\theta_i - \theta_M)/s_i^{2\star} \label{begg.stand.eff}  \\
s_i^{2\star} &= s_i^2 - 1/\sum_{i = 1}^n\frac{1}{s_i^2}1 \label{begg.stand.var} 
\end{align}

A rank correlation test based on Kendall's tau is then used. First, the pairs are ordered after their ranks based on $s^{2\star}$. Then, for each $s^{2\star}$ rank, the corresponding ranks based on $\theta^{2\star}$ that are larger are counted and summed up to $u$. The number of ranks based on $\theta^{2\star}$ that are in contrary, smaller, are counted and summed up to $l$. Then the normalized test statistic $Z$ is given as

\begin{align}
Z &= (u - l)/\sqrt{n(n-1)(2n + 5)/18} \nonumber
\end{align}

Thus, large number of concordance between pairs will reflect in large $\hat{u}$ and small $\hat{l}$ and thus lead to a large $\hat{Z}$. The $p$-value is obtained using the standard normal distribution $\Phi$:
\begin{align}
p &= 2*(1-\Phi(|Z|)) \nonumber
\end{align}

The changes that have to be made int the case of ties are small and can be found in \cite[410]{begg.ties}.


\subsubsection{Egger's Test: Weighted Linear Regression Test} \label{sec:Egger}
First, the concept of simple linear regression is introduced. In short, the model assumes a dependent variable $y$ to be a linear function of another explanatory variable $x$:

\begin{align}
y &= \beta_0 + \beta_1 x + \epsilon, & \epsilon \sim N(0, \sigma^2) \label{eq:simple.regression}
\end{align}

$\epsilon$ is the residual noise term that becomes necessary when $n$ pairs $(x_i, y_i)$ are given and there is no exact solution. Then it is common to look for the solution that minimizes the squared residuals, the least-squares solution. Formally,

\begin{align}
\operatorname*{argmin}_{\beta_0, \beta_1}(\sum_{i = 1}^n y_i - \beta_0 - \beta_1 x_i) \label{eq:least.squares}
\end{align}

Let $\mathbf{X}$ be a matrix with the explanatory variables $x$ and $\mathbf{y}$ a corresponding vector for all $y$:

\begin{equation*}
\mathbf{X} = 
\begin{bmatrix}
1 & x_{12} \\
1 & x_{22} \\
1 & x_{32} \\
\vdots & \vdots \\
1 & x_{n2} \\
\end{bmatrix} 
\qquad
\mathbf{y} = 
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
\vdots \\
y_n \\
\end{bmatrix}
\end{equation*}

Let $\mathbf{\beta} = (\beta_0, \beta_1)^\top$. It can be shown that 

\begin{align}
\hat{\mathbf{\beta}} &= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \label{eq:regression.parameters}
\end{align}

Is an estimator of $\mathbf{\beta}$ that minimizes the squared residuals. Let $\hat{\mathbf{y}} = \mathbf{X}\hat{\mathbf{\beta}}$ and $\hat{\mathbf{r}} = \hat{\mathbf{y}} - \mathbf{y}$. The variance estimates $\hat{\sigma}^2$ and $\hat{\mathbf{s}}_\beta^2$ are

\begin{align}
\hat{\sigma}^2 &= \frac{1}{n-2}\mathbf{r}^\top \hat{\mathbf{r}} & \hat{\mathbf{s}}_\beta^2 &= \hat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1} \label{eq:regression.variances}
\end{align}

The estimate $\hat{\beta}_0$ is the intercept and $\hat{\beta}_1$ the slope of the regression line. Furthermore, in the simple linear regression setting, $\hat{\beta}_0$ can also be obtained by:

\begin{align}
% \hat{\beta_1} &= \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^n (x_i - \bar{x})^2} &
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \nonumber
\end{align}

$\bar{x}, \bar{y}$ denoting the sample means of the corresponding values $x_1, .., x_n$ and $y_1, ..., y_n$. Thus, $\hat{\beta}_0$ is also called global mean.
To test whether there is evidence for the intercept $\beta_0$ to be unequal to some value $\beta_{H0}$, a $t$-test can be used. 

\begin{align}
p &= 2(1-F(|(\beta_0 - \beta_{H0})/s_{\beta_0}|)) \nonumber
\end{align}

where $F$ is the cumulative $t$ distribution with $n-2$ degrees of freedom. $p$ will give the evidence against the null hypothesis $\beta_0 = \beta_{H0}$. \\
The concept is extendable to weighted linear regression. Weighted linear regression may be used if the residuals $\mathbf{r}$ have unequal variances, which is equivalent to ascribe different precision to the observed $y$. The least squares equation \ref{eq:least.squares} is extended to

\begin{align}
\operatorname*{argmin}_{\beta_0, \beta_1}(\sum_{i = 1}^n w_i(y_i - \beta_0 - \beta_1 x_i)) \nonumber
\end{align}

with positive weights $w_i$ that penalize large squared residuals for some $i$ more if $w_i$ is larger compared to other $w_i$. \\
Let $\mathbf{W}$ be a $n \times n$ matrix with $\mathbf{W}_{ii} = w_i$, the weights on the diagonal and zeros on the off-diagonals. The estimates in \ref{eq:regression.parameters} and \ref{eq:regression.variances} can still be used if $\mathbf{X}$ is exchanged with $\mathbf{X}^\star =  \mathbf{W} \mathbf{X}$ and $\mathbf{y}$ with $\mathbf{y}^\star = \mathbf{W} \mathbf{y}$. \label{weighted.regression} \\
Now it is shown how linear regression can be used to test dependency of effect sizes on study sizes. The simplest application was introduced by \citet{Egger}.
Let $\theta/s$ be the dependent variable $y$ and $1/s$ the explanatory variable $x$. If plotted, this corresponds to a radial or Galbraith plot \citet{galbraith}. The linear regression equation as introduced before in \ref{eq:simple.regression} can be written in two ways:

\begin{align}
\theta/s &= \beta_0 + \beta_1/s + \epsilon, & \epsilon \sim N(0, \sigma^2) \label{eq:radial.plot} 
\end{align}

\ref{eq:radial.plot} is often provided due to the correspondence to the radial plot. However, it is equivalent to

\begin{align}
\theta &= \beta_0 + \beta_1 s + \epsilon, & \epsilon \sim N(0, w^{-1}\sigma^2) \label{eq:egger.plot}
\end{align}

with weights $w = 1/s^2$. Thus testing $\beta_0$ of \ref{eq:radial.plot} or $\beta_1$ is equivalent. The corresponding $p$-value is then used as evidence for a small study effect. Plugging in  $\theta_i/s_i, 1/s_i$ as $y_i, x_i$ into equation \ref{eq:regression.parameters} and \ref{eq:regression.variances} will give the estimates for $\hat{\beta}_0, \hat{\beta}_1,\hat{s}_{\beta_0}$ and $\hat{s}_{\beta_1}$.

% \begin{align}
% \hat{\beta_1} &= \frac{\sum_{i = 1}^n (\varphi_i - \bar{\varphi})(\theta^\star_i - \bar{\theta}^\star)}{\sum_{i = 1}^n (\varphi_i - \bar{\varphi})^2} &
% \hat{\beta}_0 &= \bar{\theta}^\star - \hat{\beta}_1 \bar{\varphi} \nonumber
% \end{align}


\subsubsection{Thompson and Sharp's Test: Weighted Linear Regression Random Effects Test} \label{sec:Thompson}
A method proposed in \citet{thompson.sharp} allows for between study variance $\tau^2$, as introduced before in section \ref{sec:meta.analysis}. It extends the previously seen linear regression approach with $x = 1/s$ and $y = \theta/s$ by introducing weights. The effect size $\theta_i$ is assumed to be distributed as

\begin{align}
\theta_i \sim N(\beta_0 + \beta_1 s_i, s_i^2 + \tau^2) \label{t.sharp.regression}
\end{align}

$\tau^2$ is estimated as in equation \ref{eq:Tau.definition} (method of moments). %or \ref{eq:paule.mandel}. 
The weights are set as $w_i = 1/\sqrt{s_i^2 + \tau^2})$. After adjusting for the weights as described in \ref{weighted.regression}, we can proceed analogous to Egger's test. The $p$-value for $\beta_{0} \neq 0$ reflects the evidence for a small study effect.

\subsection{Dichotomous Outcomes Tests}
 
The issue with dichotomous outcomes is that effect size and variance of effect size are correlated, which can readily be seen in \ref{eq:risk.ratio} and \ref{eq:risk.ratio.variance}. For example, a small number of event counts in one or group will inflate the variance and the effect size. Consequently, the tests above will tend to reject the null-hypothesis too often, i.e. report false positives. A number of solutions to this problem are provided in the literature.


\subsubsection{Peters Test: Weighted Linear Regression Test} \label{sec:Peter}
Instead of taking the standard error $s$ as explanatory variable $x$ as in Egger's Test, the inverse of the total sample size is used. Additionally, the variances $s_i^2$ are used as weights. Thus, the subsequent test procedure is identical to Egger's test. Peters test is a a small modification of Macaskill's test where the explanatory variable is the sample size instead of its inverse.


\subsubsection{Harbord's Test: Score based Test} \label{sec:Harbord}
A rank based alternative to Peters test for binary outcomes is Harbord's test \citep{Harbord}.
It uses a different treatment effect and variance estimate: the score $\varphi$ of the log-likelihood, evaluated at log odds ratio $\theta_0 = 0$ and its inverse Fisher information $s^2$. Formally,

\begin{align}
\varphi &= e_t - (e_t - e_c)(e_t + (n_t - e_t))/(n_t + n_c) \label{harbord.score} \\
s^2 &= \frac{(e_t + e_c)(e_t + (n_t - e_t))(e_c + (n_c - e_c))((n_t - e_t) + (n_c - e_c))}{(n_t + n_c)^2(n_t + n_c - 1)} \label{harbord.variance}
\end{align}

It can be shown that they are both good approximations of the log odds ratio and its variance if the real $\theta$ is not too far from zero. The standardized estimator $r_i/v_i$ is also known as Pet0 odds ratio. The obtained scores and variances can be used in Egger's test as treatment effects and variances.


\subsubsection{Schwarzer's Test: Rank Correlation Test} \label{sec:Schwarzer}
\citet{Schwarzer} developed a test for the correlation between $E_t - \mathbb{E}(E_t)$ and the variance of $E_t$, $E_t$ being a random variable from the non-central hypergeometric distribution, assuming a fixed log odds ratio. \\
$\mathbb{E}(E_t)$ and variance of $E_t$ are then estimated based on $e_t$. The standardized cell count deviation 
\begin{align}
(e_t - \mathbb{E}(E_t))/\sqrt(s_i^2)
\end{align}
and the inverse of $s_i^2$ is then used as before in Begg and Mazumdar's test.

\subsubsection{R\"ucker's Test: Using the Variance Stabilizing Transformation for Binomial Random Variables} \label{sec:Rucker}
The correlation between variance and effect size of dichotomous outcome measures can be abolished by the variance stabilizing transformation for binomial random variables. We use that the arcsine function is the variance stabilizing transformation for a proportion. Let

\begin{align}
\theta_i = \arcsin{e_t/n_t} - \arcsin{e_c/n_c} \nonumber &
s_i^2 = 1/4n_t + +/4n_c \nonumber
\end{align}

Then one can optionally apply Begg and Mazumdar's rank correlation test or Thompson and Sharp's test using the newly obtained estimates.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Small study effect and Publication Bias Adjustment}
There are different approaches to correct for small study effects and publication bias. They can mainly be distinguished by their underlying methods: regression based approaches aim to regress the effect to a study with infinite precision (i.e. very small standard error) or to a summary effect, corrected for publication bias. Selection models aim to simulate different, hypothetical selection processes and attain a approximate treatment effect by sensitivity analysis:

\subsection{Adjustment by Regression}
\citet{limitmeta} use a random effects model %together with shrinkage procedure 
to obtain an unbiased estimate. Similarly to 
%what has been seen in Copas selection model, we let $y_i$ depend on the intercept $\beta_{0}$ and on its standard error $\sqrt{v_i}$ as in \ref{limitmeta.regression}.
regression based tests for small study effects, we have

\begin{align}
\theta_i & = \beta_0 + \beta_1\sqrt{s_i^2 + \tau^2} + \epsilon_i\sqrt{v_i + \tau^2}, \epsilon_i \stackrel{\textrm{iid}}{\sim} N(0,1)  \label{eq:limitmeta.regression}
\end{align}

The only difference between Thompson and Sharp's variant is $x = \sqrt{s^2 + \tau^2}$ instead of $x = \sqrt{s^2}$. $\beta_{1}$ represents the bias introduced by small study effects, as illustrated in the following equations:

\begin{align}
\mathbb{E}((\theta_i - \beta_0)/\sqrt{s_i^2}) \rightarrow \beta_1 \textrm{ if } s_i \rightarrow \infty \nonumber \\ %\label{limitmeta.infinitesample} \\
\mathbb{E}(theta_i) \rightarrow \beta_0 + \beta_1 \tau \textrm{if} s_i \rightarrow 0 \nonumber
\end{align}

After estimating $\tau^2$, one can estimate $\beta_{0}$ and $\beta_{1}$ as seen before in the simple linear regression framework. Now we have basically two possible estimates at hand:
\begin{itemize}
\item $\beta_0$ the treatment effect without any influence of study precision with standard error $s_{\beta_0}$
\item $\beta_0 + \beta_1 \tau$ the treatment effect of a hypothetical study with infinite precision, corresponding standard error $s_{\beta_0} + s_{\beta_1}$
\end{itemize}

Simulations in \citet{limitmeta} suggested that the latter estimate is slightly superior with respect to coverage (and mean squared error).



\subsection{Copas Selection Model}

A method proposed in \cite{Copas1,Copas2,Copas3} 
assumes that the given sample of treatment effects and standard errors is a selected part of a larger random sample. Selection of studies depends on their effect size and variance. Smaller variance is always accompanied by larger selection probability. \\
Let $\theta_i$ be the effect size estimate of study $i$. Then 

\begin{align}
\theta_i \sim N(\mu_i, \sigma_i^2) &
\mu_i \sim N(\theta, \tau^2) \label{eq:population.model}
\end{align}

which is similar to the random-effects meta-analysis setting. $\theta$ is the population mean effect, $\sigma_i^2$ the within study variance and $\tau^2$ the between study variance. Equations in \ref{eq:population.model} are termed the \textit{population model}. \\
The \textit{selection model} is defined as follows. Suppose a selection of studies with reported ($\neq$ estimated) standard errors $s$ (likely different from $\sigma$). Only a proportion of the selection will be published, with a defining the overall proportion of published studies and b (assumed to be positive) defining how fast this proportion increases with $s$ becoming smaller. Formally,

\begin{align}
P(\textrm{select}|s) &= \Phi(a + b/s) \nonumber %\label{copas.selection1}
\end{align}

The equation can be rewritten as 

\begin{align}
z = a + b/s + \delta \nonumber %\label{copas.selection2}
\end{align}

with $\delta \sim N(0,1)$. $z$ is interpreted as the \textit{propensity for selection}. It's sign must be positive in order for the study to be selected. Thus, the larger $z$, the more likely the study will be selected. Also, $a$ is somewhat like to global retention or selection rate for each study, while $b$ decides about the decline of selection probability with increasing $s$.\\
So far, we have, for a study $i$

\begin{align}
\theta_i = \mu_i + \sigma_i\epsilon_i \nonumber \\ 
\mu_i \sim N(\theta, \tau^2) \nonumber \\
z_i = a + b/s_i + \delta_i \nonumber
\end{align}

where $(\epsilon_i, \delta_i)$ are standard normal residuals. The two models are coupled by introducing a correlation $\rho = cor(\theta_i, z_i)$ by defining $(\epsilon_i, \delta_i)$ as bivariate standard normals. It follows that,
%Every given study $i$ in the meta-analysis has $z_i > 0$. 
if $\rho_i$ is large and positive and $z_i > 0$, then the estimate of a study $i$ that is selected is likely to have positive $\epsilon_i$ and $\delta_i$ and thus, the true mean $\mu$ is likely to be overestimated. \\
Let $u_i = a + b/s_i$, $\lambda(u_i)$ the Mill's ratio $\phi(u_i)/\Phi(u_i)$ ($\phi$ is the standard normal density function and $\Phi$ the cdf) and $\tilde{\rho_i} = \sigma/\sqrt(\tau^2 + \sigma_i^2) \rho_i$. The probability of a study being selected, given $s_i$ and $\theta_i$, is

\begin{align}
P(\textrm{select}|s_i, \theta_i) = %P(a,b,s,y) = 
\Phi(\frac{u_i + \tilde{\rho_i}((\theta_i - \mu)/\sqrt(\tau^2 + \sigma_i^2))}{\sqrt{1 - \tilde{\rho_i^2}}}) \nonumber
\end{align}

Which again shows that larger $s_i$ and $\theta_i$ lead to a larger selection probability. It can also be shown that the expected value 

\begin{align}
\mathbb{E}(\theta_i|s_i, \textrm{select}) = \mu + \rho_i\sigma_i\lambda(u_i) \label{eq:copas.expectation}
\end{align}

which shows that the expected value for a study is larger for larger $\sigma$. \\
A likelihood for $\theta_i$, conditional on $z>0$ can be formulated to estimate the parameters of the model. Regarding $a$ and $b$, there is no way that they can be estimated because the number of missing studies and their effect sizes is not known. Instead, fixed values for $a$ and $b$ have to be chosen.
The nuisance parameter $\sigma_i$ can be replaced by an estimate if the sample size in the studies is large enough. Since

\begin{align}
\textrm{Var}(\theta_i| s_i, z_i > 0) = \sigma_i^2 (1 - c_i^2\rho_i^2) \nonumber
\end{align}

with $c^2 = \lambda(u_i)(u_i + \lambda(u_i))$, we can replace $\sigma_i^2$ by $\hat{\sigma}_i^2 = \frac{1}{1-c_i^2\rho_i^2}$. Although one has to evaluate the likelihood for fixed pairs $(a,b)$, one can compare the fit of the model to evaluate which one is more suitable:
With equation \ref{eq:copas.expectation}, one can obtain fitted values of $\theta_i$ based on $s_i$. Also, for two different pairs $(a,b)$, $(a^\star, b^\star)$,

\begin{align}
\mathbb{E}(\theta_i|z_i > 0, a^\star, b^\star) - \mathbb{E}(\theta_i|z_i > 0, a, b) \approx c^\star + \rho(\lambda(a^\star) - \lambda(a))s_i \nonumber
\end{align}

and that local departures of $\theta_i$ can be approximated by adding a linear term in $s_i$ to the expectation of $\theta_i$. Thus, to test a pair $(a,b)$, it is sufficient to test $\beta \neq 0$ in

\begin{align}
\theta_i &= \theta + \beta s_i + \sigma_i \epsilon_i \nonumber
\end{align}

with restriction that $\rho \geq 0$. To test some pair $(a,b)$ against the scenario with no selection, we set $a^\star = \infty$ (or $\rho = 0$). A likelihood ratio test will give a test statistic to test against $H0 =$ no selection:

\begin{align}
\chi^2 &= 2*(\operatorname*{max1}_{\theta, \tau, \beta}\tilde{L}(\theta, \tau, \beta) - \operatorname*{max}_{\theta, \tau}\tilde{L}(\theta, \tau, 0)) \label{eq:copas.small.study}
\end{align}

with 
\begin{align}
\tilde{L}(\theta, \tau, \beta) = -\frac{1}{2}\sum_{i = 1}^n[\log(\tau^2 + \sigma_i^2) + \frac{(\theta_i - \theta - \beta s_i)^2}{(\tau^2 + \sigma_i^2)}] \nonumber
\end{align}

$\chi^2$ can be used with a $\chi^2$ distribution with one degree of freedom to obtain a $p$-value. Note that the test is almost equivalent to Egger's small study effect test with $\tau^2 = 0$. Thus, although the copas selection model models publication bias, it is dependent on the small study effects to find the most suitable pair $(a,b)$. \\
If one applies the model to a single meta-analysis, a sensitivity analysis is suggested. One can observe how $\theta$ and it's confidence intervals change dependent on the underlying selection process. Selection models are in general not recommended for inference (e.g. \citet{selection.assessment}). 
\citet{limitmeta} have shown how the method can be implemented in a simulation for inference purposes. \\
A range of values of $(a,b)$ are applied, and the test for residual small study effect as described in equation \ref{eq:copas.small.study} is applied. If all obtained $p$-values from the test are above a threshold 0.1, this is interpreted as no evidence, and no need for modelling, and the standard, classical random effects meta-analysis is retained. If none of the $p$-values is above the threshold, a wider range of values for $(a,b)$ is used. When some $p$-values are above, and some below the threshold, the pair $(a,b)$ with the smallest number of missing studies is retained (that is, the least intense underlying selection model is chosen).\\
Currently, there is no test to detect miss-specifications in the model itself, the authors themselves have argued that a non-parametric test of the residuals would lack power.


\section{Transformation between effect sizes} \label{sec:transformation.effectsizes}
Assuming that binary outcomes result from a dichotomization of originally continuous random variables, in this case, the logistic distribution, a transformation from typical binary effect measures to Cohen's $d$ can be achieved \cite[47]{Intro.meta}. \\
Let $\theta$ be a log odds ratio and $s$ its standard error. Cohen's $d$ and it's variance $s_d^2$ is obtained by

\begin{align}
d &= \theta \frac{\sqrt{3}}{\pi} & s_d^2 = s^2 \frac{\sqrt{3}}{\pi} \nonumber
\end{align}

$\frac{\pi}{\sqrt{3}} = 1.81$ is the standard deviation of the logistic distribution $L(\mu, \eta)$ with scale parameter $\eta = 1$, so we just divide the log odds ratio and it's variance through the standard deviation. The approximation works only well if $e_t$ and $e_c$ are not very small, especially in the case of $s_d^2$. Plugging in the observed log odds ratio $\hat{\theta}$ and $\hat{s}^2$ will give an estimate of Cohen's $d$.\\
Pearson's correlation can be attained by the formulas (\citet{olkin1985dtor}, \cite[48]{Intro.meta})

\begin{align}
r &= \frac{d}{\sqrt{d^2 + a}} & a = (n_c + n_t)^2 / n_c n_t \nonumber
\end{align}
where $a$ is a correction factor if $n_t \neq n_c$. The variance of $r$, $s_r^2$ is computed by
\begin{align}
s_r^2 &= \frac{a^2 s_d^2}{(d^2 + a)^3} \nonumber
\end{align}

Finally, we can get to a fisher's z-scaled correlation $z$ and it's variance $s_z^2$ by using
\begin{align}
z &= 0.5 \ln(\frac{1 + r}{1 - r}) & \nonumber
s_z^2 &= \frac{1}{n-3}
\end{align}















%SIMPLE LINEAR REGRESSION
% \begin{align}
% \hat{\mathbf{\beta}} &= (\mathbf{X}^\top  \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y} \label{eq:regression.parameters}
% \end{align}

% There is a closed form solution for $\hat{\beta_0}, \hat{\beta}_1$ that minimize the sum of squares:
% 
% \begin{align}
% \hat{\beta_1} &= \frac{\sum_{i = 1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^n (x_i - \bar{x})^2} & 
% \hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \label{eq:regression.estimator}
% \end{align}
% 
% $\bar{x}, \bar{y}$ denotes the sample mean of the corresponding values $x_1, .., x_n$ and $y_1, ..., y_n$. $\hat{\beta}_0$ is the intercept and $\hat{\beta}_1$ the slope of the fitted line. Let $\hat{y}_i = \beta_0 + \beta_1 x_i$ and $e_i = y_i - \hat{y}_i$. The variances $s^2_\beta$
% 
% 
% 
% To test whether the intercept or the slope is close to a certain value, a $t$-test can be used. Simply taking the absolute value of $\beta_0/s_\beta_0$,
% \begin{align}
% p &= 2(1-F(|\beta_0/s_\beta_0|)) \nonumber
% \end{align}
% 
% where $F$ is the cumulative $t$ distribution with $n-2$ degrees of freedom. The $p$-value will give the evidence against the null-hypothesis $\beta_0 = 0$. The procedure for $\beta_1$ is equivalent. \\
% To extend the model to weighted simple linear regression is straightforward. Let $w_i$ be the corresponding weight that will be attributed to the pair $x_i, y_i$. The least squares minimization problem can be restated
% 
% \begin{align}
% \operatorname*{argmin}_{\beta_0, \beta_1}(\sum_{i = 1}^n w_i(y_i - \beta_0 - \beta_1 x_i)) \nonumber
% \end{align}
% 
% And the corresponding estimators are thus
% 
% \begin{align}
% \hat{\beta_1} &= \frac{\sum_{i = 1}^n w_i(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^n w_i(x_i - \bar{x})^2} & 
% \hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \label{eq:regression.estimator}
% \end{align}
% 



% LIMITMETa
% \vspace{0mm}
% To diminuish the random variation within studies, but keep the variation between studies, we change \ref{limitmeta.regression} to a scenario where each study has $M$-fold increased precision:
% 
% \begin{align}
% y_{M,i} &= \beta_{0}^\star + \beta_{1}^\star(\sqrt{v_{i}/M + \tau^2}) + \epsilon_{i}(\sqrt{v_{i}/M + \tau^2}) \label{limitmeta.regression}
% \end{align}
% 
% Letting $M \rightarrow \infty$, we obtain:
% 
% \begin{align}
% y_{\infty,i} &= \beta_{0}^\star + \tau(\beta_{1}^\star + \epsilon_{i}),  \epsilon_{i} \stackrel{\textrm{iid}}{\sim} N(0,1) \label{limitmeta.regression.infinity}
% \end{align}
% 
% Note that:
% 
% \begin{align}
% y_{\infty,i} &= \beta_{0}^\star + \tau\beta_{1}^\star = \beta_{0}
% \end{align}
% 
% $\beta_{0}$ is termed the limit meta analysis expectation. Now, the random errors from \ref{limitmeta.regression} are rewritten as:
% 
% \begin{align}
% \epsilon_{i} &= \frac{y_{i} - \beta_{0}^\star}{\sqrt{v_{i} + \tau^2}} - \beta_{1}^\star
% \end{align}
% 
% Assuming $\epsilon_{i}$ to be fixed, we can plug it into \ref{limitmeta.regression.infinity} and get 
% 
% \begin{align}
% y_{\infty,i} &= \beta_{0}^\star + \sqrt{\frac{\tau^2}{v_{i} + \tau^2}}(y_{i} - \beta_{0}^\star)
% \end{align}
% 
% By estimating $\tau^2, v_{i}$ and  $\beta_{0}^\star$, we can use the formula to obtain a new study means, adjusted for small study effects and shrunken to a common mean. 









% \section{Basic notation}
% The notation used here will be used throughout the chapter and exceptions will be noted. Let $i$ be the number of a study of a meta analysis with $n$ being the total number of studies. $y_{i}$ is then the effect size estimate (usually log odds ratio or mean difference) and $v_{i}$ the variance of the estimate of study $i$. $w_{i}$ is used for weights which are defined when necessary, and $\Delta$ usually denotes the summarized or pooled effect estimate of the meta-analysis, and $\eta$ the variance thereof.
% 
% \vspace{0mm}
% In the case of binary outcomes, let $e_{t}$ be the number of events and $n_{t}$ be the total number of patients in the treatment arm and $n_{c}$ and $e_{c}$ analogously for the control arm in a two-armed study $i$. 
% 
% 
% \section{Transformation between effect sizes}
% Binary and continuous outcome measures can both be transformed into correlations and fishers z-scaled correlations in order to be compared. Let $\theta$ be a log odds ratio. A standardized mean difference $d$ (also known as Cohen's $d$) is calculated by multiplying $\theta$ with $\sqrt{3}/\pi$. The variance of the standardized mean difference is attained as $v_d = v_\theta \sqrt{3}/\pi$.\\
% We get from a standardized mean difference $d$ to a correlation $r$ by using 
% \begin{align}
% r &= \frac{d}{\sqrt{d^2 + a}} \nonumber
% \end{align}
% where $a$ is a correction factor if $n_t \neq n_c, a = (n_c + n_t)^2 / n_c n_t$. The variance of $r$ is computed by using 
% \begin{align}
% v_r &= v_d
% \end{align}
% 
% Finally, we can get to a fisher's z-scaled correlation $z$ and it's variance $v_z$ by using
% \begin{align}
% z &= 0.5 \ln(\frac{1 + r}{1 - r}) \nonumber
% v_z &= \frac{1}{n-3}
% \end{align}
% 
% 
% 
% \section{Heterogeneity}
% In addition to sampling error, there can be additional, ``real'' variation between estimates of different studies, indicating real differences between the studies. This is called between study variation in contrast to within study variation (noise). 
% 
% 
% \vspace{0mm}
% The $Q$ statistic is a weighted sum of squares that quantifies the deviation from the weighted mean of study effect estimates. Let $w_i$ be the inverse of the variance and $\Delta$ be a summarized effect estimate of your choice as for example a variance-weighted mean \ref{weighted.mean}. Then $Q$ can be calculated as in \ref{Q.heterogeneity}
% 
% \begin{align}
% \Delta &= \frac{\sum_{i = 1}^n w_{i}y_{i}}{\sum_{i = 1}^n w_{i}} \label{weighted.mean} \\
% Q &= \sum_{i = 1}^n w_{i}(y_{i} - \Delta)^2 \label{Q.heterogeneity}
% \end{align}
% 
% Because $Q$ is a standardized measure, it does not depend on the effect size, but only on the study number $n$. Under the assumption of equal effect sizes of all studies, the expected value of $Q$ is $n-1$, so the excess dispersion is just $Q - n + 1$.To test the assumption of equal effect sizes one uses that $Q$ follows a central Chi-squared distribution with $n -1$ degrees of freedom under the null hypothesis of equal effect sizes. $1 - F(Q)$ will provide the $p$-value for the significance test with $F$ being the cumulative distribution function of the Chi-squared distribution with the corresponding degrees of freedom.
% 
% \vspace{0mm}
% Since $Q$ is a standardized metric, it gives no impression of the real dispersion of the effect sizes. For this purpose, $\tau^2$, the variance of true effects, can be calculated. $\tau^2$ is on the same scale as the effect size and reflects the absolute amount of dispersion. In practice, $\tau^2$ can be smaller than zero, then it is set to zero.
% 
% \begin{align}
% C &= \sum_{i = 1}^n w_{i} - \frac{\sum_{i = 1}^n w_{i}^2}{\sum_{i = 1}^n w_{i}} \label{C.definition} \\
% \tau^2 &= \max(0, \frac{Q - d}{C}) \label{Tau.definition}
% \end{align}
% 
% The estimation method for $\tau^2$ is known as DerSimonian and Laird method, but others, such as restricted maximum likelihood can be used. Note that their estimate can differ substantially and consequently also the estimate of the pooled effect size estimate.
% 
% \vspace{0mm}
% To estimate the proportion of real variance between effect estimates of the observed variance, the $I^2$ can be used. The calculation is given in \ref{I2.proportion}
% 
% \begin{align}
% I^2 &= (Q - n + 1)/Q \label{I2.proportion}
% \end{align}
% 
% There are ways to compute confidence intervals for $I^2$ and $\tau^2$ that are not shown (see \cite[122]{metaanalysis}).
% 
% \section{Meta Analysis}
% There are numerous methods to pool the estimates of multiple studies into one estimate, and two will be introduced here; fixed and random effects meta-analysis.
% First the fixed effect meta-analysis will be explained. Note that both methods can be used for continuous or dichotomous outcomes. For more details about the methods, see chapter 11 and 12 in \citet{metaanalysis}
% 
% 
% \vspace{0mm}
% Let $w_i = 1/v_{i}$ be the inverse of the variance of the estimate from study $i$. The pooled estimate $\Delta_{f}$ of the fixed effects model is then the weighted average with the weights given by the inverses of the variances, $w_{i}$, given in \ref{weighted.mean}. The variance $\eta_{f}$ is the reciprocal of the sum of the weights as shown in \ref{reciproce.variance}.
% 
% \begin{align}
% \Delta_{f} &= \frac{\sum_{i = 1}^n w_{i}y_{i}}{\sum_{i = 1}^n w_{i}} \label{weighted.mean} \\
% \eta_{f} &= \frac{1}{\sum_{i = 1}^n w_{i}} \label{reciproce.variance}
% \end{align}
% 
% The computation of random effects meta-analysis is more complicated. Random effects meta-analysis will give smaller studies with larger variance more weight in the pooled estimate. Shortly, the idea is that the estimates are allowed to vary randomly around the true estimate $\Delta$, and additionally, the estimates are subject to noise or sampling error themselves. 
% 
% \vspace{0mm}
% The variance of a study estimate $y_{i}$ of study $i$, $v_{i}^\star$ is defined as in \ref{ranef.study.variance}, with $w_{i}^\star$ being the inverse of $v_{i}^\star$. It is used to calculate a new weighted mean to obtain a pooled estimate $\Delta_{r}$ as in \ref{ranef.weighted.mean}. The variance of $\Delta_{r}$, $\eta_{r}$ is then the sum of the reciprocal variances (\ref{ranef.reciproce.variance}).
% 
% \begin{align}
% v_{i}^\star &= v_{i} + \tau^2 \label{ranef.study.variance} \\
% \Delta_{r} &= \frac{\sum_{i = 1}^n w_{i}^\star y_{i}}{\sum_{i = 1}^n w_{i}^\star} \label{ranef.weighted.mean} \\
% \eta_{r} &= \frac{1}{\sum_{i = 1}^n w_{i}^\star} \label{ranef.reciproce.variance}
% \end{align}
% 
% A $p$-value under the Null-hypothesis of $\Delta = 0$ can be obtained by calculating the $Z$-value (\ref{Zvalue}) and using the distribution function of a standard normal as shown in (\ref{p.value.calculation}), $\Phi$ being the distribution function of a standard normal distribution. 
% 
% \begin{align}
% Z &= \frac{\Delta}{\sqrt{\nu}} \label{Zvalue} \\
% p &= 2(1 - \Phi(|{Z}|) \label{p.value.calculation}
% \end{align}
% 
% \section{Small Study Effects Tests}
% One crucial assumption in meta analysis is that the availability and publication of studies does not depend on their effect and the variance of the effect. If this is not given, one often speaks of publication bias. In fact, there can also be other reasons for this (see discussion section). A more appropriate term for the phenomenon is small study effect. If small study effects are present in a meta-analysis, the classical approaches to merge single study results in to an overall intervention effect fails to provide an appropriate estimate of the treatment effect. 
% 
% \subsection{Continuous Outcome Tests}
% % Essentially, there are two kinds of tests for reporting bias, non-parametrical or rank-based tests or regression based tests. 
% % First, tests for continuous outcome studies are described, and special modifications of those for binary outcomes will be introduced later.
% 
% \subsubsection{Begg and Mazumdar: Rank Correlation Test}
% \citet{begg.ties} proposed a rank based test to test the null hypothesis of no correlation between effect size and variance.
% A standardized effect size $y_{i}^\star$ can be computed as in \ref{begg.stand.eff}. $v_{i}^\star$ is the variance of $y_{i} - \Delta_{f}$ as defined in \ref{begg.stand.var}. $\Delta_{f}$ is the pooled estimate for fixed effect estimate defined in \ref{weighted.mean}. 
% 
% \begin{align}
% y_{i}^\star &= (y_{i} - \Delta_{f})/v_{i}^\star \label{begg.stand.eff} \\
% v_{i}^\star &= v_{i} - 1/\sum_{i = 1}^n v_{i}^-1 \label{begg.stand.var} 
% \end{align}
% 
% A rank correlation test based on Kendall's tau is then used. The pairs $(y_{i}^\star, v_{i}^\star)$ that are ranked in the same order are enumerated. Let $u$ be the number of pairs ranked in the same order, and $l$ the number of pairs ranked in the opposite order (e.g. larger standardized effect size and smaller variance). Then the normalized test statistic $Z$ is given in \ref{begg.teststat}. 
% 
% \begin{align}
% Z &= (u - l)/\sqrt{n(n-1)(2n + 5)/18} \label{begg.teststat}
% \end{align}
% 
% The changes in the case of ties are negligible \cite[410]{begg.ties}.
% 
% \subsubsection{Egger's Test: Linear Regression Test}
% Alternatively, one can use Eggers test \citep{Egger} that is based on linear regression. Let $y_{i}^\star = y_{i}/\sqrt{v_{i}}$ and $x_{i} = 1/\sqrt{v_{i}}$. 
% Using $y_{i}^\star$ as dependent, and  $x_{i}$ as explanatory variable in linear regression, one obtains an intercept $\beta_{0}$ and a slope. 
% 
% \vspace{0mm}
% If $\beta_{0} \ne 0$, the null hypothesis of no small study effect may be contested, using that $\beta_{0} \sim t_{n-1}$, $n-1$ being the degrees of freedom of the $t$-distribution. The $p$-value for $\beta_{0} = 0$ (no reporting bias) is then given by \ref{egger.pvalue}.
% 
% \begin{align}
% p &= 2*(1 - t_{n-1}(\beta_{0}/se(\beta_{0}))) \label{egger.pvalue}
% \end{align}
% 
% \subsubsection{Thompson and Sharp's Test: Weighted Linear Regression Random Effects Test} \label{Thompson}
% A method proposed in \citet{thompson.sharp} allows for between study heterogeneity. Let $\tau^2$ be equal to \ref{Tau.definition}. The effect size estimates are then assumed to be distributed as in \ref{t.sharp.regression}. 
% 
% \begin{align}
% y_{i} \sim N(\beta_{0} + \beta_{1}x_{i}, v_{i} + \tau^2) \label{t.sharp.regression}
% \end{align}
% 
% Then, a weighted regression is carried out with weights $1/v_{i}^\star$ based on the inverse of the variance as in \ref{ranef.study.variance}. Analogous to Egger's test, $\beta_{0}$ is then tested with respect to the null hypothesis $\beta_{0} = 0$.
% 
% \subsection{Dichotomous Outcomes Tests}
% 
% The issue with dichotomous outcomes is that effect size and variance of effect size are not independent. Consequently, the tests above will tend to reject the null-hypothesis too often, i.e. that they are not conservative enough. A number of solutions to this problem are existing in the literature.
% 
% 
% \subsubsection{Peters Test: Weighted Linear Regression Test}
% A modification of the weighted linear regression test that takes into account effect size and variance interdependence for dichotomous outcomes is proposed in \citet{Peters}.
% 
% \vspace{0mm}
% Let $y_i$ be the log-odds ratio estimate \ref{log.odds.ratio} and $v_{i}$ its variance \ref{variance.log.odds.ratio}
% 
% \begin{align}
% y_{i} &= \log(e_{t}*(n_{c} - e_{c})/e_{c}*(n_{t} - e_{t})) \label{log.odds.ratio}\\
% v_{i} &= 1/(e_{t}+(n_{t} - e_{t}) + 1/(e_{c}+(n_{c} - e_{c}) \label{variance.log.odds.ratio}
% \end{align}
% 
% and $x_{i}$ be the total sample size $n_{t} + n_{c}$. Instead of taking the variance as explanatory or independent variable in regression as in Egger's Test, the inverse of the total sample size $x_{i}$ is used, and the variance $v_{i}$ is used as a weight. The subsequent test procedure is then identical to Egger's test. 
% 
% \vspace{0mm}
% Peters test is a a small modification of Macaskill's test where the explanatory variable is the sample size instead of its inverse.
% 
% 
% \subsubsection{Harbord's Test: Score based Test}
% A rank based alternative to Peters test for binary outcomes is the Harbord's test \citep{Harbord}. The score $r_{i}$ (the first derivative of the log-likelihood of a proportion with treatment effect equal 0) and its variance $v_{i}$ can be computed as shown in \ref{harbord.score} and \ref{harbord.variance}.
% 
% \begin{align}
% r_{i} &= e_{t} - (e_{t} - e_{c})(e_{t} + (n_{t} - e_{t}))/(n_{t} + n_{c}) \label{harbord.score} \\
% v_{i} &= \frac{(e_{t} + e_{c})(e_{t} + (n_{t} - e_{t}))(e_{c} + (n_{c} - e_{c}))((n_{t} - e_{t}) + (n_{c} - e_{c}))}{(n_{t} + n_{c})^2(n_{t} + n_{c} - 1)} \label{harbord.variance}
% \end{align}
% 
% Similarly to Egger's or Peters Test, now a weighted linear regression can be performed on $r_{i}/v_{i}$ with the standard error $1/\sqrt{v_{i}}$ as explanatory variable and $1/v_{i}$ as a weight. Note that $r_{i}/v_{i}$ is also known as peto odds ratio. 
% 
% \subsubsection{Schwarzer's Test: Rank Correlation Test}
% \citet{Schwarzer} developed a test for the correlation between $e_{t} - \mathbb{E}(E_{t})$ and the variance of $E_{t}$, $E_{t}$ being a random variable from the non-central hypergeometric distribution with fixed log odds ratio. $\mathbb{E}(E_{t})$ and variance of $E_{t}$ are then estimated based on $e_{t}$.
% 
% \vspace{0mm}
% The standardized cell count deviation $(e_{t} - \mathbb{E}(E_{t}))/\sqrt(v_{i})$ and the inverse of $v_{i}$ is then used in the way as before in Begg and Mazumdar's test.
% 
% \subsubsection{R\"ucker's Test: Using the Variance Stabilizing Transformation for Binomial Random Variables}
% The correlation between variance and effect size of dichotomous outcome measures can be abolished by the variance stabilizing transformation for binomial random variables. Let 
% 
% \begin{align}
% y_{i} = \arcsin{e_{t}/n_{t}} - \arcsin{e_{c}/n_{c}} \\
% v_{i} = 1/4n_{t} + +/4n_{c}
% \end{align}
% 
% Then one can for example apply Begg and Mazumdar's rank correlation test or Thompson and Sharp's test using the newly obtained variances. 
% 
% 
% \section{Small Study Effect Adjustment}
% \subsection{Trim and Fill}
% One method to account for reporting bias in meta-analysis is to apply the Trim and Fill adjustment method \citep{trimfill}. It is a nonparametric test based on a funnel plot, on which the effect size estimates of studies are plotted against their standard error. 
% %If reporting bias is present, the estimate will shift in average towards higher or lower efffect sizes compared to the estimates of larger studies
% 
% \vspace{0mm}
% The algorithm for the method tries to estimate the number of studies $k$ that are not available due to reporting bias (different estimators are available for $k$). First, $\Delta$ is estimated using a fixed or random effects model. Then, the $k$ effect size estimates with the smallest standard errors are trimmed, and $\Delta$ is estimated again. The procedure is repeated until $k$ is 0 and the funnel plot is symmetric. The total number of missing studies is then mirrored with respect to the final effect size estimate $\Delta$, and $\Delta$ and its standard error is then computed to obtain an unbiased estimate.
% 
% \subsection{Copas Selection Model}
% 
% A method proposed in \cite{Copas1,Copas2,Copas3} assumes that there is a population of studies of which only a part has been published dependent on the variance and size of their estimated effects. Studies with small variance and large effect sizes are more likely to be published than studies with large variance and small effect sizes. Note that small effect size means here a treatment effect close to the control effect.
% 
% Let $y_i$ be the effect size estimate of study $i$. Then 
% 
% \begin{align}
% y_{i} \sim N(\mu_{i}, \sigma_{i}^2) \\
% \mu_{i} \sim N(\mu, \tau^2)
% \end{align}
% 
% corresponding to a standard random effects meta-analysis. $\mu$ is the overall mean effect, $\sigma_{i}^2$ the within study variance and $\tau^2$ the between study variance. This is the \textit{population model}.
% 
% \vspace{0mm}
% The \textit{selection model} is defined as follows. Suppose a selection of studies with reported standard errors $s$ (likely different from $\sigma$). Only a proportion 
% 
% \begin{align}
% P(\textrm{select}|s) &= \Phi(a + b/s) \label{copas.selection1}
% \end{align}
% 
% of the selection will be published, with a defining the overall proportion of published studies and b (assumed to be positive) defining how fast this proportion increases with $s$ becoming smaller. \ref{copas.selection1} can be rewritten as 
% 
% \begin{align}
% z = a + b/s + \delta \label{copas.selection2}
% \end{align}
% 
% with $\delta \sim N(0,1)$. The study with standard error $s$ is only selected if $z$ is positive. Therefore, the larger $z$, the more likely the study is selected.
% Combining population and selection model for study $i$, we have 
% 
% \begin{align}
% y_{i} = \mu_{i} + \sigma_{i}\epsilon_{i} \\
% \mu_{i} \sim N(\mu, \tau^2) \\
% z_{i} = a + b/s_{i} + \delta_{i}
% \end{align}
% 
% where $(\epsilon_{i}, \delta_{i})$ are standard normal residuals and jointly normal with correlation $\rho = cor(y_{i}, z_{i})$. Every given study $i$ in the meta-analysis has $z_{i} > 0$. If $\rho$ is large and positive and $z_{i} > 0$, then the estimate of a study $i$ that is selected is likely to have positive $\epsilon_{i}$ and $\delta_{i}$. Thus, the true mean $\mu$ is likely to be overestimated. 
% 
% \vspace{0mm}
% Let $u = a + b/s$, $\lambda(u) = \phi(u)/\Phi(u)$ ($\phi$ is the standard normal density function) and $\tilde{\rho} = \sigma/\sqrt(\tau^2 + \sigma^2) \rho$. The probability of a study being selected is
% 
% \begin{align}
% P(\textrm{select}|s, y) = P(a,b,s,y) = \Phi(\frac{u + \tilde{\rho}((y - \mu)/\sqrt(\tau^2 + \sigma^2))}{\sqrt{1 - \tilde{\rho^2}}})
% \end{align}
% 
% It can also be shown that the expected value 
% 
% \begin{align}
% \mathbb{E}(y|s, select) = \mu + \rho\sigma\lambda(u) \label{copas.expectation}
% \end{align}
% 
% which shows that the expected value for a study is larger for larger $\sigma$. 
% 
% \vspace{0mm}
% One can compute a likelihood function based on the distribution of $y$ conditional on $z > 0$. The likelihood can be maximized for any given pair $a,b$ (can not be estimated since the number of missing studies is not known), and a maximum likelihood estimate $\hat{\mu}$ for the true mean $\mu$ can be obtained. One can then perform a sensitivity analysis. First, one looks how $\hat{\mu}$ changes for different values of $a,b$. One can then compare the fitted values in \ref{copas.expectation} with the real values. To test the fit of the model (while keeping all other parts unchanged), the model can be extended in the following way :
% 
% \begin{align}
% y_{i} &= \mu_{i} + \beta s_{i} + \sigma_{i}\epsilon_{i}
% \end{align}
% 
% If we accept $\beta = 0$, then we accept that the selection model has satisfactorily explained any relationship between $y$ and $s$. Only if the value is large enough, typically $p > 0.05$, one concludes that the selection model has explained the observed data. The $p$-value is obtained by a likelihood ratio test comparing the maximum of the likelihood with the $\beta$ term added and without it, and by a likelihood ratio test.
% 
% \vspace{0mm}
% To find out if the null-hypothesis of, say, $\mu = 0$ can be rejected, another likelihood ratio test can be performed, this time with imputing $\mu = 0$ and comparing the two maximum likelihoods.
% 
% \vspace{0mm}
% In practice, only a range of values for $a,b$ are reasonable. For those values, the quantities above can be calculated and illustrated. Values for $\mu$ that have $p$-values over a predefined significance threshold can be used for inference of the effect size. 
% 
% \subsection{Adjustment by Regression}
% There are multiple ways to adjust for small study effects by regression. The general idea is to extrapolate the effect size of a study with a variance of zero based on the given effects and variances. 
% 
% \citet{limitmeta} use a random effects model together with shrinkage procedure to obtain an unbiased estimate. Similarly to what has been seen in Copas selection model, we let $y_{i}$ depend on the intercept $\beta_{0}$ and on its standard error $\sqrt{v_{i}}$ as in \ref{limitmeta.regression}.
% 
% \begin{align}
% y_{i}& = \beta_{0} + \beta_{1}(\sqrt{v_{i} + \tau^2}) + \epsilon_{i}(\sqrt{v_{i} + \tau^2}), \epsilon_{i} \stackrel{\textrm{iid}}{\sim} N(0,1)  \label{limitmeta.regression}
% \end{align}
% 
% $\beta_{1}$ represents the bias introduced by small study effects, as can be seen when looking at \ref{limitmeta.infinitesample}
% 
% \begin{align}
% \mathbb{E}((y_{i} - \beta_{0})/\sqrt{v_{i}}) \rightarrow \beta_{1} \textrm{ if } \sqrt{v_{i}} \rightarrow \infty \label{limitmeta.infinitesample} \\
% \mathbb{E}(y_{i}) \rightarrow \beta_{0} + \beta_{1}\tau \textrm{if} \sqrt{v_{i}} \rightarrow 0 
% \end{align}
% 
% After estimating $\tau^2$, one can estimate $\beta_{0}$ and $\beta_{1}$ as seen before e.g. in Thompson and Sharp's Test with weights also equal to Thompson and Sharp's Test (see \ref{Thompson}).
% 
% \vspace{0mm}
% To diminuish the random variation within studies, but keep the variation between studies, we change \ref{limitmeta.regression} to a scenario where each study has $M$-fold increased precision:
% 
% \begin{align}
% y_{M,i} &= \beta_{0}^\star + \beta_{1}^\star(\sqrt{v_{i}/M + \tau^2}) + \epsilon_{i}(\sqrt{v_{i}/M + \tau^2}) \label{limitmeta.regression}
% \end{align}
% 
% Letting $M \rightarrow \infty$, we obtain:
% 
% \begin{align}
% y_{\infty,i} &= \beta_{0}^\star + \tau(\beta_{1}^\star + \epsilon_{i}),  \epsilon_{i} \stackrel{\textrm{iid}}{\sim} N(0,1) \label{limitmeta.regression.infinity}
% \end{align}
% 
% Note that:
% 
% \begin{align}
% y_{\infty,i} &= \beta_{0}^\star + \tau\beta_{1}^\star = \beta_{0}
% \end{align}
% 
% $\beta_{0}$ is termed the limit meta analysis expectation. Now, the random errors from \ref{limitmeta.regression} are rewritten as:
% 
% \begin{align}
% \epsilon_{i} &= \frac{y_{i} - \beta_{0}^\star}{\sqrt{v_{i} + \tau^2}} - \beta_{1}^\star
% \end{align}
% 
% Assuming $\epsilon_{i}$ to be fixed, we can plug it into \ref{limitmeta.regression.infinity} and get 
% 
% \begin{align}
% y_{\infty,i} &= \beta_{0}^\star + \sqrt{\frac{\tau^2}{v_{i} + \tau^2}}(y_{i} - \beta_{0}^\star)
% \end{align}
% 
% By estimating $\tau^2, v_{i}$ and  $\beta_{0}^\star$, we can use the formula to obtain a new study means, adjusted for small study effects and shrunken to a common mean. 







% \begin{align} \label{cox reg}
% P(Y < y|\mathbf{x}) = 1 - exp(-exp(h(y) - (\mathbf{x})^\top\beta)
% \end{align}
% 
% See \cite{hothorn} for further information. It specifies the instantaneous risk of an event at time t to be
% 
% \begin{align}
% \lambda_{0}(t)exp((\mathbf{x})^\top\beta) \nonumber
% \end{align}
% 
% The model is regarded as a semi parametric model since it includes the covariates in a linear fashion but leaves the baseline hazard $\lambda_{0}(t)$ unspecified. The factor $exp((\mathbf{x})^\top\beta)$ has the interpretation of a hazard ratio between the reference with $\mathbf{x} = 0$. The formal definition of the hazard is
% 
% \begin{align}
% h(t) = \lim_{h \to 0} \frac{P(t < T < t + h| T > t)}{h}
% \end{align}
% 
% 
% \begin{align} \label{generalized linear model}
% \mathbf{Y}{_{ij}}|U{_i},\epsilon{_{ij}} = (1,x{_i}^\top)\beta + U{_i} + \epsilon{_{ij}}
% \end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%<<'child-chapter05', child='chapter05.Rnw'>>=
%@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

% LaTeX file for Chapter 01



\chapter{Appendix}

Maybe some R code here, probably a \rr{sessionInfo()}




\cleardoublepage
\phantomsection
\addtocontents{toc}{\protect \vspace*{10mm}}
\addcontentsline{toc}{chapter}{\bfseries Bibliography}


\bibliographystyle{mywiley} 
\bibliography{biblio}

\cleardoublepage

\end{document}

