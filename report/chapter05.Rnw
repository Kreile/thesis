% LaTeX file for Chapter 05
<<'preamble05',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch05_fig', 
    self.contained=FALSE,
    cache=TRUE
) 
@


\chapter{Discussion} \label{ch:Discussion}

The aim of this study was to assess publication bias in the Cochrane Library, ``the single, most reliable source of evidence in healthcare'' (citation from Cochrane). We applied a series of methods to test and adjust for publication bias in meta-analyses that matched our criteria. \\
The main result was that among the analysed meta-analyses, approximately 20\% beared evidence for publication bias. Adjustment for publication bias lead to a decrease in evidence for treatment efficacy. After comparison with previous studies that analysed publication bias in the Cochrane Library, we found that our results were similar to the results of these studies (\eg \citealp{Egger}, \citealp{Ioannidis2007}). A summary and comparison of this work to most relevant research can be found in the appendix Chapter \ref{ch:Appendix}.\\
% In this study, evidence for publication bias has been found in approximately 20\% of the meta-analyses. The results are oftentimes consistent among different tests and subsets of the data. A mixed effects meta regression confirms these findings with very large evidence for small study effects (indicating publication bias). Since the results of two-sided tests do not differ substantially from previous findings, the results are in line with previous research.\\
% There have been proposed a number of reasons for this tendencies in smaller studies (see \citealp{Egger} for a thorough discussion). In clinical settings, it might be that the requirement for more study participants in large studies might systematically change the study population. However, this systematic differences could influence the results in two ways: If recruitment of participants with a disease is difficult, and thus, also participants with less severe types of the disease are included, it might be that the effect of treatment is larger in the smaller study. But one could also argue that the opposite could eventually be the case. \\
% The main cause for funnel plot asymmetry with smaller studies having larger effects is publication bias. Publication bias enforces questionable research practices from researchers such as selective outcome reporting, $p$-hacking and data fabrication. Larger studies are less affected, and have also in general higher methodological quality \citep{Egger}, which leads to smaller effect sizes.\\
Presence of publication bias in meta-analyses can ultimately lead to patient harm if decision makers in clinical practice use the meta-analyses to decide about application of a treatment. Publication bias does in general lead to exaggerated confidence in the results of the meta-analyses. It is possible that the  evidence for a treatment in a meta-analysis is merely an artifact of publication bias. This also leads to a waste of resources in science. \\
The fact that Cochrane also uses study reports from Grey literature and other unpublished data (1.5\% of the analysed data), which contain usually smaller effect sizes, likely mitigates our estimate of publication bias.
% The term publication bias is frequently used in this study, but \Sexpr{round((length(which(meta.data$study.data_source == "UNPUB"))/dim(meta.data)[1])*100, 1)}\% of the analysis dataset (1) are unpublished results. This mitigates the estimated extent of publication bias in this study since it has been shown repeatedly that effects in the grey literature are smaller.


\section{Limitations}
This is an exploratory study, and the results are rather suggestive than confirmatory. As a part of evidence it is supposed to strengthen previous assessments of publication bias and possibly, guide further research. \\
It is clear that publication bias could only be assessed in a part of all meta-analyses of the Cochrane Library and that the extent of publication bias in the remaining data is unknown. Because one inclusion criterion was that more than ten study results had to be available for a given research question, the analysed dataset contains rather the part of data with established and enduring research. When research is more experimental and exploratory, it may well be that publication bias is an issue as well, or even more of an issue. Publication bias is possibly related to more discovery oriented, modern research \citep{ioannidis.2005}.\\
There is an unknown amount of meta-analyses with secondary outcomes in the analysis. It is not well investigated if and how secondary outcomes are affected by publication bias, and how much they are affected by between-study heterogeneity, as researchers might not follow common protocols as rigidly when assessing secondary outcomes. Together with unremoved adverse effect meta-analyses, which are likely subject to different kinds of publication bias, they can distort or mitigate the estimated extent of publication bias in this study. If one assumes that publication bias is not as strong in secondary outcomes as in primary outcomes, and that adverse effects are subject to bias for small effects (as suggested by \citealp{kicinsky}), the extent of publication bias will be underestimated.\\
Some criticism can be addressed on the methods. The small study effect which is used as an indication for publication bias can be caused by other biases such as selective outcome reporting, data fabrication, poor methodological quality of smaller studies or delayed publication of non-significant results. It can also be caused by true heterogeneity between studies; early and small studies are more likely to include high-risk patients for which treatment can have larger benefits. However, true heterogeneity between smaller and larger studies might as well mask the presence of publication bias, for example if smaller studies have smaller effects due to true between-study heterogeneity. Where effect size estimates and their standard errors are not independent (\eg log risk ratios and std. mean differences), small study effect can be a statistical artifact. However, methods to avoid this issue have largely been applied.\\
Also, small study effect tests do not use statistical significance directly. They will fail when there are no significant results, but the smaller studies have larger effects than the larger studies, in which case no publication bias for significant results could exist. The only test that is applied that takes into account statistical significance per se is the excess significance test, which is underpowered. Therefore, we restricted the meta-analyses such that at least one significant effect had to be included. \\
Most small study effect tests rely on linear regression. Linear regression is prone to outliers, especially if the sample size is small. The issue is partially resolved by weighting, but the usual procedure to assess if the model assumptions are fulfilled is not applicable for such a large number of model fits.\\
All the criticism also applies for adjustment methods, as they also rely on small study effects. A weakness of the regression adjustment is that it also adjusts if the uncertainty in the small study effect is large. To compare it in this case with the meta-analysis estimate is difficult because the uncertainty in estimating the small study effect is added to the uncertainty of the treatment effect, which inflates the uncertainity of the adjusted treatment effect. \\
Copas selection model uses the $p$-value threshold of 0.1 to decide if adjustment is necessary or not. One could argue that this threshold is somewhat arbitrary. %When assessing the evidence for the treatment effect being different from the effect under the null, a likelihood ratio test would provide more appropriate results than the Wald test. %Selection models do not account for the uncertainity that is contained in the selection process parameters they use, which makes it difficult to use their results to test against the null hypothesis of no treatment effect.\\
Another issue that the model is not estimating the selection process parameters, but rather applying some criteria of parsimony (the model that assumes fewest publication bias with $p$-value for small study effect > 0.1 is chosen). \\
Both models use various assumptions that likely affect the results. But in simulations, the methods all had fewer mean-squared error and bias than the classical meta-analysis methods when publication bias was present.\\
Publication bias can also be present if a meta-analysis lacks a small study effect, for example if significant results are published irrespective of the sign of the effect estimate. Methods as the excess significance test of \citet{excess.significance} are applied to overcome this issue, but the method is known to lack power. No methods exist so far to adjust for excess significance. This could lead to  underestimated.\\
Given the large body of evidence for publication bias by other means, it is unlikely that the unexpectedly large proportion of significant test results and the downward corrected treatment effect estimates are merely false positives or caused by other biases. Of course, the general practice is to apply all the methods in this study carefully, and with background knowledge about each single study in the meta-analyses, to differentiate between true heterogeneity and publication bias. But this does not disqualify these results, but stress the necessity that authors of meta-analyses investigate possible publication bias thoroughly. 
% Therefore, the results are not exact and could be improved if more suitable methods are applied. The Bayesian approach taken by \citet{kicinsky} has many advantages over the applied methods, \eg by accounting for statistical significance, but relies on assumptions over the weight function and is at least somewhat sensible on the choice of the prior distributions. \\
% Furthermore, the sub-partitioning of meta-analyses based on comparison, outcome and subgroup is only one possibility%, which is in fact not recommended by the Cochrane Organisation. 
% Cochrane usually performs multiple meta-analyses with the same results, for example by doing sensitivity analyses (not an issue in this study, since they are removed) and subgroup analyses and overall analyses. One can as well repeat the analysis without considering subgroups. This will have the advantage of increasing the sample size, but it will also result in increased heterogeneity in meta-analyses, and if one subgroup with small studies yields large effects, the cause must not be publication bias. \\
% It also has to be said that the removal of safety outcomes is only partially successful. This is an issue, because it is not well known if and how the publication probability is affected by the size of safety outcomes. \citet{kicinsky} suggests that they may inversely affect publication (\ie non-significant effects are preferred).\\
% Also, it has been previously discussed that the method to detect the side on which bias is not working perfectly. One could also discuss in more general terms if one can speak of publication bias in meta-analyses %that have only one significant effect out of ten ($n$ = \Sexpr{length(which(meta.f$n.sig.single == 1))}), \ie 
% if statistical significance is largely absent in a meta-analysis.\\

\subsection{A Note on the Use of Effect Size Measures}
An inevitable issue for the analysis of publication bias and adjustment for it is the choice of the effect size measure. Researchers and journal editors rely on effect sizes as mean differences or log odds ratios to assess treatment efficacy and statistical significance. Transformation of the effect sizes might change a significant result to non-significant or vice-versa. Thus, transformation will lead to a loss of accuracy. Furthermore, transformation of mean differences to std. mean differences also comes with the unpleasant consequence that the estimates of effect size and std. mean difference are no longer independent (as for mean differences, where the effect size and standard error is not mathematically linked). This will lead to false positives. When transforming odds ratios to std. mean differences, the dependence between the estimated log odds ratio and std. error is also retained, again leading to false positives. See \citet{deeks.2005} for a thorough discussion of correlation between effect sizes and standard errors in publication bias tests. Using Fisher's $z$-score is more appropriate since standard errors and effect sizes are independent.\\



\section{Outlook}
It would be possible to further narrow the subset of meta-analyses where publication bias is likely. For example, one could choose meta-analyses with large proportions of significant results. Treatment effect adjustment and consequences of adjustment for evidence for treatment effect could subsequently be analysed more carefully. Related to this, implementation of a likelihood ratio test in Copas selection model, as proposed by the authors, would be feasible and increase accuracy. \\
Many researchers have come up with hypotheses about the reasons and circumstances that promote publication bias, which could be tested if the dataset is extended. Meta meta-regression as introduced in the last chapter might be a suitable way to test these hypotheses in an exploratory manner.\\
There are numerous suggestions for different measures of publication bias and small study effects, which could be applied on the dataset (see \citealp{mueller.2016}). This study mainly relies on suggestions from \citet{Sterne}, \citet{Ioannidis2007} and \citet{limitmeta}. It uses most often methods that are well integrated in packages (\citealp{metafor.package}, \citealp{meta.package}). Although there are well addressed, mathematical justifications for these methods, there might be better methods not yet tested on large datasets. An evaluation of all suggested methods was unfortunately beyond the scope of this study.


\section{Implications}
As far as we know, this study is so far one of the largest assessment of publication bias by small study effects and uses also data that has been collected after major efforts have been made to curb publication bias. %It is the first analysis of the Cochrane Library of Systematic Reviews also analyzing publication bias using hazard and rate ratios. 
Furthermore, the analysis includes time-to-event data which is, as far as we are aware of, unprecedented, and continuous outcomes, which are often not considered. Thus, we assume that it is so far the most complete, thorough analysis of publication bias in the Cochrane library. \\
We are not aware of any use of one-sided tests for publication bias as used in this study, which allows to look more specifically for publication bias for significant, ``positive'' or ``desired'' treatment effects in a large scale. We find a proportion of significant test results well above the expected false positive rate of the tests of 10\%.\\
Also, it is among the first study to extensively adjust the combined effect sizes from meta-analyses for publication bias. It has been found repeatedly that publication bias might threaten the validity of the findings of some meta-analyses from Cochrane systematic reviews. Cochrane did so far not extend their protocols to publication bias adjustment methods.\\
Therefore, we suggest that publication bias in meta-analyses of clinical trials remains an important issue to be considered while doing meta-analyses. 
% Since Cochrane uses results from grey literature to limit the effects of publication bias, but still does not manage to abolish it, there is also the need for journal editors to change their publication policies.
