% LaTeX file for Chapter 05
<<'preamble05',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch05_fig', 
    self.contained=FALSE,
    cache=TRUE
) 
@


<<echo=FALSE, warning=FALSE, cache = TRUE>>= 
#Load data:
rm(list = ls())
PATH_HOME = path.expand("~") # user home
PATH = file.path(PATH_HOME, 'Data/PubBias')
PATH2 = file.path(PATH_HOME, 'PubBias')
FILE = 'cochrane_2019-07-04.csv'
PATH_DATA = file.path(PATH, 'data')
PATH_CODE = file.path(PATH2, 'code')
PATH_RESULTS = file.path(PATH2, 'results_new')
# PATH_FIGURES = file.path(PATH_RESULTS, 'figures')

source(file.path(PATH_CODE, 'PubBias_functions.R'))

load(file.path(PATH_DATA, "PubBias_2019-07-19.RData"))
load(file.path(PATH_RESULTS, "data_used_for_analysis.RData"))
load(file.path(PATH_RESULTS, "meta_analyses_summary_complete.RData"))
load(file.path(PATH_RESULTS, "meta_id_vector.RData"))
load(file.path(PATH_RESULTS, "meta_id_I2.RData"))
load(file.path(PATH_RESULTS, "meta.bin.RData"))
load(file.path(PATH_RESULTS, "meta.cont.RData"))
load(file.path(PATH_RESULTS, "meta.iv.RData"))
#----------------------------------------------------------------------------------------------------#


adjustment.diff.keep <- meta.f %>% 
  mutate(est.copas.f = case_when(sign(est.copas) == sign(est.fixef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.fixef) ~ -abs(est.copas)),
         est.reg.f = case_when(sign(est.reg) == sign(est.fixef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.fixef) ~ -abs(est.reg)),
         est.copas.r = case_when(sign(est.copas) == sign(est.ranef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.ranef) ~ -abs(est.copas)),
         est.reg.r = case_when(sign(est.reg) == sign(est.ranef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.ranef) ~ -abs(est.reg)),
         
         est.d.copas.f = case_when(sign(est.d.copas) == sign(est.d.fixef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.fixef) ~ -abs(est.d.copas)),
         est.d.reg.f = case_when(sign(est.d.reg) == sign(est.d.fixef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.fixef) ~ -abs(est.d.reg)),
         est.d.copas.r = case_when(sign(est.d.copas) == sign(est.d.ranef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.ranef) ~ -abs(est.d.copas)),
         est.d.reg.r = case_when(sign(est.d.reg) == sign(est.d.ranef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.ranef) ~ -abs(est.d.reg)),
         
         est.z.copas.f = case_when(sign(est.z.copas) == sign(est.z.fixef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.fixef) ~ -abs(est.z.copas)),
         est.z.reg.f = case_when(sign(est.z.reg) == sign(est.z.fixef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.fixef) ~ -abs(est.z.reg)),
         est.z.copas.r = case_when(sign(est.z.copas) == sign(est.z.ranef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.ranef) ~ -abs(est.z.copas)),
         est.z.reg.r = case_when(sign(est.z.reg) == sign(est.z.ranef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.ranef) ~ -abs(est.z.reg)),
         
         est.fixef = abs(est.fixef),
         est.ranef = abs(est.ranef),
         est.z.fixef = abs(est.z.fixef),
         est.z.ranef = abs(est.z.ranef),
         est.d.fixef = abs(est.d.fixef),
         est.d.ranef = abs(est.d.ranef)) 

truebias.bin.percentage <- round(adjustment.diff.keep %>% filter(outcome.flag == "DICH") %>% filter(est.fixef - est.reg > 0) %>% count()/(dim(meta.bin)[1])*100,1)

wider.dataset.ioannidis.comparison <- data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% summarise(n()) %>% ungroup() %>% count() 

restr.dataset.ioannidis.comparison.count <- data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>%  ungroup() %>% distinct(id, .keep_all = T) %>% count()

restr.dataset.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>%  ungroup() %>% distinct(id, .keep_all = T) %>% count() / wider.dataset.ioannidis.comparison)*100

largerten.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 9) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>% summarise(n()) %>% ungroup() %>% count() / wider.dataset.ioannidis.comparison)*100

variance.ratio.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>% filter((max(se)^2)/(min(se)^2) > 4) %>% summarise(n()) %>% ungroup() %>% count() / wider.dataset.ioannidis.comparison)*100

onesig.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>% 
  filter(sum(pval.single < 0.05, na.rm = T) > 0) %>% summarise(n()) %>% ungroup() %>% count() / wider.dataset.ioannidis.comparison)*100

allcriteria <- (dim(meta.bin)[1]/wider.dataset.ioannidis.comparison)*100

harbord.ioannidis.comparison <- (meta.bin %>% filter(pval.harbord < 0.1) %>% count()/dim(meta.bin)[1])*100

own.results <- c(wider.dataset.ioannidis.comparison$n, paste(round(c(largerten.ioannidis.comparison$n, variance.ratio.ioannidis.comparison$n, onesig.ioannidis.comparison$n, allcriteria$n,
                       harbord.ioannidis.comparison$n)), "%", sep = ""))

ioannidis.results <- c(6873, paste(c(13, 72, 55, 5, 12), "%", sep = ""))

comparison.ioannidis <- data.frame(Ioannidis = ioannidis.results, Thesis = own.results)
rownames(comparison.ioannidis) <- c("Wider dataset (n)", "study number > 10", "variance ratio > 4",
                                    "study sig. number > 1", "all exclusion criteria", "harbord test p-value < 0.1")

meta.data <- data.ext2 %>% filter(meta.id %in% meta.f$meta.id)

@

\chapter{Discussion} \label{ch:Discussion}

\section{Results in the light of the Literature}\label{sec:discussion.results}

Before discussing the results of the previous analyses, some results of similar studies are discussed briefly and compared to the results of this study, if possible.
When clear methodological drawbacks are found in the analysis, the reader is referred to chapter \ref{ch:methods} for comments on the disadvantages. The amount of studies analyzing publication bias makes it not possible to discuss any of them, and thus we only some interesting and/or representative examples are shown.

\subsection{\citet{Egger}}
The Cochrane database (1996 issue 2) and publications from the four leading medicine journals (the Lancet, BMJ, JAMA and Annals of Internal Medicine) between 1993 and 1996 were used to find systematic reviews with randomized controlled trials for the application of Egger's small study effect test. They included 38 meta-analyses with at least 5 studies and binary outcomes from the Cochrane Library and 37 from the journals. Five (13\%) meta-analyses from Cochrane and 13 (38\%) from the journals had significant two sided small study effect test results ($p$-value < 0.1). Also, they found that test-statistics were more often negative, which corresponded in their setup to larger effects in small studies. (\Sexpr{round(((24/38))*100, 1)} among Cochrane meta-analyses and \Sexpr{round(((26/37))*100, 1)} for journal meta-analyses). The results from this report with R\"ucker's test are: \Sexpr{round((length(which(meta.bin$pval.rucker < 0.1))/dim(meta.bin)[1])*100,1)}\%, and when using regression adjustment to decide about the direction of publication bias, we get \Sexpr{truebias.bin.percentage$n}\%.


\subsection{\citet{sutton.2000}}
Out of 397 systematic reviews from the Cochrane database (complete number for 1998, issue 3), 49 had more than ten included studies and binary outcomes and 48 compared two treatments. They were analysed by trim-and-fill method \citep{trimfill} to detect and adjust for funnel plot asymmetry (similar to small study effect tests, but the method is known to overestimate bias). 23 were found to have missing studies, and eight had more than three missing studies which was considered to be significant publication bias. Additionally, they found that three estimates of random effects meta-analysis became non-significant after adjustment, and one became significant (by negative adjustment). The results are difficult to compare, but the methodological limitations make this findings unreliable.


\subsection{\citet{Ioannidis2007}}
Data processing steps were however different, and only binary outcomes as used in two-by-two tables were analysed. The approximately corresponding numbers are put in parentheses for comparison. The Cochrane Library from 2003 (issue 2) was used. After removal of duplicates and intractable meta-analyses, they had 6,873 meta-analyses with more than two studies left (\Sexpr{format(wider.dataset.ioannidis.comparison$n, big.mark = ",")}). When only using one meta-analysis per review, this reduced to 846 (\Sexpr{format(restr.dataset.ioannidis.comparison.count$nn, big.mark = ",")}. Then, the criteria that have also been applied in this masters thesis are applied: $I^2 < 0.5$, variance ratio of smallest and largest effects > 4, at least one significant study result and at least 10 studies to be used. Afterwards, they applied Harbord's, Egger's and Begg's Test to the dataset. The reader can compare some corresponding numbers in Table \ref{ioannidis}.
% The results seem to be similar enough to claim that neither the methods nor the data and it's results are differing substantially. Because the thesis relies on more data, it is likely to be more accurate and can possibly add to the findings of \citet{Ioannidis2007}.

<<echo = FALSE, results = 'asis'>>=
print(xtable(comparison.ioannidis, caption = "Comparison of results from Ioannidis et.al. (2007) to the results of the thesis. The percentage of meta-analysis which match all exclusion criteria denotes the ones that apply to all criteria in the table plus the small heterogeneity criterium. Harbord test is two-sided.", label = "ioannidis", align = "lrr"))
@

\subsection{\citet{souza.2007}}
Reviews of the World Health Organization (WHO) Reproductive Health Library (RHL), issue 9, were analysed with the trim-and-fill method. The RHL reproduces and expands reviews from the Cochrane Library with implications for developing countries. 21 of 105 reviews contained more than ten studies and were used. Trim-and-fill found asymmetry in 18 of 21 studies, and 10 had more than 3 missing studies (``significance''). Two of those and one with one missing studies found no evidence for treatment effects after the 0.05 $p$-value threshold after adjustment by trim-and-fill.

\subsection{\citet{kicinsky}}
The author uses a Bayesian hierarchical selection model, but does not analyse treatment effects, but the parameters of the weight function of the selection model, which is estimated with a Bayesian approach and MCMC sampling. \\
The author provides an estimate of the probability of including significant findings versus non-significant findings in Cochrane meta-analyses over time. The data is from the Cochrane Library from 2013 (issue number not provided). The author excluded treatment - treatment comparisons and analysed safety and efficacy meta-analyses separately (how this was achieved is not documented in the paper). \\
From 3845 reviews, the author separated 907 reviews with more than ten studies. From those, 539 compared placebo to treatment. After removing duplicates and sensitivity analyses, 358 analyses with 1297 meta-analyses remained. From these 191 were excluded because they comprised overall mortality and withdrawal, because these could not clearly be specified as safety or efficacy, respectively. \\
1106 meta-analyses from 329 meta-analyses, containing 802 efficacy and 304 safety meta-analyses. The median publication year per meta-analysis was 1997 for efficacy meta-analyses and 1999 for safety meta-analyses. Then, a Bayesian two-step hierarchical selection model was applied (\citet{bayesian.selection.model}, \citet{bayesian.selection.model.2}). It assumes that the selection process is a two-step weight, which assigns different probabilities to non-significant and significant effects. Thus, a ratio of publication probability between significant and non-significant study estimates using a two-step weight function. The model was fitted with the Monte-Carlo Markov-Chain algorithm STAN and the geometric mean was used as an estimate for the publication probability ratio. Simulations performed in \citep{bayesian.selection.model} indicate that the method performs well if the true mean effect size was not small, and also robust to small study effects. It outperformed Egger's test and Begg's test in assessing publication bias, especially when small study effects were absent, and had lower false-positive rates. \\
The results showed a clear publication bias for significant results for efficacy meta-analyses (27\% higher for significant studies, 95\%CI credible intervals: 1.18 to 1.36). The probability was more than twice as high in 27\% of the meta-analyses (95\% CI: 23\% to 31\%). But the probability decreased: from 1.65 (95\% CI: 1.31 to 2.15) in 1980 (average publication year) to 1.36 (95\% CI: 1.17 to 1.62) in 1990 to 1.18 (95\% CI: 1.04 to 1.33) in 2000. For sake of completeness, the probability of inclusion was  1.78 (95\% CI: 1.51 to 2.13) larger for non-significant safety effect estimates, but again, decreased with time (1.77, 95\% CI: 1.46 to 2.21 in 2000). The results are in line with the results from this study, but it was not possible to reaffirm the finding of weaker publication bias in reviews published more frequently.\\

\subsection{\citet{vanAert.2019}}
The Authors of this recent study sample 366 meta-analyses randomly from the Cochrane Library (supposedly 2018 or 2019, not mentioned). They exclude any meta-analysis which include effect sizes identical to effect sizes in other meta-analyses (the larger meta-analysis is retained). Additional criteria were: $I^2 < 0.5$ and at least 5 studies. The meta-analyses were analysed using standardized mean differences and four different tests for publication bias: Egger's test, p-uniform test \citep{p.uniform}, Begg and Mazumdar's rank correlation test and the excess significance test. \\
The authors found, based on the significance threshold of $p$ < 0.1, the following results (own results in brackets):
12.2 \% significant results from Egger's test (\Sexpr{round((length(which(meta.bin$pval.rucker < 0.1)) + length(which(meta.cont$pval.egger < 0.1)) + length(which(meta.iv$pval.egger < 0.1)))/dim(meta.f)[1]*100,1)}),
8.5 \% significant results from Begg and Mazumdar's rank correlation test (\Sexpr{round((length(which(meta.bin$pval.schwarzer < 0.1)) + length(which(meta.cont$pval.begg < 0.1)) + length(which(meta.iv$pval.begg < 0.1)))/dim(meta.f)[1]*100,1)}) and
4.4 \% significant excess significance test results (\Sexpr{round((length(which(meta.f$pval.d.tes < 0.1))/dim(meta.f)[1])*100, 1)}).
It is known that the publication bias tests are lacking power in general as sample size is usually small. Decreasing sample size will result in lower power, especially if the maximal sample size is $n \geq 5$. Additionally, it was also not taken into consideration that it may be difficult to state that publication bias is present in a meta-analysis when no result within it is statistically significant (only 18.8 \% of the effects of the sampled meta-analyses were). This, together that it has not been tried to exclude safety outcomes, may account for the large differences between the results. \\
The author's come to the conclusion in their study that in contrast to other studies, they find few evidence for publication

\subsection{Further Studies on Publication Bias}
\citet{Zhang.2013} find publication bias in critical care studies with similar methods. \citet{Nusch} report publication bias in clinical osteoarthritis research because of funnel plot asymmetry and pledge for routine assessment of publication bias. \citet{Dechartres.2013} analyse publication bias based on sample size in meta-analyses from top journals and Cochrane reviews in 93 Meta-analyses and find that, for example, effects in trials with less than 50 patients were 48\% larger than in larger trials. \citet{Onishi.2014} find that in 36 Meta-Analyses without comprehensive literature research, there are 19.4\% significant publication bias tests (Egger's test).

\section{Interpretation}
In this study, evidence for publication bias has been found in approximately 20\% of the meta-analyses. The results are often times consistent among different tests and subsets of the data. A mixed effects meta regression confirms these findings with very large evidence for small study effects (indicating publication bias). Since the results of two-sided tests do not differ substantially from previous findings, the results are in line with previous research.\\
% There have been proposed a number of reasons for this tendencies in smaller studies (see \citet{Egger} for a thorough discussion). In clinical settings, it might be that the requirement for more study participants in large studies might systematically change the study population. However, this systematic differences could influence the results in two ways: If recruitment of participants with a disease is difficult, and thus, also participants with less severe types of the disease are included, it might be that the effect of treatment is larger in the smaller study. But one could also argue that the opposite could eventually be the case. \\
% The main cause for funnel plot asymmetry with smaller studies having larger effects is publication bias. Publication bias enforces questionable research practices from researchers such as selective outcome reporting, $p$-hacking and data fabrication. Larger studies are less affected, and have also in general higher methodological quality \citep{Egger}, which leads to smaller effect sizes.\\
Presence of publication bias in meta-analyses can ultimately lead to patient harm if decision makers in clinical practice use the meta-analyses to decide about application of a treatment. On the swiss Cochrane website, it is stated that Cochrane is the ``single most reliable source of evidence on the effects of health care''. However, publication bias does in general lead to too large confidence in the results of the meta-analyses. In some cases, it might be that the  evidence for a treatment in a meta-analysis is an artifact of publication bias. This also leads to a waste of resources in science. \\
The term publication bias is frequently used in this study, but \Sexpr{round((length(which(meta.data$study.data_source == "UNPUB"))/dim(meta.data)[1])*100, 1)}\% of the analysis dataset (1) are unpublished results. This mitigates the estimated extent of publication bias in this study since it has been shown repeatedly that effects in the grey literature are smaller.


\section{Limitations}
After application of the criteria of \citet{Ioannidis2007}, there are left \Sexpr{round((740/6354)*100, 1)} \% of the reviews and \Sexpr{round((23243/70662)*100, 1)}\% of the studies. It is not clear if the results also apply for the excluded data, \ie if the extent of publication bias is similar. The most restrictive criterium of exclusion was that a meta-analysis had to have at least ten studies. The included studies are thus likely to be part of enforced and sustained research efforts. When fewer and unparalleled studies are available, the quality of the research is not necessarily better. Publication bias may be even more of an issue in more discovery oriented research \citet{ioannidis.2005}. By excluding meta-analyses where $I^2 \geq 0.5$, the most extreme cases of publication bias are possibly removed since publication bias increases the between-study heterogeneity.\\
% Furthermore, the sub-partitioning of meta-analyses based on comparison, outcome and subgroup is only one possibility%, which is in fact not recommended by the Cochrane Organisation. 
% Cochrane usually performs multiple meta-analyses with the same results, for example by doing sensitivity analyses (not an issue in this study, since they are removed) and subgroup analyses and overall analyses. One can as well repeat the analysis without considering subgroups. This will have the advantage of increasing the sample size, but it will also result in increased heterogeneity in meta-analyses, and if one subgroup with small studies yields large effects, the cause must not be publication bias. \\
It also has to be said that the removal of safety outcomes is only partially successful. This is an issue, because it is not well known if and how the publication probability is affected by the size of safety outcomes. \citet{kicinsky} suggests that they may inversely affect publication (\ie non-significant effects are preferred).\\
Also, it has been previously discussed that the method to detect the side on which bias is to be expected can fail and provide meaningless results. One could also discuss in more general terms if one can speak of publication bias in meta-analyses that have only one significant effect out of ten ($n$ = \Sexpr{length(which(meta.f$n.sig.single == 1))}), \ie if statistical significance is largely absent in a meta-analysis.\\
There is also criticism that one can address on the methods. It is possible that the results are caused by systematic differences in the population of study participants only. This is considered rather unlikely. Other criticism is more severe. The small study effect tests applied do not account for statistical significance of effects directly. They work because smaller studies with larger errors need larger effects to be significant. The only test that is applied that takes into account statistical significance per se is the excess significance test, which is underpowered.\\
The criticism also applies for adjustment methods, as they also rely on small study effects. A weakness of the regression adjustment is that it also adjusts if the uncertainty in the small study effect is large. To compare it in this case with the meta-analysis estimate is difficult because the uncertainty in estimating the small study effect affects the uncertainty of the treatment effect. Copas selection model uses the $p$-value threshold of 0.1 to decide if adjustment is necessary or not. One could argue that this threshold is arbitrary. Additionally, the threshold is used in two manners: first to decide if adjustment is necessary at all, and secondly, if the adjusted model does suitably account for publication bias ($p$ > 0.1). Hypothesis tests against the null hypothesis can however not be used to decide in favor of the null-hypothesis, and a separate test would be needed there. \\
With Copas selection model, it is in general the issue that the model is not estimating the selection process parameters, but rather applying some criteria of parsimony (the model with least publication bias/selection with $p$-value for small study effect > 0.1 is chosen). \\
Publication bias can also be present if a meta-analysis lacks a small study effect, for example if by chance, the smaller studies were published irrespectively of the statistical significance of their findinfs. Methods as the excess significance test of \citet{excess.significance} are applied to overcome this issue, but the method is known to lack power. No methods exist so far to adjust for excess significance.\\
Therefore, the results are not exact and could be improved if more suitable methods are applied. The Bayesian approach taken by \citet{kicinsky} has many advantages over the applied methods, \eg by accounting for statistical significance, but relies on assumptions over the weight function and is at least somewhat sensible on the choice of the prior distributions. \\

\section{Outlook}
Given the size and abundance of information in the dataset, the possibilities to investigate sources of bias in the Cochrane Database of Systematic Reviews are not exhausted (see for example in \citet{ioannidis.2017}). In particular, one could easily come up with more hypotheses that could be tested in meta meta-regression, as introduced in the last section of the results. Many researchers have come up with hypotheses about the reasons and circumstances that promote publication bias, which could be tested if the information is extended.\\
There are numerous suggestions for different measures of publication bias and small study effects, which could be applied on the dataset (see \citet{mueller.2016}). This study mainly relies on suggestions from \citet{Sterne}, \citet{Ioannidis2007} and \citet{limitmeta}. It uses most often methods that are well integrated in packages (\citet{metafor.package}, \citet{meta.package}). Although there are well addressed, mathematical justifications for these methods, there might be better methods not yet tested on large datasets. An evaluation of all suggested methods was unfortunately beyond the scope of this study.


\section{Implications}
This study is so far the largest assessment of publication bias by small study effects and uses data that has been collected after major efforts have been made to curb publication bias. It is the first analysis of the Cochrane Database of Systematic Reviews also analyzing publication bias using hazard and rate ratios. The results indicate, similarly to other studies, that the efforts did not yet resolve the issue. \\
The findings further imply that when using meta-analyses, it is key to investigate sources of between-study heterogeneity; an alternative is to rely solely on the largest trials \citep{largest.study.only}. Since the Cochrane Organization uses results from grey literature to limit the effects of publication bias, but still does not manage to abolish it, there is also the need for journal editors to change their publication policies.


