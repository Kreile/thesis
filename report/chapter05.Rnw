% LaTeX file for Chapter 05
<<'preamble05',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch05_fig', 
    self.contained=FALSE,
    cache=TRUE
) 
@


\chapter{Discussion} \label{ch:Discussion}

The aim of this study was to assess publication bias in the Cochrane Library, ``the single, most reliable source of evidence in healthcare'' (citation from Cochrane). We applied a series of methods to test and adjust for publication bias in meta-analyses that matched our criteria. \\
The main result was that among the analysed meta-analyses, approximately 20\% beared evidence for publication bias. Adjustment for publication bias lead to a decrease in evidence for treatment efficacy. After comparison with previous studies that analysed publication bias in the Cochrane Library, we found that our results were similar to the results of these studies (\eg \citealp{Egger}, \citealp{Ioannidis2007}). Further information can be found in the Appendix\\
% In this study, evidence for publication bias has been found in approximately 20\% of the meta-analyses. The results are oftentimes consistent among different tests and subsets of the data. A mixed effects meta regression confirms these findings with very large evidence for small study effects (indicating publication bias). Since the results of two-sided tests do not differ substantially from previous findings, the results are in line with previous research.\\
% There have been proposed a number of reasons for this tendencies in smaller studies (see \citealp{Egger} for a thorough discussion). In clinical settings, it might be that the requirement for more study participants in large studies might systematically change the study population. However, this systematic differences could influence the results in two ways: If recruitment of participants with a disease is difficult, and thus, also participants with less severe types of the disease are included, it might be that the effect of treatment is larger in the smaller study. But one could also argue that the opposite could eventually be the case. \\
% The main cause for funnel plot asymmetry with smaller studies having larger effects is publication bias. Publication bias enforces questionable research practices from researchers such as selective outcome reporting, $p$-hacking and data fabrication. Larger studies are less affected, and have also in general higher methodological quality \citep{Egger}, which leads to smaller effect sizes.\\
Presence of publication bias in meta-analyses can ultimately lead to patient harm if decision makers in clinical practice use the meta-analyses to decide about application of a treatment. However, publication bias does in general lead to too large confidence in the results of the meta-analyses. In some cases, it might be that the  evidence for a treatment in a meta-analysis is an artifact of publication bias. This also leads to a waste of resources in science. \\
The fact that Cochrane also uses study reports from Grey literature and other unpublished data (1.5\% of the analysed data), which contain usually smaller effect sizes, likely mitigates our estimate of publication bias.
% The term publication bias is frequently used in this study, but \Sexpr{round((length(which(meta.data$study.data_source == "UNPUB"))/dim(meta.data)[1])*100, 1)}\% of the analysis dataset (1) are unpublished results. This mitigates the estimated extent of publication bias in this study since it has been shown repeatedly that effects in the grey literature are smaller.


\section{Limitations}
This is an exploratory study, and the results are rather suggestive than confirmatory. As a part of evidence it is supposed to strengthen previous assessments of publication bias and possibly, guide further research. \\
It is clear that publication bias could only be assessed in a part of all meta-analyses of the Cochrane Library and that the extent of publication bias in the remaining data is unknown. Because one inclusion criterion was that more than ten study results had to be available for a given research question, the analysed dataset contains rather the part of data with established and enduring research. When research is more experimental and exploratory, it may well be that publication bias is an issue as well, or even more of an issue. Publication bias is possibly related to more discovery oriented, modern research \citet{ioannidis.2005}.\\
There is an unknown amount of meta-analyses with secondary outcomes in the analysis. It is not well investigated if and how secondary outcomes are affected by publication bias, and how much they are affected by between-study heterogeneity, as researchers might not follow the same stringent protocols when assessing secondary outcomes. Together with unremoved adverse effect meta-analyses, which are likely subject to different kinds of publication bias, they can distort or mitigate the estimated extent of publication bias in this study. If one assumes that publication bias is not as strong in secondary outcomes as in primary outcomes, and that adverse effects are subject to bias for small effects (as suggested by \citealp{kicinsky}), the extent of publication bias will be underestimated.\\
Some criticism can be addressed on the methods. The small study effect which is used as an indication for publication bias can be caused by other biases such as selective outcome reporting, or poor methodological quality of smaller studies. It can also be caused by true heterogeneity between studies; early and small studies are more likely to include high-risk patients for which treatment can have larger benefits. Where effect size estimates and their standard errors are not independent (\eg log risk ratios, std. mean differences), small study effect can be a statistical artifact. However, true heterogeneity between smaller and larger studies might as well mask the presence of publication bias, for example if smaller studies have smaller effects due to true between-study heterogeneity.\\
Also, small study effect tests do not use statistical significance directly. They will fail when there are no significant results, but the smaller studies have larger effects than the larger studies, in which case no publication bias for significant results could exist. The only test that is applied that takes into account statistical significance per se is the excess significance test, which is underpowered. However, we restricted the meta-analyses such that at least one significant effect had to be included. \\
Most small study effect tests rely on linear regression. Linear regression is prone to outliers, especially if the sample size is small. The issue is partially resolved by weighting, but the usual procedure to assess if the model assumptions are fulfilled are not possible for such a large number of model fits.\\
All the criticism also applies for adjustment methods, as they also rely on small study effects. A weakness of the regression adjustment is that it also adjusts if the uncertainty in the small study effect is large. To compare it in this case with the meta-analysis estimate is difficult because the uncertainty in estimating the small study effect affects the uncertainty of the treatment effect. Copas selection model uses the $p$-value threshold of 0.1 to decide if adjustment is necessary or not. One could argue that this threshold is somewhat arbitrary. When assessing the evidence for the treatment effect being different from the effect under the null, a likelihood ratio test would provide more appropriate results than the Wald test. %Selection models do not account for the uncertainity that is contained in the selection process parameters they use, which makes it difficult to use their results to test against the null hypothesis of no treatment effect.\\
Another issue that the model is not estimating the selection process parameters, but rather applying some criteria of parsimony (the model with least publication bias/selection with $p$-value for small study effect > 0.1 is chosen). As the regression model, this model also comes with various assumptions that likely affect the results. But in simulations, the methods all had fewer mean-squared error and bias than the classical meta-analysis methods when publication bias was present.\\
Publication bias can also be present if a meta-analysis lacks a small study effect, for example if significant results are published irrespective of the sign of the effect estimate. Methods as the excess significance test of \citet{excess.significance} are applied to overcome this issue, but the method is known to lack power. No methods exist so far to adjust for excess significance. This would be a reason to assume that publication bias is underestimated.\\
Given the large body of evidence for publication bias by other means, it is unlikely that the unexpectedly large proportion of significant test results and the downward corrected treatment effect estimates are merely false positives. Of course, the general practice is to apply all the methods in this study carefully, and with background knowledge about each single study in the meta-analyses, to differentiate between true heterogeneity and publication bias. But this does not disqualify these results, but stress the necessity that authors of meta-analyses investigate possible publication bias thoroughly. 
% Therefore, the results are not exact and could be improved if more suitable methods are applied. The Bayesian approach taken by \citet{kicinsky} has many advantages over the applied methods, \eg by accounting for statistical significance, but relies on assumptions over the weight function and is at least somewhat sensible on the choice of the prior distributions. \\
% Furthermore, the sub-partitioning of meta-analyses based on comparison, outcome and subgroup is only one possibility%, which is in fact not recommended by the Cochrane Organisation. 
% Cochrane usually performs multiple meta-analyses with the same results, for example by doing sensitivity analyses (not an issue in this study, since they are removed) and subgroup analyses and overall analyses. One can as well repeat the analysis without considering subgroups. This will have the advantage of increasing the sample size, but it will also result in increased heterogeneity in meta-analyses, and if one subgroup with small studies yields large effects, the cause must not be publication bias. \\
% It also has to be said that the removal of safety outcomes is only partially successful. This is an issue, because it is not well known if and how the publication probability is affected by the size of safety outcomes. \citet{kicinsky} suggests that they may inversely affect publication (\ie non-significant effects are preferred).\\
% Also, it has been previously discussed that the method to detect the side on which bias is not working perfectly. One could also discuss in more general terms if one can speak of publication bias in meta-analyses %that have only one significant effect out of ten ($n$ = \Sexpr{length(which(meta.f$n.sig.single == 1))}), \ie 
% if statistical significance is largely absent in a meta-analysis.\\

\subsection{A Note on the Use of Effect Size Measures}
An inevitable issue for the analysis of publication bias and adjustment for it is the choice of the effect size measure. Researchers and journal editors rely on effect sizes as mean differences or log odds ratios to assess treatment efficacy and statistical significance. Transformation of the effect sizes might change a significant result to non-significant or vice-versa. Thus transformation will lead to a loss of accuracy. Furthermore, transformation of mean differences to std. mean differences also comes with the unpleasant consequence that effect size and std. mean difference are no longer independent (smaller differences for large std. errors). When transforming odds ratios to std. mean differences, the interdependence between effect size and std. mean difference is also retained.\\
Transformation to Fisher's $z$ transformed correlation will abolish this dependency, but since correlations are bounded, linear regression will no longer be suitable to detect very large effect sizes. Furthermore, the applied adjustment methods which are based on Fisher's $z$ transformed correlations are not adapted to correlation coefficients and sometimes provide meaningless results (coefficients $> 1$ or $< -1$). Thus, one transformation will tend to underestimate publication bias, while the other likely overestimates it, and both are not accurate. 


\section{Outlook}
Given the size and abundance of information in the dataset, the possibilities to investigate sources of bias in the Cochrane Library of Systematic Reviews are not exhausted. In particular, one could easily come up with more hypotheses that could be tested in meta meta-regression, as introduced in the last section of the results. Many researchers have come up with hypotheses about the reasons and circumstances that promote publication bias, which could be tested if the dataset is extended.\\
There are numerous suggestions for different measures of publication bias and small study effects, which could be applied on the dataset (see \citealp{mueller.2016}). This study mainly relies on suggestions from \citet{Sterne}, \citet{Ioannidis2007} and \citet{limitmeta}. It uses most often methods that are well integrated in packages (\citealp{metafor.package}, \citealp{meta.package}). Although there are well addressed, mathematical justifications for these methods, there might be better methods not yet tested on large datasets. An evaluation of all suggested methods was unfortunately beyond the scope of this study.


\section{Implications}
As far as we know, this study is so far the largest assessment of publication bias by small study effects and uses also data that has been collected after major efforts have been made to curb publication bias. %It is the first analysis of the Cochrane Library of Systematic Reviews also analyzing publication bias using hazard and rate ratios. 
Furthermore, the analysis includes time-to-event data which is, as far as we are aware of, unprecedented, and continuous outcomes, which is unusual. Thus we assume that it is so far the most complete, thorough analysis of publication bias in the Cochrane library. \\
We are not aware of any use of one-sided tests for publication bias as used in this study, which allows to look more specifically for publication bias for ``positive'' or desired treatment effects. The results indicate that the efforts to curb down publication bias are only partially successful.\\
Also, it is the first study to extensively adjust the combined effect sizes from meta-analyses for publication bias. It has been found repeatedly that publication bias might threaten the validity of the findings of some meta-analyses from Cochrane systematic reviews. Cochrane did so far not extend their protocols to publication bias adjustment methods.\\
Therefore, we suggest that publication bias in meta-analyses of clinical trials remains an important issue to be considered while doing meta-analyses. 
% Since Cochrane uses results from grey literature to limit the effects of publication bias, but still does not manage to abolish it, there is also the need for journal editors to change their publication policies.
