% LaTeX file for Chapter 05
<<'preamble05',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch05_fig', 
    self.contained=FALSE,
    cache=TRUE
) 
@


<<echo=FALSE, warning=FALSE>>= 
#Load data:
rm(list = ls())
PATH_HOME = path.expand("~") # user home
PATH = file.path(PATH_HOME, 'Data/PubBias')
PATH2 = file.path(PATH_HOME, 'PubBias')
FILE = 'cochrane_2019-07-04.csv'
PATH_DATA = file.path(PATH, 'data')
PATH_CODE = file.path(PATH2, 'code')
PATH_RESULTS = file.path(PATH2, 'results_new')
# PATH_FIGURES = file.path(PATH_RESULTS, 'figures')

source(file.path(PATH_CODE, 'PubBias_functions.R'))

load(file.path(PATH_DATA, "PubBias_2019-07-19.RData"))
load(file.path(PATH_RESULTS, "data_used_for_analysis.RData"))
load(file.path(PATH_RESULTS, "meta_analyses_summary_complete.RData"))
load(file.path(PATH_RESULTS, "meta_id_vector.RData"))
load(file.path(PATH_RESULTS, "meta_id_I2.RData"))
load(file.path(PATH_RESULTS, "meta.bin.RData"))
load(file.path(PATH_RESULTS, "meta.cont.RData"))
load(file.path(PATH_RESULTS, "meta.iv.RData"))
#----------------------------------------------------------------------------------------------------#


adjustment.diff.keep <- meta.f %>% 
  mutate(est.copas.f = case_when(sign(est.copas) == sign(est.fixef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.fixef) ~ -abs(est.copas)),
         est.reg.f = case_when(sign(est.reg) == sign(est.fixef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.fixef) ~ -abs(est.reg)),
         est.copas.r = case_when(sign(est.copas) == sign(est.ranef) ~  abs(est.copas),
                                 sign(est.copas) != sign(est.ranef) ~ -abs(est.copas)),
         est.reg.r = case_when(sign(est.reg) == sign(est.ranef) ~  abs(est.reg),
                               sign(est.reg) != sign(est.ranef) ~ -abs(est.reg)),
         
         est.d.copas.f = case_when(sign(est.d.copas) == sign(est.d.fixef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.fixef) ~ -abs(est.d.copas)),
         est.d.reg.f = case_when(sign(est.d.reg) == sign(est.d.fixef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.fixef) ~ -abs(est.d.reg)),
         est.d.copas.r = case_when(sign(est.d.copas) == sign(est.d.ranef) ~  abs(est.d.copas),
                                   sign(est.d.copas) != sign(est.d.ranef) ~ -abs(est.d.copas)),
         est.d.reg.r = case_when(sign(est.d.reg) == sign(est.d.ranef) ~  abs(est.d.reg),
                                 sign(est.d.reg) != sign(est.d.ranef) ~ -abs(est.d.reg)),
         
         est.z.copas.f = case_when(sign(est.z.copas) == sign(est.z.fixef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.fixef) ~ -abs(est.z.copas)),
         est.z.reg.f = case_when(sign(est.z.reg) == sign(est.z.fixef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.fixef) ~ -abs(est.z.reg)),
         est.z.copas.r = case_when(sign(est.z.copas) == sign(est.z.ranef) ~  abs(est.z.copas),
                                   sign(est.z.copas) != sign(est.z.ranef) ~ -abs(est.z.copas)),
         est.z.reg.r = case_when(sign(est.z.reg) == sign(est.z.ranef) ~  abs(est.z.reg),
                                 sign(est.z.reg) != sign(est.z.ranef) ~ -abs(est.z.reg)),
         
         est.fixef = abs(est.fixef),
         est.ranef = abs(est.ranef),
         est.z.fixef = abs(est.z.fixef),
         est.z.ranef = abs(est.z.ranef),
         est.d.fixef = abs(est.d.fixef),
         est.d.ranef = abs(est.d.ranef)) 

truebias.bin.percentage <- round(adjustment.diff.keep %>% filter(outcome.flag == "DICH") %>% filter(est.fixef - est.reg > 0) %>% count()/(dim(meta.bin)[1])*100,1)

wider.dataset.ioannidis.comparison <- data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% summarise(n()) %>% ungroup() %>% count() 

restr.dataset.ioannidis.comparison.count <- data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>%  ungroup() %>% distinct(id, .keep_all = T) %>% count()

restr.dataset.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>%  ungroup() %>% distinct(id, .keep_all = T) %>% count() / wider.dataset.ioannidis.comparison)*100

largerten.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 9) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>% summarise(n()) %>% ungroup() %>% count() / wider.dataset.ioannidis.comparison)*100

variance.ratio.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>% filter((max(se)^2)/(min(se)^2) > 4) %>% summarise(n()) %>% ungroup() %>% count() / wider.dataset.ioannidis.comparison)*100

onesig.ioannidis.comparison <- (data.ext2 %>% filter(outcome.flag == "DICH") %>% group_by(meta.id) %>% filter(n() > 2) %>% filter(dupl.remove == 0) %>% filter(!all(events1 == 0) & !all(events2 == 0)) %>% filter(!all(events1 == total1) & !(all(events2 == total2))) %>% filter(se > 0) %>% 
  filter(sum(pval.single < 0.05, na.rm = T) > 0) %>% summarise(n()) %>% ungroup() %>% count() / wider.dataset.ioannidis.comparison)*100

allcriteria <- (dim(meta.bin)[1]/wider.dataset.ioannidis.comparison)*100

harbord.ioannidis.comparison <- (meta.bin %>% filter(pval.harbord < 0.1) %>% count()/dim(meta.bin)[1])*100

own.results <- c(wider.dataset.ioannidis.comparison$n, paste(round(c(largerten.ioannidis.comparison$n, variance.ratio.ioannidis.comparison$n, onesig.ioannidis.comparison$n, allcriteria$n,
                       harbord.ioannidis.comparison$n)), "%", sep = ""))

ioannidis.results <- c(6873, paste(c(13, 72, 55, 5, 12), "%", sep = ""))

comparison.ioannidis <- data.frame(Ioannidis = ioannidis.results, Thesis = own.results)
rownames(comparison.ioannidis) <- c("Wider dataset (n)", "study number > 10", "variance ratio > 4",
                                    "study sig. number > 1", "all exclusion criteria", "harbord test p-value < 0.1")

@

\chapter{Discussion} \label{ch:Discussion}

\section{Results in the light of the Literature}\label{sec:discussion.results}

Before discussing the results of the previous analyses, some results of similar studies are discussed briefly and compared to the results of this study, if possible.
When clear methodological drawbacks are found in the analysis, the reader is referred to chapter \ref{ch:methods} for comments on the disadvantages. The amount of studies analysing publication bias makes it not possible to discuss any of them, and thus we only some interesting and/or representative examples are shown.

\subsection{\citet{Egger}}
The Cochrane database (1996 issue 2) and publications from the four leading medicine journals (the Lancet, BMJ, JAMA and Annals of Internal Medicine) between 1993 and 1996 were used to find systematic reviews with randomized controlled trials for the application of Egger's small study effect test. They included 38 meta-analyses with at least 5 studies and binary outcomes from the Cochrane Library and 37 from the journals. Five (13\%) meta-analyses from Cochrane and 13 (38\%) from the journals had significant two sided small study effect test results ($p$-value < 0.1). Also, they found that test-statistics were more often negative, which correspondend in their setup to larger effects in small studies. (\Sexpr{round(((24/38))*100, 1)} among Cochrane meta-analyses and \Sexpr{round(((26/37))*100, 1)} for journal meta-analyses). The results from this report with R\"ucker's test are: \Sexpr{round((length(which(meta.bin$pval.rucker < 0.1))/dim(meta.bin)[1])*100,1)}\%, and when using regression adjustment to decide about the direction of publication bias, we get \Sexpr{truebias.bin.percentage$n}\%.


\subsection{\citet{sutton.2000}}
Out of 397 systematic reviews from the Cochrane database (complete number for 1998, issue 3), 49 had more than ten included studies and binary outcomes and 48 compared two treatments. They were analysed by trim-and-fill method \citep{trimfill} to detect and adjust for funnel plot asymmetry (similar to small study effect tests, but the method is known to overestimate bias). 23 were found to have missing studies, and eight had more than three missing studies which was considered to be significant publication bias. Additionally, they found that three estimates of random effects meta-analysis became non-significant after adjustment, and one became significant (by negative adjustment). The results are difficult to compare, but the methodological limitations make this findings unreliable.


\subsection{\citet{Ioannidis2007}}
Data processing steps were however different, and only binary outcomes as used in two-by-two tables were analysed. The approximately corresponding numbers are put in parentheses for comparison. The Cochrane Library from 2003 (issue 2) was used. After removal of duplicates and intractable meta-analyses, they had 6,873 meta-analyses with more than two studies left (\Sexpr{format(wider.dataset.ioannidis.comparison$n, big.mark = ",")}). When only using one meta-analysis per review, this reduced to 846 (\Sexpr{format(restr.dataset.ioannidis.comparison.count$nn, big.mark = ",")}. Then, the criteria that have also been applied in this masters thesis are applied: $I^2 < 0.5$, variance ratio of smallest and largest effects > 4, at least one significant study result and at least 10 studies to be used. Afterwards, they applied Harbord's, Egger's and Begg's Test to the dataset. The reader can compare some corresponding numbers in Table \ref{ioannidis}.
% The results seem to be similar enough to claim that neither the methods nor the data and it's results are differing substantially. Because the thesis relies on more data, it is likely to be more accurate and can possibly add to the findings of \citet{Ioannidis2007}.

<<echo = FALSE, results = 'asis'>>=
print(xtable(comparison.ioannidis, caption = "Comparison of results from Ioannidis et.al. (2007) to the results of the thesis. The percentage of meta-analysis which match all exclusion criteria denotes the ones that apply to all criteria in the table plus the small heterogeneity criterium. Harbord test is two-sided.", label = "ioannidis", align = "lrr"))
@

\subsection{\citet{souza.2007}}
Reviews of the World Health Organization (WHO) Reproductive Health Library (RHL), issue 9, were analysed with the trim-and-fill method. The RHL reproduces and expands reviews from the Cochrane Library with implications for developping countries. 21 of 105 reviews contained more than ten studies and were used. Trim-and-fill found asymmetry in 18 of 21 studies, and 10 had more than 3 missing studies (``significance''). Two of those and one with one missing studies found no evidence for treatment effects after the 0.05 $p$-value threshold after adjustment by trim-and-fill.

\subsection{\citet{kicinsky}}
The author uses a bayesian hierarchical selection model, but does not analyse treatment effects, but the parameters of the weight function of the selection model, which is estimated with a bayesian approach and MCMC sampling. \\
The author provides an estimate of the probability of including significant findings versus non-significant findings in Cochrane meta-analyses over time. The data is from the Cochrane Library from 2013 (issue number not provided). The author excluded treatment - treatment comparisons and analysed safety and efficacy meta-analyses separately (how this was achieved is not documented in the paper). \\
From 3845 reviews, the author separated 907 reviews with more than ten studies. From those, 539 compared placebo to treatment. After removing duplicates and sensitivity analyses, 358 analyses with 1297 meta-analyses remained. From these 191 were excluded because they comprised overall mortality and withdrawal, because these could not clearly be specified as safety or efficacy, respectively. \\
1106 meta-analyses from 329 meta-analyses, containing 802 efficacy and 304 safety meta-analyses. The median publication year per meta-analysis was 1997 for efficacy meta-analyses and 1999 for safety meta-analyses. Then, a bayesian two-step hierarchical selection model was applied (\citet{bayesian.selection.model}, \citet{bayesian.selection.model.2}). It assumes that the selection process is a two-step weight, which assigns different probabilities to non-significant and significant effects. Thus, a ratio of publication probability between significant and non-significant study estimates using a two-step weight function. The model was fitted with the Monte-Carlo Markov-Chain algorithm STAN and the geometric mean was used as an estimate for the publication probability ratio. Simulations performed in \citep{bayesian.selection.model} indicate that the method performes well if the true mean effect size was not small, and also robust to small study effects. It outperformed Egger's test and Begg's test in assessing publication bias, especially when small study effects were absent, and had lower false-positive rates. \\
The results showed a clear publication bias for significant results for efficacy meta-analyses (27\% higher for significant studies, 95\%CI credible intervals: 1.18 to 1.36). The probability was more than twice as high in 27\% of the meta-analyses (95\% CI: 23\% to 31\%). But the probability decreased: from 1.65 (95\% CI: 1.31 to 2.15) in 1980 (average publication year) to 1.36 (95\% CI: 1.17 to 1.62) in 1990 to 1.18 (95\% CI: 1.04 to 1.33) in 2000. For sake of completeness, the probability of inclusion was  1.78 (95\% CI: 1.51 to 2.13) larger for non-significant safety effect estimates, but again, decreased with time (1.77, 95\% CI: 1.46 to 2.21 in 2000). The results are in line with the results from this study, but it was not possible to reafirm the finding of weaker publication bias in reviews published more frequently.\\

\subsection{\citet{vanAert.2019}}
The Authors of this recent study sample 366 meta-analyses randomly from the Cochrane Library (supposedly 2018 or 2019, not mentioned). They exclude any meta-analysis which include effect sizes identical to effect sizes in other meta-analyses (the larger meta-analysis is retained). Additional criteria were: $I^2 < 0.5$ and at least 5 studies. The meta-analyses were analysed using standardized mean differences and four different tests for publication bias: Egger's test, p-uniform test \citep{p.uniform}, Begg and Mazumdar's rank correlation test and the excess significance test. \\
The authors found, based on the significance threshold of $p$ < 0.1, the following results (own results in brackets):
12.2 \% significant results from Egger's test (\Sexpr{round((length(which(meta.bin$pval.rucker < 0.1)) + length(which(meta.cont$pval.egger < 0.1)) + length(which(meta.iv$pval.egger < 0.1)))/dim(meta.f)[1]*100,1)}),
8.5 \% significant results from Begg and Mazumdar's rank correlation test (\Sexpr{round((length(which(meta.bin$pval.schwarzer < 0.1)) + length(which(meta.cont$pval.begg < 0.1)) + length(which(meta.iv$pval.begg < 0.1)))/dim(meta.f)[1]*100,1)}) and
4.4 \% significant excess significance test results (\Sexpr{round((length(which(meta.f$pval.d.tes < 0.1))/dim(meta.f)[1])*100, 1)}).
It is known that the publication bias tests are lacking power in general as sample size is usually small. Decreasing sample size will result in lower power, especially if the maximal sample size is $n \geq 5$. Additionally, it was also not taken into consideration that it may be difficult to state that publication bias is present in a meta-analysis when no result within it is statistically significant (only 18.8 \% of the effects of the sampled meta-analyses were). This, together that it has not been tried to exclude safety outcomes, may account for the large differences between the results. \\
The author's come to the conclusion in their study that in contrast to other studies, they find few evidence for publication

\subsection{Further Studies on Publication Bias}
\citet{Zhang.2013} find publication bias in critical care studies with similar methods. \citet{Nusch} report publication bias in clinical osteoarthritis research because of funnel plot asymmetry and pledge for routine assessment of publication bias. \citet{Dechartres.2013} analyse publication bias based on sample size in meta-analyses from top journals and Cochrane reviews in 93 Meta-analyses and find that, for example, effects in trials with less than 50 patients were 48\% larger than in larger trials. \citet{Onishi.2014} find that in 36 Meta-Analyses without comprehensive literature research, there ar 19.4\% significant publication bias tests (Egger's test).

\section{Interpretation}
In this study, a proportion of the meta-analyses that have been scrutinized have publication bias. Since regression and rank test have been applied one-sided, they have very likely larger effects in smaller studies that in larger studies. \\
There have been proposed a number of reasons for this tendencies in smaller studies (see \citet{Egger} for a thorough discussion). In clinical settings, it might be that the requirement for more study participants in large studies might systematically change the study population. However, this systematic differences could influence the results in two ways: If recruitement of participants with a disease is difficult, and thus, also participants with less severe types of the disease are included, it might be that the effect of treatment is larger in the smaller study. But one could also argue that the opposite could eventually be the case. \\
The main cause for funnel plot asymmetry with smaller studies having larger effects is publication bias. Publication bias enforces questionable research practices from researchers such as selective outcome reporting, $p$-hacking and data fabrication. Larger studies are less affected, and have also in general higher methodological quality \citep{Egger}, which leads to smaller effect sizes.\\
Presence of publication bias in meta-analyses can ultimately lead to patient harm, because decision makers in clinical practice use the meta-analyses to take decisions if they use a treatment. On the swiss Cochrane website, it is stated that Cochrane is the ``single most reliable source of evidence on the effects of health care''. However, publication bias does in general lead to too large confidence in the results of the meta-analyses. In some cases, it might be that the  evidence for a treatment in a meta-analysis is an artifact of publication bias. Besides harm to patient, this can cause a waste of ressources. \\
The comparison of the results with previous analyses of publication bias in the Cochrane Database of Systematic Reviews implies gives more confidence in the results of this study, because they are largely in line. \\


\section{Limitations}
After application of the criteria of \citet{Ioannidis2007}, there are left \Sexpr{round((740/6354)*100, 1)} \% of the reviews and \Sexpr{round((23243/70662)*100, 1)}\% of the studies. It is not clear how the results would be in the remaining dataset if tests were applicable. The most restrictive criterium of exclusion was that a meta-analysis has to have at least ten studies. Thus, in the data that is scrutinized, there has been enforced and ongoing research. \citet{ioannidis.2005} state that publication bias is also a consequence of modern discovery oriented research, and thus it may well be that publication bias is also an issue when the number of studies are lower.\\
It also has to be said that the removal of safety outcomes is only partially successfull. This is an issue, because it is not well known if and how publication bias affect safety outcomes. \citet{kicinsky} suggests that they are inversely affected, with editors publishing in favor of non-significant adverse effects. \\
Also, it has been previously discussed that the method to detect the side on which bias is to be expected can fail and provide meaningless results, in which case a two-sided test is more meaningful. However, one could also discuss in more general terms if one can speak of publication bias in meta-analyses that have only one significant effect out of ten ($n$ = \Sexpr{length(which(meta.f$n.sig.single == 1))}). \\
There is also criticism that one can address on the methods. It is possible that the results are caused by systematic differences in the population of study participants only. This is considered rather unlikely. Other cirticism is more severe. The small study effect tests fail to account for statistical significance, however, this is a major component of publication bias. Small study effect test work because they take into account that smaller studies need larger effects to be significant. The only test that is applied that takes into account statistical significance per se is the excess significance test. When looking at it's results, it seems as if it is rather a test for between study heterogeneity per se, because it does agree very poorly with small study effect tests, and finds almost no excess significance in tests with $I^2 = 0$. The choice of the effect size and proxy of the study size is crucial when assessing publication bias. It is not clear which measure of effect size should be used when applying the test; the original measure (\eg a mean difference) may have standard errors that do not only depend on study size, but journal editors use this measure as the basis for their decision of publication. Fisher's $z$ transformed correlations have standard errors that are a function of study participant number only, but in contrast the effect size is bounded at$[-1,1]$, making it less suitable for linear regression. Standardized mean differences are possibly a good compromise.\\
The ciriticism also applies for adjustment methods, as they also do not use statistical significance but the size of the effects. A weakness of the regression adjustment is that it also adjusts if the uncertainity in the small study effect is large. To compare it in this case with the meta-analysis estimate is difficult because the uncertainity in estimating the small study effect affects the uncertainity of the treatment effect. It is not clear how to proceed when one wants to adjust for publication bias. Copas selection model uses the $p$-value threshold of 0.1 to decide if adjustment is necessary or not. One can argue that this threshold is arbitrary. Additionally, the threshold is used in two manners: first to decide if adjustment is necessary at all, and secondly, if the adjusted model does suitably account for publication bias ($p$ > 0.1). Hypothesis tests against the null hypothesis can however not be used to decide in favor of the null-hypothesis, and a separate test would be needed there. \\
With Copas selection model, it is in general the issue that it the model is not estimating, but rather applying some criteria of parsimony (the model with least publication bias with $p$-value for small study effect > 0.1 is chosen). Some authors find in their simulation studies that regression methods overperform the Copas selection model \citep{limitmeta}. \\
Thus, the results are not exact and could be improved if more suitable methods are applied. The bayesian approach taken by \citet{kicinsky} has many advantages over the applied methods, \eg by accounting for statistical significance, but relies on assumptions over the weight function and is at least somewhat sensible on the choice of the prior distributions. \\

\section{Outlook}
Given the size and abundancy of information in the dataset, the possibilities to investigate sources of bias in the Cochrane Database of Systematic Reviews are not exhausted (see for example in \citet{ioannidis.2017}). In particular, one could easily come up with more hypotheses that could be tested in meta meta-regression, as introduced in the last section of the results. \\
There are numerous suggestions for diffeerent measures of publication bias and small study effects, which could be applied on the dataset (see \citet{mueller.2016}). This study mainly relies on suggestions from \citet{Sterne}, \citet{Ioannidis2007} and \citet{limitmeta}. It uses most often methods that are well integrated in packages (\citet{metafor.package}, \citet{meta.package}). Although there are well addressed, mathematical justifications for these methods, there might be better methods not yet tested on large datasets. An evaluation of all suggested methods was far beyond the scope of this study.


\section{Implications}
This study is so far the largest assessment of publication bias by small study effects and uses data that has been collected after major efforts have been made to curb publication bias. The results indicate, similarly to other studies, that the efforts did not yet resolve the issue. \\
The findings further imply that when using meta-analyses, it is key to investigate sources of between-study heterogeneity; an alternative is to rely solely on the largest trials \citep{largest.study.only}. Since the Cochrane Organization uses results from grey literature to limit the effects of publication bias, but still does not manage to abolish it, there is also the need for journal editors to change their publication policies.


