Exclusion criteria:
- I2 > 0.5
- n < 5
- overlap of effect sizes with effect sizes of other meta-analyses

-> 366 Meta-analyses

results:
- 18.9% statistical sig. results
- 58.5% contained at least one sig. result
- 10 % most precise results were mostly based on one study (92.3%)
- median of the average ss 126.6 (IQR: 42, 173)

- 12.2% egger, 8.5% rank, 5.9% p-uniform and 4.4 TES

If controlled for number of results, only a difference between p-uniform results btw. cochrane and psyc. meta-analyses could be found.

Predicting effect sizes of r.e. and p-uniform: -> smd is dependent variable
r.e.: R2 = 0.152, I2 positive association w. effect size (0.002), h. mean of s.e. (0.776)
p-uniform: R2 = 0.014, no improvement of the 0 of no effects of I2, h.mean, discipline and n of sig. results

Overestimation of effect sizes:
mean y (= r.e. effect - p-uniform) = 0.042(0.002, 0.083), median = 0.051
mean y (= r.e. effect - 10% most precise) = 0.038(0.016, 0.061), median = 0.023

Predicting overestimation: see vanAllen_1

Monte Carlo Simulation study:
"unction of publication bias (pub) and average true effect size (μ). Average statistical power of all methods increased as a function of pub and decreased as a function of μ. None of the publication bias tests achieved an average statistical power larger than 0.5 if pub ≤ 0.95, and average statistical power did not exceed 0.2 for pub ≤ 0.75 for any true effect size examined... The results of the simulations also indicate that the observed proportions of statistically significant publication bias tests for the homogeneous subsets could have occurred for a large range of values for pub... 
The overestimation observed in the homogeneous subsets is in line with a large range of values for pub."

"A meta-meta-regression on the random-effects meta-analytic estimates revealed, in line with the hypothesis, a negative association of primary studies’ precision with a meta-analytic estimate. Since only weak evidence for publication bias was observed, this association was most likely mainly caused by differences in sample sizes between research fields. For instance, if researchers use statistical power analysis to determine the sample size of their study or if researchers in fields characterized by lower effect sizes use larger sample sizes by habit, larger true effect sizes will be associated with studies using smaller sample sizes."

"One possible explanation for this disparity is that we created homogeneous subsets of primary studies in this paper. Small-study effects can be caused by heterogeneity in true effect size, so eliminating this heterogeneity by creating homogeneous subsets may have led to not observing small-study effects. Publication bias could, however, have gone undetected due to a variety of reasons. First, publication bias is less of an issue if the relationship of interest in a meta-analysis was not the main focus of the primary studies. Statistical significance of the main result in a primary study probably determines whether a result gets published, rather than whether a secondary outcome or supplementary result is significant.. Second, meta-analysts included many unpublished studies in their meta-analyses, which might have decreased the severity and detectability of publication bias in our selected meta-analyses. Third, questionable research practices may have also decreased the detectability of publication bias in the meta-analyses, because questionable research practices may bias the effect size estimates of meta-analysis methods in any direction [52].. The weak evidence for publication bias may be also caused by the challenging characteristics of the homogeneous subsets. Our Monte-Carlo simulation study revealed that publication bias tests did not achieve reasonable statistical power to detect moderate publication bias, and overestimation of effect size only became apparent in conditions where at most 1 out of 4 statistically nonsignificant effect size was included in a meta-analysis (pub ≥ 0.75)."

"However, drawbacks of the I2-statistic are that it heavily depends on the sample size of the primary studies [86] and the statistic is imprecise in case of a small number of primary studies in a meta-analysis [87, 88]... 
Moreover, our selection of homogeneous subsets could have led to the exclusion of subsets with severe publication bias. Imagine a subset with a number of statistically significant effect sizes that were published in a field with considerable publication bias, and a few statistically nonsignificant effect sizes that were obtained from unpublished research. The inclusion of the effect sizes from the unpublished research may cause heterogeneity in true effect size, and therefore a subset with potentially severe publication bias was excluded from our study."
